[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rによる統計解析実戦ガイド",
    "section": "",
    "text": "はじめに\nこちらのサイトは、英語教育および第二言語習得理論の研究で必要とされる統計解析手法について、Rを用いて実践的に学ぶことを目的に作成されました。",
    "crumbs": [
      "はじめに"
    ]
  },
  {
    "objectID": "index.html#特徴",
    "href": "index.html#特徴",
    "title": "Rによる統計解析実戦ガイド",
    "section": "特徴",
    "text": "特徴\n\n実践的なアプローチ：理論と実践のバランスを重視\n再現可能な分析：すべてのコードが実行可能\n段階的な学習：基礎から応用まで体系的に構成",
    "crumbs": [
      "はじめに"
    ]
  },
  {
    "objectID": "index.html#対象",
    "href": "index.html#対象",
    "title": "Rによる統計解析実戦ガイド",
    "section": "対象",
    "text": "対象\n\n英語教育・第二言語習得研究に携わる大学生・大学院生\nRを用いた統計解析を学びたい方\n言語テスト理論に興味のある方",
    "crumbs": [
      "はじめに"
    ]
  },
  {
    "objectID": "index.html#構成",
    "href": "index.html#構成",
    "title": "Rによる統計解析実戦ガイド",
    "section": "構成",
    "text": "構成\n第I部：統計モデルの基礎\nt検定から分散分析まで、基本的な統計手法を学びます。\n第II部：回帰分析\n線形回帰から一般化線形モデル、混合効果モデルまでカバーします。\n第III部：ベイズ統計と機械学習\n現代的な統計手法とその応用を扱います。\n第IV部：多変量解析\n因子分析や項目応答理論など、多変量解析に関する手法を学びます。",
    "crumbs": [
      "はじめに"
    ]
  },
  {
    "objectID": "index.html#必要な環境",
    "href": "index.html#必要な環境",
    "title": "Rによる統計解析実戦ガイド",
    "section": "必要な環境",
    "text": "必要な環境\n\n\nコードを表示\n# 必要なパッケージの一括インストール\nrequired_packages &lt;- c(\n  # 基本パッケージ\n  \"tidyverse\", \"knitr\", \"rmarkdown\",\n  \n  # 統計解析\n  \"car\", \"effectsize\", \"pwr\", \"emmeans\",\n  \n  # 可視化\n  \"patchwork\", \"GGally\",\n  \n  # 高度な解析\n  \"lme4\", \"lmerTest\", \"lavaan\", \"psych\",\n  \"brms\", \"rstanarm\", \"bayesplot\"\n)\n\ninstall.packages(required_packages)",
    "crumbs": [
      "はじめに"
    ]
  },
  {
    "objectID": "1_T検定と効果量.html",
    "href": "1_T検定と効果量.html",
    "title": "3  T検定と効果量",
    "section": "",
    "text": "4 はじめに\nまずは英語教育研究で用いられる分析の基礎となる回帰分析 (regression analysis) について紹介します。回帰分析とは、\\(y = ax + b +ε\\)のような関数で表される統計モデルです。yを目的変数 (outcome variable; 従属変数 [dependent variable] や応答変数 [response variable] とも)、xを説明変数 (explanatory variable; 独立変数 [independent variable] や予測変数 [predictor variable] とも)、aを傾き (slope)、bを切片 (intercept)、εは誤差 (error) と呼ばれます。例えば、期末テストの点数を中間テストの点数で予測する分析をする際、期末テスト = 中間テスト * x + b + 誤差と表されます。説明変数を含めた「中間テスト * x + b + 誤差」の結びつきは線形結合 (linear combination) と呼ばれています。このような線型結合の統計モデルに対し、xが2乗や3乗になったり、対数を取ったりする回帰モデルは非線形モデル (nonlinear model) と呼ばれます。\n線型モデルは目的変数が正規分布 (normal distribution) に従うことを前提とします。そのため、線型モデルでは正規分布に従わないデータ (e.g., 正解・不正解のような2値データ、発生回数のような頻度) は扱うことができません。このような場合、「分析の前提を満たしていないため目的変数を対数変換した」「ボンフェローニ補正を用いたノンパラメトリック分析で検討した」のような分析が行われることがありますが、正規分布に従わないデータは一般化線型モデル (generalized linear model) を用いた分析を用いることが一般的です。一般化線型モデルを検討する前に、まずは基本となるt検定 (t-test) から復習していきましょう。",
    "crumbs": [
      "第I部：統計モデルの基礎",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "1_T検定と効果量.html#データの作成",
    "href": "1_T検定と効果量.html#データの作成",
    "title": "3  T検定と効果量",
    "section": "4.1 データの作成",
    "text": "4.1 データの作成\nデータセットは、50人の参加者が2種類の語彙学習方法を用いて比較された架空のデータです。この研究では、目標語彙に集中する学習方法がより効果的であると仮定します。具体的には、注釈付きの読解よりも、目標語彙を使用して文章を作る方が学習効果が高いと考えられます。参加者は15個ずつの目標語を2つの方法で学び、L2からL1への翻訳テスト (recall test) と選択問題 (recognition test) でその知識を評価します。このテストは学習直後と1週間後の遅延で実施しました。この実験設定を前提に、t検定をRで実施してみましょう。\nt検定：各学習方法で15語ずつ学習し、15点満点のテストで評価します。再生テストと再認テストの両方でt検定を行います。\n以下のコードを実行してデータセットを作成します。\n\n\nコードを表示\n# パッケージの読み込み\npacman::p_load(tidyverse, gt, psych, performance, emmeans, lmerTest, DHARMa, sjPlot, effectsize, car, compute.es, pwr, brunnermunzel, MASS)\n\n# テーマと乱数種の設定\ntheme_set(theme_bw())\nset.seed(123)\n\n\n\n\nコードを表示\n# サンプルデータの作成\n# パラメータの設定\nn_participants &lt;- 50 # 50人の参加者\nn_items &lt;- 30 # 30項目の目標語\ntreatments &lt;- c(\"Glossing\", \"Writing\") # 学習条件は2種類\ntests &lt;- c(\"Recall\", \"Recognition\") # テストは2種類\ntimes &lt;- c(\"Immediate\", \"Delayed\") # テスト実施時期は2種類\n\n# データフレームの作成\ndat &lt;- expand.grid(\n  Participant = 1:n_participants, \n  Item = 1:n_items,\n  Treatment = treatments,\n  Test = tests,\n  Time = times)\n\n# アイテムの割り当てをTreatmentに基づいて修正\ndat &lt;- dat[dat$Treatment == \"Glossing\" & dat$Item &lt;= 15 | \n           dat$Treatment == \"Writing\" & dat$Item &gt; 15, ]\n\n# スコアの割り当て\ndat$Score &lt;- ifelse(\n  dat$Treatment == \"Glossing\",\n  rbinom(nrow(dat), 1, ifelse(dat$Test == \"Recognition\", 0.5, 0.2)), \n  rbinom(nrow(dat), 1, ifelse(dat$Test == \"Recognition\", 0.9, 0.5)))\n\n# Timeによるスコアの調整\ndat$Score &lt;- ifelse(\n  dat$Time == \"Immediate\",\n  dat$Score + sample(0:1, nrow(dat), replace = TRUE),\n  dat$Score)\n\n# スコアが1を超えないように調整\ndat$Score &lt;- pmin(dat$Score, 1)\n\n# csvとしてデータを出力\n# write.csv(dat, \"sample.csv\")\n\n\n作成されたデータは50名の参加者に対して、2つの介入 (GlossingとWriting) を行っています。Glossingでは15語、Writingでは15語を学び、合計30語の目標語を学習します。その成果を直後と遅延で測定します。測定方法にはRecallとRecognitionの2種類のテストがあります。\nこのデータは以下のようにScoreの列が1か0の2値データとなっています。\n\n\nコードを表示\nhead(dat) |&gt; \n  gt()\n\n\n\n\n\n\n\n\nParticipant\nItem\nTreatment\nTest\nTime\nScore\n\n\n\n\n1\n1\nGlossing\nRecall\nImmediate\n1\n\n\n2\n1\nGlossing\nRecall\nImmediate\n1\n\n\n3\n1\nGlossing\nRecall\nImmediate\n1\n\n\n4\n1\nGlossing\nRecall\nImmediate\n1\n\n\n5\n1\nGlossing\nRecall\nImmediate\n1\n\n\n6\n1\nGlossing\nRecall\nImmediate\n1\n\n\n\n\n\n\n\nロジスティック回帰分析は目的変数 (Score) が2値のまま実行できるのですが、t検定やMANOVAは実行できません。そこで以下のようにコードで連続値へと足して15点満点にする必要があります。\n\n\nコードを表示\n# 参加者ごとにスコアを集計\ndat_summarized &lt;- dat |&gt;\n  mutate(Participant = factor(Participant),\n         Item = factor(Item)) |&gt;\n  group_by(Participant, Treatment, Test, Time) |&gt;\n  summarize(Score = sum(Score), .groups = 'drop')\n\n# 記述統計\ndat_summarized |&gt;\n  group_by(Treatment, Test, Time) |&gt;\n  summarise(\n    Mean = mean(Score),\n    SD = sd(Score),\n    N = n(),\n    SE = SD / sqrt(N),\n    CI_lower = Mean - 1.96 * SE,\n    CI_upper = Mean + 1.96 * SE) |&gt; \n  gt() |&gt; \n  tab_header(\n    title = \"Descriptive Statistics by Treatment, Test, and Time\") |&gt; \n  fmt_number(\n    columns = c(Mean, SD, SE, CI_lower, CI_upper),\n    decimals = 2)\n\n\n\n\n\n\n\n\nDescriptive Statistics by Treatment, Test, and Time\n\n\nTime\nMean\nSD\nN\nSE\nCI_lower\nCI_upper\n\n\n\n\nGlossing - Recall\n\n\nImmediate\n8.74\n1.97\n50\n0.28\n8.19\n9.29\n\n\nDelayed\n3.26\n1.82\n50\n0.26\n2.76\n3.76\n\n\nGlossing - Recognition\n\n\nImmediate\n11.42\n1.53\n50\n0.22\n11.00\n11.84\n\n\nDelayed\n7.44\n1.81\n50\n0.26\n6.94\n7.94\n\n\nWriting - Recall\n\n\nImmediate\n11.32\n1.71\n50\n0.24\n10.85\n11.79\n\n\nDelayed\n7.62\n1.96\n50\n0.28\n7.08\n8.16\n\n\nWriting - Recognition\n\n\nImmediate\n14.54\n0.61\n50\n0.09\n14.37\n14.71\n\n\nDelayed\n13.56\n1.05\n50\n0.15\n13.27\n13.85\n\n\n\n\n\n\n\n以下は参加者がTreatment、Test、Timeごとに15点満点のテストで何点かを表しています。\n\nデータ可視化\n\n\n\n\nコードを表示\nhead(dat_summarized) |&gt; \n  gt()\n\n\n\n\n\n\n\n\nParticipant\nTreatment\nTest\nTime\nScore\n\n\n\n\n1\nGlossing\nRecall\nImmediate\n10\n\n\n1\nGlossing\nRecall\nDelayed\n3\n\n\n1\nGlossing\nRecognition\nImmediate\n11\n\n\n1\nGlossing\nRecognition\nDelayed\n7\n\n\n1\nWriting\nRecall\nImmediate\n14\n\n\n1\nWriting\nRecall\nDelayed\n8\n\n\n\n\n\n\n\n\n\n記述統計をプロットを見ると、RecallおよびRecognitionでImmediateの方がDelayedよりも高く、Writingの方がGlossingよりも高い点数である傾向があることがわかります。\n\n\nコードを表示\nggplot(dat_summarized, aes(x = Treatment, y = Score, fill = Time)) +\n  geom_boxplot() +\n  facet_wrap(~ Test) +\n  labs(x = \"Treatment\", y = \"Score\") +\n  theme_bw()",
    "crumbs": [
      "第I部：統計モデルの基礎",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "1_T検定と効果量.html#分析方法",
    "href": "1_T検定と効果量.html#分析方法",
    "title": "3  T検定と効果量",
    "section": "5.1 分析方法",
    "text": "5.1 分析方法\n\n\nコードを表示\n# 直後における再生テストに関するt検定\nRecall_Score_Imm &lt;- subset(dat_imm, Test == \"Recall\")\nRecall_T_Imm &lt;- t.test(Score ~ Treatment, Recall_Score_Imm)\n\ndata.frame(\n  t_value = Recall_T_Imm$statistic,\n  df = Recall_T_Imm$parameter,\n  p_value = Recall_T_Imm$p.value,\n  CI_lower = Recall_T_Imm$conf.int[1],\n  CI_upper = Recall_T_Imm$conf.int[2],\n  Mean_Glossing = Recall_T_Imm$estimate[1],\n  Mean_Writing = Recall_T_Imm$estimate[2]) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Recall Score: Treatment Effect Test Results\") |&gt;\n  fmt_number(\n    columns = t_value,\n    decimals = 4) |&gt;\n  fmt_number(\n    columns = df,\n    decimals = 2) |&gt;\n  fmt_scientific(\n    columns = p_value,\n    decimals = 2) |&gt;\n  fmt_number(\n    columns = c(CI_lower, CI_upper),\n    decimals = 3) |&gt;\n  fmt_number(\n    columns = c(Mean_Glossing, Mean_Writing),\n    decimals = 2) |&gt;\n  cols_merge(\n    columns = c(CI_lower, CI_upper),\n    pattern = \"[{1}, {2}]\") |&gt;\n  cols_label(\n    t_value = \"t\",\n    df = \"df\",\n    p_value = \"p-value\",\n    CI_lower = \"95% CI\",\n    Mean_Glossing = \"Glossing\",\n    Mean_Writing = \"Writing\") |&gt;\n  tab_spanner(\n    label = \"Test Statistics\",\n    columns = c(t_value, df, p_value)) |&gt;\n  tab_spanner(\n    label = \"Group Means\",\n    columns = c(Mean_Glossing, Mean_Writing)) |&gt;\n  cols_align(\n    align = \"center\",\n    columns = everything()) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#f8f9fa\"),\n      cell_text(weight = \"bold\")),\n    locations = cells_column_spanners())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecall Score: Treatment Effect Test Results\n\n\n\nTest Statistics\n\n95% CI\n\nGroup Means\n\n\n\nt\ndf\np-value\nGlossing\nWriting\n\n\n\n\n−7.0034\n96.10\n3.40 × 10−10\n[−3.311, −1.849]\n8.74\n11.32\n\n\n\n\n\n\n\n\n\nコードを表示\n# 遅延における再認テストに関するt検定\nRecall_Score_Del &lt;- subset(dat_del, Test == \"Recall\") \nRecall_T_Del &lt;- t.test(Score ~ Treatment, Recall_Score_Del)\n\ndata.frame(\n  t_value = Recall_T_Del$statistic,\n  df = Recall_T_Del$parameter,\n  p_value = Recall_T_Del$p.value,\n  CI_lower = Recall_T_Del$conf.int[1],\n  CI_upper = Recall_T_Del$conf.int[2],\n  Mean_Glossing = Recall_T_Del$estimate[1],\n  Mean_Writing = Recall_T_Del$estimate[2]) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Recall Score: Treatment Effect Test Results\") |&gt;\n  fmt_number(\n    columns = t_value,\n    decimals = 4) |&gt;\n  fmt_number(\n    columns = df,\n    decimals = 2) |&gt;\n  fmt_scientific(\n    columns = p_value,\n    decimals = 2) |&gt;\n  fmt_number(\n    columns = c(CI_lower, CI_upper),\n    decimals = 3) |&gt;\n  fmt_number(\n    columns = c(Mean_Glossing, Mean_Writing),\n    decimals = 2) |&gt;\n  cols_merge(\n    columns = c(CI_lower, CI_upper),\n    pattern = \"[{1}, {2}]\") |&gt;\n  cols_label(\n    t_value = \"t\",\n    df = \"df\",\n    p_value = \"p-value\",\n    CI_lower = \"95% CI\",\n    Mean_Glossing = \"Glossing\",\n    Mean_Writing = \"Writing\") |&gt;\n  tab_spanner(\n    label = \"Test Statistics\",\n    columns = c(t_value, df, p_value)) |&gt;\n  tab_spanner(\n    label = \"Group Means\",\n    columns = c(Mean_Glossing, Mean_Writing)) |&gt;\n  cols_align(\n    align = \"center\",\n    columns = everything()) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#f8f9fa\"),\n      cell_text(weight = \"bold\")),\n    locations = cells_column_spanners())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecall Score: Treatment Effect Test Results\n\n\n\nTest Statistics\n\n95% CI\n\nGroup Means\n\n\n\nt\ndf\np-value\nGlossing\nWriting\n\n\n\n\n−11.5454\n97.45\n6.05 × 10−20\n[−5.109, −3.611]\n3.26\n7.62\n\n\n\n\n\n\n\n~ (チルダ) の左側には目的変数、右側には説明変数を取ります。t検定は目的変数が1つの連続値、説明変数が2水準のカテゴリカル変数の線型モデルです。そのため、正規性や等分散性などの分析の前提をクリアしていなければなりません。",
    "crumbs": [
      "第I部：統計モデルの基礎",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "1_T検定と効果量.html#分析の前提",
    "href": "1_T検定と効果量.html#分析の前提",
    "title": "3  T検定と効果量",
    "section": "5.2 分析の前提",
    "text": "5.2 分析の前提\n正規性はシャピロ・ウィルク検定やQQプロット、等分散性はF検定やルビーン検定を使って判定します。具体的には以下のようなコードで実行できます。\n\n\n\n\n\n\n前提条件のチェックポイント\n\n\n\n\n正規性：各グループのデータが正規分布に従うか\n\nShapiro-Wilk検定：p &gt; .05なら正規性の仮定を満たす\nQQプロット：点が直線上に並んでいれば正規分布に近い\n\n等分散性：両グループの分散が等しいか\n\nLevene検定/F検定：p &gt; .05なら等分散性の仮定を満たす\n\n独立性：観測値が互いに独立か（実験デザインで確保）\n\n\n\n【正規性の確認：シャピロ・ウィルク検定】\n\n\nコードを表示\n# 正規分布のチェック：p値が.05より小さければOK\n# Run Shapiro-Wilk tests and store results\nshapiro_glossing &lt;- shapiro.test(subset(dat_imm, Treatment == \"Glossing\")$Score)\nshapiro_writing &lt;- shapiro.test(subset(dat_imm, Treatment == \"Writing\")$Score)\n\n# Create data frame with raw values\ndata.frame(\n  Treatment = c(\"Glossing\", \"Writing\"),\n  W_statistic = c(shapiro_glossing$statistic, shapiro_writing$statistic),\n  p_value = c(shapiro_glossing$p.value, shapiro_writing$p.value),\n  Normality = c(\n    ifelse(shapiro_glossing$p.value &gt; 0.05, \"Normal\", \"Non-normal\"),\n    ifelse(shapiro_writing$p.value &gt; 0.05, \"Normal\", \"Non-normal\"))) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Shapiro-Wilk Normality Test Results\",\n    subtitle = \"Score Distribution by Treatment Group\") |&gt;\n  fmt_number(\n    columns = W_statistic,\n    decimals = 5) |&gt;\n  fmt_number(\n    columns = p_value,\n    decimals = 5) |&gt;\n  cols_label(\n    Treatment = \"Treatment Group\",\n    W_statistic = \"W Statistic\",\n    p_value = \"p-value\",\n    Normality = \"Distribution\") |&gt;\n  tab_style(\n    style = cell_fill(color = \"#ffe6e6\"),\n    locations = cells_body(\n      columns = c(p_value, Normality),\n      rows = p_value &lt; 0.05)) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#e6ffe6\"),\n    locations = cells_body(\n      columns = c(p_value, Normality),\n      rows = p_value &gt;= 0.05)) |&gt;\n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\"),\n      cell_fill(color = \"#f0f0f0\")),\n    locations = cells_body(\n      columns = Treatment)) |&gt;\n  cols_align(\n    align = \"center\",\n    columns = c(W_statistic, p_value, Normality)) |&gt;\n  tab_footnote(\n    footnote = \"α = 0.05; p &lt; 0.05 indicates departure from normality\",\n    locations = cells_column_labels(columns = p_value))\n\n\n\n\n\n\n\n\nShapiro-Wilk Normality Test Results\n\n\nScore Distribution by Treatment Group\n\n\nTreatment Group\nW Statistic\np-value1\nDistribution\n\n\n\n\nGlossing\n0.95019\n0.03475\nNon-normal\n\n\nWriting\n0.94252\n0.01696\nNon-normal\n\n\n\n1 α = 0.05; p &lt; 0.05 indicates departure from normality\n\n\n\n\n\n\n\n\n【正規性の確認：QQプロット】\n\nGlossingWriting\n\n\n\n\nコードを表示\n# QQプロット（Glossing）\nqqnorm(subset(dat_imm, Treatment == \"Glossing\")$Score, \n       main = \"Normal Q-Q Plot (Glossing)\")\nqqline(subset(dat_imm, Treatment == \"Glossing\")$Score)\n\n\n\n\n\n\n\n\n\n\n\n\n\nコードを表示\n# QQプロット（Writing）\nqqnorm(subset(dat_imm, Treatment == \"Writing\")$Score,\n       main = \"Normal Q-Q Plot (Writing)\")\nqqline(subset(dat_imm, Treatment == \"Writing\")$Score)\n\n\n\n\n\n\n\n\n\n\n\n\n【等分散性の検定：F検定】\n\n\nコードを表示\n# F検定(等分散性のチェック)：p値が.05より大きければOK\nvar_test_result &lt;- var.test(Score ~ Treatment, data = dat_imm)\n\ndata.frame(\n  F_stat = var_test_result$statistic,\n  df_num = var_test_result$parameter[1],\n  df_denom = var_test_result$parameter[2],\n  p_value = var_test_result$p.value,\n  CI_lower = var_test_result$conf.int[1],\n  CI_upper = var_test_result$conf.int[2],\n  ratio = var_test_result$estimate) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Levene's Test Alternative: F Test for Homogeneity of Variances\",\n    subtitle = \"Comparing Glossing and Writing Groups\") |&gt;\n  fmt_number(\n    columns = c(F_stat, ratio),\n    decimals = 4) |&gt;\n  fmt_number(\n    columns = c(df_num, df_denom),\n    decimals = 0) |&gt;\n  fmt_number(\n    columns = p_value,\n    decimals = 4) |&gt;\n  fmt_number(\n    columns = c(CI_lower, CI_upper),\n    decimals = 4) |&gt;\n  cols_merge(\n    columns = c(CI_lower, CI_upper),\n    pattern = \"[{1}, {2}]\") |&gt;\n  cols_merge(\n    columns = c(df_num, df_denom),\n    pattern = \"{1}, {2}\") |&gt;\n  cols_label(\n    F_stat = \"F\",\n    df_num = \"df\",\n    p_value = \"p-value\",\n    CI_lower = \"95% CI\",\n    ratio = \"Variance Ratio\") |&gt;\n  cols_align(\n    align = \"center\",\n    columns = everything()) |&gt;\n  tab_style(\n    style = case_when(\n      var_test_result$p.value &gt; 0.05 ~ list(\n        cell_fill(color = \"#e6ffe6\"),\n        cell_text(weight = \"bold\")),\n      TRUE ~ list(\n        cell_fill(color = \"#ffe6e6\"),\n        cell_text(weight = \"bold\"))),\n    locations = cells_body(columns = p_value)) |&gt;\n  tab_spanner(\n    label = \"Test Statistics\",\n    columns = c(F_stat, df_num, p_value)) |&gt;\n  tab_spanner(\n    label = \"Variance Comparison\",\n    columns = c(CI_lower, ratio)) |&gt;\n  tab_source_note(\n    source_note = md(\"**Decision**: p = 0.3252 &gt; 0.05 → Assume equal variances (use Student's t-test)\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevene's Test Alternative: F Test for Homogeneity of Variances\n\n\nComparing Glossing and Writing Groups\n\n\n\nTest Statistics\n\n\nVariance Comparison\n\n\n\nF\ndf\np-value\n95% CI\nVariance Ratio\n\n\n\n\n1.3271\n49, 49\n0.3252\n[0.7531, 2.3386]\n1.3271\n\n\n\nDecision: p = 0.3252 &gt; 0.05 → Assume equal variances (use Student’s t-test)\n\n\n\n\n\n\n\n\n【等分散性の検定：Levene検定】\n\n\nコードを表示\n# Levene検定(等分散性のチェック)：p値が.05より大きければOK\nlevene_result &lt;- leveneTest(Score ~ Treatment, data = dat_imm)\n\ndata.frame(\n  F_statistic = levene_result$`F value`[1],\n  df_group = levene_result$Df[1],\n  df_residual = levene_result$Df[2],\n  p_value = levene_result$`Pr(&gt;F)`[1]) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Levene's Test for Homogeneity of Variance\",\n    subtitle = \"Score by Treatment (center = median)\") |&gt;\n  fmt_number(\n    columns = F_statistic,\n    decimals = 3) |&gt;\n  fmt_number(\n    columns = c(df_group, df_residual),\n    decimals = 0) |&gt;\n  fmt_number(\n    columns = p_value,\n    decimals = 4) |&gt;\n  cols_merge(\n    columns = c(df_group, df_residual),\n    pattern = \"{1}, {2}\") |&gt;\n  cols_label(\n    F_statistic = \"F\",\n    df_group = \"df\",\n    p_value = \"p-value\") |&gt;\n  cols_add(\n    Decision = ifelse(levene_result$`Pr(&gt;F)`[1] &gt; 0.05, \n                      \"Equal variances\", \n                      \"Unequal variances\")) |&gt;\n  cols_align(\n    align = \"center\",\n    columns = everything()) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#e6ffe6\"),\n      cell_text(weight = \"bold\")),\n    locations = cells_body(\n      columns = p_value,\n      rows = p_value &gt; 0.05)) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#ffe6e6\"),\n      cell_text(weight = \"bold\")),\n    locations = cells_body(\n      columns = p_value,\n      rows = p_value &lt;= 0.05)) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_body(columns = Decision)) |&gt;\n  tab_source_note(\n    source_note = \"H₀: Group variances are equal | α = 0.05\") |&gt;\n  tab_options(\n    table.width = pct(80),\n    table.font.size = 13)\n\n\n\n\n\n\n\n\nLevene's Test for Homogeneity of Variance\n\n\nScore by Treatment (center = median)\n\n\nF\ndf\np-value\nDecision\n\n\n\n\n0.177\n1, 98\n0.6749\nEqual variances\n\n\n\nH₀: Group variances are equal | α = 0.05\n\n\n\n\n\n\n\n\n等分散性が満たされなかった場合やサンプルサイズが小さい場合には、Welchのt検定の使用が推奨されます。\n\n\nコードを表示\n# 等分散性が満たされなかった場合にはWelchのt検定を行う\nRecall_T_Imm_Welch &lt;- t.test(Score ~ Treatment, Recall_Score_Imm, var.equal = F)\n\n# Create compact table with raw values\ndata.frame(\n  t_value = Recall_T_Imm_Welch$statistic,\n  df = Recall_T_Imm_Welch$parameter,\n  p_value = Recall_T_Imm_Welch$p.value,\n  CI_lower = Recall_T_Imm_Welch$conf.int[1],\n  CI_upper = Recall_T_Imm_Welch$conf.int[2],\n  Mean_Glossing = Recall_T_Imm_Welch$estimate[1],\n  Mean_Writing = Recall_T_Imm_Welch$estimate[2],\n  Mean_Diff = Recall_T_Imm_Welch$estimate[2] - Recall_T_Imm_Welch$estimate[1]) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Welch Two Sample t-test\",\n    subtitle = \"Recall Score: Glossing vs Writing (unequal variances assumed)\") |&gt;\n  fmt_number(\n    columns = t_value,\n    decimals = 4) |&gt;\n  fmt_number(\n    columns = df,\n    decimals = 3) |&gt;\n  fmt_scientific(\n    columns = p_value,\n    decimals = 3) |&gt;\n  fmt_number(\n    columns = c(CI_lower, CI_upper),\n    decimals = 3) |&gt;\n  fmt_number(\n    columns = c(Mean_Glossing, Mean_Writing, Mean_Diff),\n    decimals = 2) |&gt;\n  cols_merge(\n    columns = c(CI_lower, CI_upper),\n    pattern = \"[{1}, {2}]\") |&gt;\n  cols_label(\n    t_value = \"t\",\n    df = \"df\",\n    p_value = \"p-value\",\n    CI_lower = \"95% CI\",\n    Mean_Glossing = \"Glossing\",\n    Mean_Writing = \"Writing\",\n    Mean_Diff = \"Difference\") |&gt;\n  tab_spanner(\n    label = \"Test Statistics\",\n    columns = c(t_value, df, p_value)) |&gt;\n  tab_spanner(\n    label = \"Group Means\",\n    columns = c(Mean_Glossing, Mean_Writing)) |&gt;\n  cols_align(\n    align = \"center\",\n    columns = everything()) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#ffe6e6\"),\n      cell_text(weight = \"bold\")),\n    locations = cells_body(\n      columns = p_value)) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#fff3cd\"),\n      cell_text(weight = \"bold\")),\n    locations = cells_body(\n      columns = Mean_Diff)) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#f8f9fa\"),\n      cell_text(weight = \"bold\")),\n    locations = cells_column_spanners()) |&gt;\n  tab_footnote(\n    footnote = \"Significant difference: Writing group scored 2.58 points higher\",\n    locations = cells_body(\n      columns = Mean_Diff,\n      rows = 1)) |&gt;\n  tab_source_note(\n    source_note = \"***p &lt; 0.001; Effect is statistically significant\")\n\n\n\n\n\n  \n    \n      Welch Two Sample t-test\n    \n    \n      Recall Score: Glossing vs Writing (unequal variances assumed)\n    \n    \n      \n        Test Statistics\n      \n      95% CI\n      \n        Group Means\n      \n      Difference\n    \n    \n      t\n      df\n      p-value\n      Glossing\n      Writing\n    \n  \n  \n    −7.0034\n96.101\n3.402 × 10−10\n[−3.311, −1.849]\n8.74\n11.32\n2.581\n  \n  \n    \n      ***p &lt; 0.001; Effect is statistically significant\n    \n  \n  \n    \n      1 Significant difference: Writing group scored 2.58 points higher\n    \n  \n\n\n\n\n\n\n\n\n\n\n前提条件が満たされない場合の対処法\n\n\n\n正規性が満たされない場合： - Mann-Whitney U検定（Wilcoxon rank-sum test） - 順位変換後のt検定 - ブートストラップ法\n等分散性が満たされない場合： - Welchのt検定（上記） - ブルンナー・ムンツェル検定\n\n\nコードを表示\n# Mann-Whitney U検定\nwilcox_result &lt;- wilcox.test(Score ~ Treatment, data = Recall_Score_Imm)\n\ndata.frame(\n  W_statistic = wilcox_result$statistic,\n  p_value = wilcox_result$p.value,\n  Method = \"Mann-Whitney U\") |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Wilcoxon Rank Sum Test (Mann-Whitney U)\",\n    subtitle = \"Recall Score: Glossing vs Writing\") |&gt;\n  fmt_number(\n    columns = W_statistic,\n    decimals = 1) |&gt;\n  fmt_scientific(\n    columns = p_value,\n    decimals = 3) |&gt;\n  cols_label(\n    W_statistic = \"W\",\n    p_value = \"p-value\",\n    Method = \"Test\") |&gt;\n  cols_add(\n    Decision = ifelse(wilcox_result$p.value &lt; 0.001, \n                      \"Significant difference***\", \n                      ifelse(wilcox_result$p.value &lt; 0.05, \n                             \"Significant difference*\", \n                             \"No significant difference\"))) |&gt;\n  cols_align(\n    align = \"center\",\n    columns = everything()) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#ffe6e6\"),\n      cell_text(weight = \"bold\")),\n    locations = cells_body(\n      columns = p_value)) |&gt;\n  tab_source_note(\n    source_note = \"H₀: No location shift between groups | ***p &lt; 0.001\")\n\n# Brunner-Munzel検定\nbm_result &lt;- brunnermunzel.test(Score ~ Treatment, data = Recall_Score_Imm)\n\ndata.frame(\n  BM_statistic = bm_result$statistic,\n  df = bm_result$parameter,\n  p_value = bm_result$p.value,\n  CI_lower = bm_result$conf.int[1],\n  CI_upper = bm_result$conf.int[2],\n  Probability = bm_result$estimate) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Brunner-Munzel Test\",\n    subtitle = \"Recall Score: Glossing vs Writing (robust to unequal variances)\") |&gt;\n  fmt_number(\n    columns = BM_statistic,\n    decimals = 4) |&gt;\n  fmt_number(\n    columns = df,\n    decimals = 3) |&gt;\n  fmt_scientific(\n    columns = p_value,\n    decimals = 3) |&gt;\n  fmt_number(\n    columns = c(CI_lower, CI_upper),\n    decimals = 4) |&gt;\n  fmt_number(\n    columns = Probability,\n    decimals = 3) |&gt;\n  cols_merge(\n    columns = c(CI_lower, CI_upper),\n    pattern = \"[{1}, {2}]\") |&gt;\n  cols_label(\n    BM_statistic = \"BM Statistic\",\n    df = \"df\",\n    p_value = \"p-value\",\n    CI_lower = \"95% CI\",\n    Probability = \"P(X&lt;Y) + 0.5P(X=Y)\") |&gt;\n  tab_spanner(\n    label = \"Test Statistics\",\n    columns = c(BM_statistic, df, p_value)) |&gt;\n  tab_spanner(\n    label = \"Effect Size\",\n    columns = c(CI_lower, Probability)) |&gt;\n  cols_align(\n    align = \"center\",\n    columns = everything()) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#ffe6e6\"),\n      cell_text(weight = \"bold\")),\n    locations = cells_body(\n      columns = p_value)) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#e6f3ff\"),\n      cell_text(weight = \"bold\")),\n    locations = cells_body(\n      columns = Probability)) |&gt;\n  tab_footnote(\n    footnote = \"Probability that a Writing score exceeds a Glossing score\",\n    locations = cells_body(\n      columns = Probability,\n      rows = 1)) |&gt;\n  tab_source_note(\n    source_note = \"***p &lt; 0.001; Writing group significantly outperforms Glossing group\")\n\n\n\n\nt検定をはじめ、カテゴリカル変数の比較ではサンプルサイズが大きくなると、p値が小さくなります。これは、サンプルサイズが大きくなると標準誤差が小さくなることに起因します。標準誤差はp値を計算するために必要なt値の値を変化させ、間接的にp値の推定に影響を与えます。そのため介入の効果がほとんど差がない状態であったとしても、サンプルサイズが大きくなれば、p値は小さくなり、有意差が見られます。サンプルサイズに影響を受けない値として効果量があります。",
    "crumbs": [
      "第I部：統計モデルの基礎",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "1_T検定と効果量.html#効果量",
    "href": "1_T検定と効果量.html#効果量",
    "title": "3  T検定と効果量",
    "section": "5.3 効果量",
    "text": "5.3 効果量\nt検定で使われる効果量にはCohen’s d、Hedges’s gがあります。\n\n5.3.1 Cohen’s d\n\n定義：Cohen’s dは、二つのグループ間の平均値の差を、プールされた標準偏差 (両グループの標準偏差の平均値) で割ったものです。\n計算式：\\(d=\\frac{\\bar{X_1}-\\bar{X_2}}{S_{pooled}}\\)\n\n\\(\\bar{X_1}\\)はグループ1の平均値、\\(\\bar{X_2}\\)はグループ2の平均値を表しています。\n\\(s_{pooled}\\)は2群間の標準偏差の平均値で以下のように計算されます\n\n\\(S_{\\text{pooled}} = \\sqrt{\\frac{(n_1 - 1) S_1^2 + (n_2 - 1) S_2^2}{n_1 + n_2 - 2}}\\)\n\n\n\n\n効果量の解釈基準\n\n\n\nCohen’s d\n効果の大きさ\n解釈\n\n\n\n\n0.2\n小 (Small)\nわずかな差。肉眼では判別困難\n\n\n0.5\n中 (Medium)\n中程度の差。注意深く観察すれば分かる\n\n\n0.8\n大 (Large)\n大きな差。明確に判別可能\n\n\n\n\n\n\n\n\n\n符号の解釈に注意\n\n\n\n効果量の符号（＋/－）は、どちらのグループを基準にするかで変わります。解釈する際は絶対値で判断し、どちらのグループが優れているかは平均値や箱ひげ図で確認してください。\n\n\n\n\n\n5.3.2 Hedges’s g\n\n定義：Hedges’s gは、小さなサンプルサイズの影響を補正しています。\n計算式：\\(g = \\frac{\\bar{X_1}-\\bar{X_2}}{S_{pooled}}×(1-\\frac{3}{4df-9})\\)\n\n\\(\\frac{\\bar{X_1}-\\bar{X_2}}{S_{pooled}}\\)はCohen’s d、\\(df\\)は自由度であり、参加者間デザインの場合には\\(n_1+n_2-2\\)となります。\n\n解釈：Cohen’s dと同様の基準で解釈されます。\n\nRでこれらの効果量を計算する際にはeffectsizeパッケージを使うこととなります。\n\n\nコードを表示\n# 効果量\ncohens_result &lt;- effectsize::cohens_d(Score ~ Treatment, data = Recall_Score_Imm)\nhedges_result &lt;- effectsize::hedges_g(Score ~ Treatment, data = Recall_Score_Imm)\n\n# Create compact effect size table\ndata.frame(\n  Effect_Size = c(\"Cohen's d\", \"Hedges' g\"),\n  Estimate = c(cohens_result$Cohens_d, hedges_result$Hedges_g),\n  CI_lower = c(cohens_result$CI_low, hedges_result$CI_low),\n  CI_upper = c(cohens_result$CI_high, hedges_result$CI_high)) |&gt;\n  mutate(\n    Magnitude = case_when(\n      abs(Estimate) &lt; 0.2 ~ \"Negligible\",\n      abs(Estimate) &lt; 0.5 ~ \"Small\",\n      abs(Estimate) &lt; 0.8 ~ \"Medium\",\n      abs(Estimate) &lt; 1.2 ~ \"Large\",\n      TRUE ~ \"Very Large\")) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Effect Size Measures\",\n    subtitle = \"Recall Score: Glossing vs Writing (negative values favor Writing)\") |&gt;\n  fmt_number(\n    columns = c(Estimate, CI_lower, CI_upper),\n    decimals = 2) |&gt;\n  cols_merge(\n    columns = c(CI_lower, CI_upper),\n    pattern = \"[{1}, {2}]\") |&gt;\n  cols_label(\n    Effect_Size = \"Measure\",\n    Estimate = \"Value\",\n    CI_lower = \"95% CI\",\n    Magnitude = \"Interpretation\") |&gt;\n  cols_align(\n    align = \"center\",\n    columns = c(Estimate, CI_lower, Magnitude)) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#e6f3ff\"),\n      cell_text(weight = \"bold\")),\n    locations = cells_body(\n      columns = Estimate)) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#fff3cd\"),\n      cell_text(weight = \"bold\")),\n    locations = cells_body(\n      columns = Magnitude)) |&gt;\n  tab_footnote(\n    footnote = \"Estimated using pooled SD\",\n    locations = cells_column_labels(columns = Estimate)) |&gt;\n  tab_source_note(\n    source_note = \"Interpretation: |d| &gt; 1.2 indicates a very large effect size\")\n\n\n\n\n\n  \n    \n      Effect Size Measures\n    \n    \n      Recall Score: Glossing vs Writing (negative values favor Writing)\n    \n    \n      Measure\n      Value1\n      95% CI\n      Interpretation\n    \n  \n  \n    Cohen's d\n−1.40\n[−1.84, −0.96]\nVery Large\n    Hedges' g\n−1.39\n[−1.82, −0.95]\nVery Large\n  \n  \n    \n      Interpretation: |d| &gt; 1.2 indicates a very large effect size\n    \n  \n  \n    \n      1 Estimated using pooled SD\n    \n  \n\n\n\n\n得られた結果は以下の通りです。Cohen’s d = -1.40 [-1.84, -0.96]、Hedge’s d = -1.39 [-1.82, -0.95] であることから、効果量大であると判断できます。\n得られた結果は以下の通りです。この結果はp値が非常に小さいため、GlossingとWritingの間で有意な差が見られたという結果でした。具体的に、どちらが大きかったのかは結果を可視化して確認することが望ましいです。t検定やANOVAであれば可視化せずとも間違えないかもしれませんが、複雑なモデリングでは符号を誤ってしまうタイプSエラーが生じる可能性があります。\nggplot2パッケージを使った可視化の方法については以下の通りです。\n\n基本的な箱ひげ図信頼区間付き平均値プロット結果の解釈\n\n\n\n\nコードを表示\n# 直後における再生テストスコアのデータで箱ひげ図を作成\nggplot(Recall_Score_Imm, aes(x = Treatment, y = Score, fill = Treatment)) +\n  geom_boxplot() +\n  labs(x = \"学習方法\", y = \"直後における再生テストのスコア\") +\n  theme_bw(base_family = \"HiraKakuPro-W3\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nコードを表示\n# 平均値と95%信頼区間を表示\ndat_summary &lt;- Recall_Score_Imm |&gt;\n  group_by(Treatment) |&gt;\n  summarise(\n    mean = mean(Score),\n    sd = sd(Score),\n    n = n(),\n    se = sd/sqrt(n),\n    ci_lower = mean - qt(0.975, n-1) * se,\n    ci_upper = mean + qt(0.975, n-1) * se)\n\nggplot(dat_summary, aes(x = Treatment, y = mean, fill = Treatment)) +\n  geom_col(alpha = 0.7) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), \n                width = 0.2, size = 1) +\n  geom_point(data = Recall_Score_Imm, \n             aes(x = Treatment, y = Score),\n             position = position_jitter(width = 0.1),\n             alpha = 0.3) +\n  labs(x = \"学習方法\", \n       y = \"スコア（平均値 ± 95% CI）\",\n       title = \"直後再生テストの結果\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n箱ひげ図を見るとWritingの方がGlossingよりも有意に高いスコアを獲得することがわかります (p &lt; .01, t = -7.00, df = 96.10)。この結果は大きな効果量が得られたことから、実質的に差があることが示されました (Cohen’s d = -1.40 [-1.84, -0.96])。",
    "crumbs": [
      "第I部：統計モデルの基礎",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "5_一般化線形モデル_一般化加法モデル.html",
    "href": "5_一般化線形モデル_一般化加法モデル.html",
    "title": "7  一般化線形モデル・一般化加法モデル",
    "section": "",
    "text": "8 はじめに\n線形回帰分析（lm）は誤差が正規分布に従うことを前提にしています。しかし現実のデータは、2値（正解/不正解）、頻度（0,1,2,…）、歪みの大きい正の連続値など、正規分布から外れることが少なくありません。一般化線形モデル（GLM; glm）は、リンク関数と分布族（誤差分布）を指定して、これらを統一的に扱います。さらに、一般化加法モデル（GAM; mgcv::gam）は、説明変数ごとに平滑関数を用いて非線形を柔軟にモデリングできます。",
    "crumbs": [
      "第II部：回帰分析",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>一般化線形モデル・一般化加法モデル</span>"
    ]
  },
  {
    "objectID": "5_一般化線形モデル_一般化加法モデル.html#glm-の要点",
    "href": "5_一般化線形モデル_一般化加法モデル.html#glm-の要点",
    "title": "7  一般化線形モデル・一般化加法モデル",
    "section": "8.1 GLM の要点",
    "text": "8.1 GLM の要点\n\n線形予測子：\\(\\eta = \\mathbf{X}\\beta\\)\n\nリンク関数：\\(g(\\mu) = \\eta\\)（\\(\\mu = \\mathbb{E}[Y]\\)）\n\n分布族：指数型分布族（正規、二項、ポアソン、ガンマ、逆正規 など）\n\n代表的な分布とリンク関数（既定）：\n\n\n\n\n\n\n\n\n分布 (family)\n\\(g(\\mu)\\)\nRでの指定\n\n\n\n\n正規 (gaussian)\n\\(\\mu\\)\nfamily = gaussian(link = \"identity\")\n\n\n二項 (binomial)\n\\(\\log\\frac{\\mu}{1-\\mu}\\)\nfamily = binomial(link = \"logit\")\n\n\nポアソン (poisson)\n\\(\\log(\\mu)\\)\nfamily = poisson(link = \"log\")\n\n\nガンマ (Gamma)\n\\(1/\\mu\\)\nfamily = Gamma(link = \"inverse\")\n\n\n逆正規 (Inverse.gaussian)\n\\(1/\\mu^2\\)\nfamily = inverse.gaussian(link = \"1/mu^2\")\n\n\n\n\n\n\n\n\n\nノート\n\n\n\nGLMの理論メモ（重要ポイント） - 正準リンク（canonical link）：各分布に「数式的に扱いやすい既定リンク」があります（例：二項→logit、ポアソン→log）。正準リンクは推定・漸近特性に利点がある一方、実務上は解釈の容易さで他のリンクを選ぶこともあります。\n- 分散関数と分散パラメータ：\\(\\mathrm{Var}(Y)=\\phi\\,V(\\mu)\\)。二項・ポアソンの\\(\\phi\\)（分散パラメータ）は既定で1。過分散がある場合はquasibinomialやquasipoisson、あるいは別分布（負の二項など）を検討します。\n- 逸脱度（Deviance）：飽和モデルとの対数尤度差に基づく当てはまりの指標。AICは\\(-2\\log L + 2k\\)。GAMでは平滑化パラメータも複雑さとして考慮されます。\n- 係数の解釈：リンク関数の世界で線形和を作り、リンクの逆関数で平均\\(\\mu\\)に戻して解釈します（例：ロジット→オッズ比、ポアソン→IRR）。\n\n\n\nglm(formula, family, data) で、formula は lm と同様に y ~ x1 + x2 形式です。違いは family を明示する点です。\n\n\n\n\nコードを表示\n# パッケージの読み込み\npacman::p_load(tidyverse, gt, psych, performance, emmeans, lmerTest, DHARMa, sjPlot, effectsize, car, compute.es, pwr, brunnermunzel, MASS, mgcv)\n\n# テーマと乱数種の設定\ntheme_set(theme_bw())\nset.seed(123)",
    "crumbs": [
      "第II部：回帰分析",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>一般化線形モデル・一般化加法モデル</span>"
    ]
  },
  {
    "objectID": "5_一般化線形モデル_一般化加法モデル.html#gamの要点と診断",
    "href": "5_一般化線形モデル_一般化加法モデル.html#gamの要点と診断",
    "title": "7  一般化線形モデル・一般化加法モデル",
    "section": "10.1 GAMの要点と診断",
    "text": "10.1 GAMの要点と診断\n\n平滑基底とEDF平滑化パラメータの推定診断（必須）\n\n\n\ns(x) はスプライン基底の線形結合。EDF（有効自由度）が曲線の複雑さ。\n\nk は基底次元。小さすぎるとパターンを捉えきれず、大きすぎると計算負荷↑。\n\n\n\n\n既定のmethod = \"REML\"（推奨）か\"GCV.Cp\"で過剰適合を抑制。\n\nselect = TRUEで不要な平滑の自動縮退（ゼロ効果の罰則）を有効化。\n\n\n\n\ngam.check(fit)：残差/QQ/スケール・kの適否（k'検定）\n\nconcurvity(fit)：曲線同士の依存（多重共線性のGAM版）\n\nplot(fit, pages = 1, shade = TRUE, rug = TRUE, residuals = TRUE)：平滑の形状を確認\n\n\n\nコードを表示\n# 例：平滑と診断\n# gam_fit &lt;- gam(response ~ cat_var1 + cat_var2 + s(cont_var), data = data, method = \"REML\", select = TRUE)\n# gam.check(gam_fit)\n# concurvity(gam_fit)\n# plot(gam_fit, pages = 1, shade = TRUE, rug = TRUE, residuals = TRUE)",
    "crumbs": [
      "第II部：回帰分析",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>一般化線形モデル・一般化加法モデル</span>"
    ]
  },
  {
    "objectID": "5_一般化線形モデル_一般化加法モデル.html#lm-と-gam-の当てはめ",
    "href": "5_一般化線形モデル_一般化加法モデル.html#lm-と-gam-の当てはめ",
    "title": "7  一般化線形モデル・一般化加法モデル",
    "section": "11.1 LM と GAM の当てはめ",
    "text": "11.1 LM と GAM の当てはめ\n\n\nコードを表示\n# LM\nlinear_model &lt;- lm(response ~ cat_var1 + cat_var2 + cont_var, data = data)\n\n# GAM（連続変数を s() で平滑化）\ngam_model &lt;- gam(response ~ cat_var1 + cat_var2 + s(cont_var), data = data)\n\ntab_model(linear_model, show.aic = TRUE, show.aicc = TRUE, show.stat = TRUE, show.est = TRUE)\n\n\n\n\n\n \nresponse\n\n\nPredictors\nEstimates\nCI\nStatistic\np\n\n\n(Intercept)\n-104.50\n-119.06 – -89.93\n-14.24\n&lt;0.001\n\n\ncat var1 [2]\n3.26\n0.69 – 5.84\n2.52\n0.014\n\n\ncat var1 [3]\n-10.95\n-13.49 – -8.41\n-8.56\n&lt;0.001\n\n\ncat var2 [1]\n-1.18\n-3.26 – 0.91\n-1.12\n0.264\n\n\ncont var\n4.30\n4.06 – 4.54\n36.08\n&lt;0.001\n\n\nObservations\n100\n\n\nR2 / R2 adjusted\n0.945 / 0.942\n\n\nAIC\n619.081\n\n\nAICc\n619.985\n\n\n\n\n\nコードを表示\ntab_model(gam_model,    show.aic = TRUE, show.aicc = TRUE, show.stat = TRUE, show.est = TRUE)\n\n\n\n\n\n \nresponse\n\n\nPredictors\nEstimates\nCI\nStatistic\np\n\n\n(Intercept)\n152.80\n150.82 – 154.78\n153.15\n&lt;0.001\n\n\ncat var1 [2]\n3.74\n1.33 – 6.15\n3.08\n0.003\n\n\ncat var1 [3]\n-11.70\n-14.10 – -9.29\n-9.67\n&lt;0.001\n\n\ncat var2 [1]\n-1.03\n-3.00 – 0.95\n-1.03\n0.305\n\n\nSmooth term (cont var)\n\n\n186.62\n&lt;0.001\n\n\nObservations\n100\n\n\nR2\n0.952\n\n\nAIC\n607.243\n\n\nAICc\n611.174\n\n\n\n\n\nコードを表示\nplot_model(linear_model, type = \"pred\", terms = c(\"cont_var\", \"cat_var1\", \"cat_var2\"))\n\n\n\n\n\n\n\n\n\nコードを表示\nplot_model(gam_model,    type = \"pred\", terms = c(\"cont_var\", \"cat_var1\", \"cat_var2\"))\n\n\n\n\n\n\n\n\n\nコードを表示\nmodel_performance(linear_model)\n\n\n# Indices of model performance\n\nAIC     |    AICc |     BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n---------------------------------------------------------------\n619.081 | 619.985 | 634.712 | 0.945 |     0.942 | 5.035 | 5.166\n\n\nコードを表示\nmodel_performance(gam_model)\n\n\n# Indices of model performance\n\nAIC     |    AICc |     BIC |    R2 |  RMSE | Sigma\n---------------------------------------------------\n607.243 | 611.174 | 639.926 | 0.952 | 4.445 | 4.726",
    "crumbs": [
      "第II部：回帰分析",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>一般化線形モデル・一般化加法モデル</span>"
    ]
  },
  {
    "objectID": "5_一般化線形モデル_一般化加法モデル.html#glmロジスティックと-gamロジスティック",
    "href": "5_一般化線形モデル_一般化加法モデル.html#glmロジスティックと-gamロジスティック",
    "title": "7  一般化線形モデル・一般化加法モデル",
    "section": "13.1 GLM（ロジスティック）と GAM（ロジスティック）",
    "text": "13.1 GLM（ロジスティック）と GAM（ロジスティック）\n\n\nコードを表示\n# GLM\nlogistic_model &lt;- glm(response ~ cat_var1 + cat_var2 + cont_var, family = binomial(), data = data)\n\nsummary(logistic_model)\n\n\n\nCall:\nglm(formula = response ~ cat_var1 + cat_var2 + cont_var, family = binomial(), \n    data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  0.32821    3.15322   0.104  0.91710   \ncat_var12    0.04154    0.53536   0.078  0.93816   \ncat_var13   -1.76201    0.55097  -3.198  0.00138 **\ncat_var21   -0.54481    0.45164  -1.206  0.22771   \ncont_var     0.01080    0.05132   0.210  0.83328   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.47  on 99  degrees of freedom\nResidual deviance: 121.57  on 95  degrees of freedom\nAIC: 131.57\n\nNumber of Fisher Scoring iterations: 4\n\n\nコードを表示\ntab_model(logistic_model, show.aic = TRUE, show.aicc = TRUE, show.stat = TRUE, show.est = TRUE)\n\n\n\n\n\n \nresponse\n\n\nPredictors\nOdds Ratios\nCI\nStatistic\np\n\n\n(Intercept)\n1.39\n0.00 – 711.64\n0.10\n0.917\n\n\ncat var1 [2]\n1.04\n0.36 – 3.01\n0.08\n0.938\n\n\ncat var1 [3]\n0.17\n0.06 – 0.49\n-3.20\n0.001\n\n\ncat var2 [1]\n0.58\n0.23 – 1.39\n-1.21\n0.228\n\n\ncont var\n1.01\n0.91 – 1.12\n0.21\n0.833\n\n\nObservations\n100\n\n\nR2 Tjur\n0.161\n\n\nAIC\n131.571\n\n\nAICc\n132.209\n\n\n\n\n\nコードを表示\nplot_model(logistic_model, type = \"pred\", terms = c(\"cont_var\", \"cat_var1\", \"cat_var2\"), show.data = TRUE)\n\n\n\n\n\n\n\n\n\nコードを表示\nmodel_performance(logistic_model)\n\n\n# Indices of model performance\n\nAIC     |    AICc |     BIC | Tjur's R2 |  RMSE | Sigma | Log_loss | Score_log\n------------------------------------------------------------------------------\n131.571 | 132.209 | 144.597 |     0.161 | 0.458 | 1.000 |    0.608 |   -40.363\n\nAIC     | Score_spherical |   PCP\n---------------------------------\n131.571 |           0.018 | 0.581\n\n\nコードを表示\n(pw_logit &lt;- emmeans(logistic_model, pairwise ~ cat_var1))\n\n\n$emmeans\n cat_var1 emmean    SE  df asymp.LCL asymp.UCL\n 1         0.702 0.377 Inf   -0.0367     1.441\n 2         0.744 0.386 Inf   -0.0120     1.500\n 3        -1.060 0.393 Inf   -1.8308    -0.289\n\nResults are averaged over the levels of: cat_var2 \nResults are given on the logit (not the response) scale. \nConfidence level used: 0.95 \n\n$contrasts\n contrast              estimate    SE  df z.ratio p.value\n cat_var11 - cat_var12  -0.0415 0.535 Inf  -0.078  0.9967\n cat_var11 - cat_var13   1.7620 0.551 Inf   3.198  0.0040\n cat_var12 - cat_var13   1.8035 0.554 Inf   3.255  0.0032\n\nResults are averaged over the levels of: cat_var2 \nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nコードを表示\nplot(pw_logit, comparisons = TRUE)\n\n\n\n\n\n\n\n\n\nコードを表示\nemmip(logistic_model, ~ cat_var1, CIs = TRUE)\n\n\n\n\n\n\n\n\n\nコードを表示\n# GAM（ロジスティック）\ngam_logistic_model &lt;- gam(response ~ cat_var1 + cat_var2 + s(cont_var), family = binomial(), data = data)\n\nsummary(gam_logistic_model)\n\n\n\nFamily: binomial \nLink function: logit \n\nFormula:\nresponse ~ cat_var1 + cat_var2 + s(cont_var)\n\nParametric coefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.0574     0.4724   2.239 0.025186 *  \ncat_var12     0.1784     0.5482   0.325 0.744840    \ncat_var13    -2.1153     0.5996  -3.528 0.000419 ***\ncat_var21    -0.5498     0.4743  -1.159 0.246371    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n              edf Ref.df Chi.sq p-value\ns(cont_var) 3.841  4.853  6.499   0.242\n\nR-sq.(adj) =  0.186   Deviance explained = 19.2%\nUBRE = 0.27568  Scale est. = 1         n = 100\n\n\nコードを表示\ntab_model(gam_logistic_model, show.aic = TRUE, show.aicc = TRUE, show.stat = TRUE, show.est = TRUE)\n\n\n\n\n\n \nresponse\n\n\nPredictors\nOdds Ratios\nCI\nStatistic\np\n\n\n(Intercept)\n2.88\n1.14 – 7.27\n2.24\n0.025\n\n\ncat var1 [2]\n1.20\n0.41 – 3.50\n0.33\n0.745\n\n\ncat var1 [3]\n0.12\n0.04 – 0.39\n-3.53\n&lt;0.001\n\n\ncat var2 [1]\n0.58\n0.23 – 1.46\n-1.16\n0.246\n\n\nSmooth term (cont var)\n\n\n6.50\n0.242\n\n\nObservations\n100\n\n\nR2\n0.186\n\n\nAIC\n127.568\n\n\nAICc\n129.089\n\n\n\n\n\nコードを表示\nplot_model(gam_logistic_model, type = \"pred\", terms = c(\"cont_var\", \"cat_var1\", \"cat_var2\"), show.data = TRUE)\n\n\n\n\n\n\n\n\n\nコードを表示\nmodel_performance(gam_logistic_model)\n\n\n# Indices of model performance\n\nAIC     |    AICc |     BIC |    R2 |  RMSE | Sigma | Log_loss | Score_log\n--------------------------------------------------------------------------\n127.568 | 129.089 | 147.994 | 0.186 | 0.435 | 1.000 |    0.559 |   -42.564\n\nAIC     | Score_spherical |   PCP\n---------------------------------\n127.568 |           0.028 | 0.614\n\n\nコードを表示\n(pw_gam_logit &lt;- emmeans(gam_logistic_model, pairwise ~ cat_var1))\n\n\n$emmeans\n cat_var1 emmean    SE   df lower.CL upper.CL\n 1         0.387 0.461 92.2   -0.529    1.303\n 2         0.565 0.465 92.2   -0.358    1.488\n 3        -1.728 0.556 92.2   -2.833   -0.623\n\nResults are averaged over the levels of: cat_var2 \nResults are given on the logit (not the response) scale. \nConfidence level used: 0.95 \n\n$contrasts\n contrast              estimate    SE   df t.ratio p.value\n cat_var11 - cat_var12   -0.178 0.548 92.2  -0.325  0.9433\n cat_var11 - cat_var13    2.115 0.600 92.2   3.528  0.0019\n cat_var12 - cat_var13    2.294 0.619 92.2   3.707  0.0010\n\nResults are averaged over the levels of: cat_var2 \nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nコードを表示\nplot(pw_gam_logit, comparisons = TRUE)\n\n\n\n\n\n\n\n\n\nコードを表示\nemmip(gam_logistic_model, ~ cat_var1, CIs = TRUE)\n\n\n\n\n\n\n\n\n\n\nオッズ比の解釈ROC/AUC としきい値キャリブレーション（較正）\n\n\n\n係数の指数変換がオッズ比。1を跨ぐ95%CI→差がない可能性。\n\n連続変数は「1単位増でオッズが何倍か」。標準化すれば1SD増の解釈に。\n\n\n\n\n\nコードを表示\nperformance::performance_roc(logistic_model)\n\n\nAUC: 71.19%\n\n\nコードを表示\nperformance::performance_roc(gam_logistic_model)\n\n\nAUC: 76.44%\n\n\n\nAUCは1に近いほど識別性能が高い。しきい値0.5固定は最適でない場合がある。\n\n\n\n\n\nコードを表示\ncheck_model(logistic_model)       # ロジスティックの診断（残差/QQ/影響点）\n\n\n\n\n\n\n\n\n\nコードを表示\ncheck_model(gam_logistic_model)\n\n\n\n\n\n\n\n\n\n\n予測確率が過大/過小に寄っていないかを視覚的に確認。\n\n\n\n\n\n出力のオッズ比は係数の指数変換です。95%CIに 1 を含むかどうかが差の判定の目安になります。",
    "crumbs": [
      "第II部：回帰分析",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>一般化線形モデル・一般化加法モデル</span>"
    ]
  },
  {
    "objectID": "5_一般化線形モデル_一般化加法モデル.html#glmポアソンと-gamポアソン",
    "href": "5_一般化線形モデル_一般化加法モデル.html#glmポアソンと-gamポアソン",
    "title": "7  一般化線形モデル・一般化加法モデル",
    "section": "14.1 GLM（ポアソン）と GAM（ポアソン）",
    "text": "14.1 GLM（ポアソン）と GAM（ポアソン）\n\n\nコードを表示\n# GLM\npoisson_model &lt;- glm(counts ~ cat_var1 + cat_var2 + cont_var, family = poisson(), data = data)\nsummary(poisson_model)\n\n\n\nCall:\nglm(formula = counts ~ cat_var1 + cat_var2 + cont_var, family = poisson(), \n    data = data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -4.559517   0.192249 -23.717  &lt; 2e-16 ***\ncat_var12    0.011995   0.037872   0.317   0.7514    \ncat_var13    0.201803   0.034815   5.796 6.77e-09 ***\ncat_var21   -0.058325   0.029324  -1.989   0.0467 *  \ncont_var     0.136931   0.002901  47.205  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2354.58  on 99  degrees of freedom\nResidual deviance:  108.67  on 95  degrees of freedom\nAIC: 669.67\n\nNumber of Fisher Scoring iterations: 4\n\n\nコードを表示\ntab_model(poisson_model, show.aic = TRUE, show.aicc = TRUE, show.stat = TRUE, show.est = TRUE)\n\n\n\n\n\n \ncounts\n\n\nPredictors\nIncidence Rate Ratios\nCI\nStatistic\np\n\n\n(Intercept)\n0.01\n0.01 – 0.02\n-23.72\n&lt;0.001\n\n\ncat var1 [2]\n1.01\n0.94 – 1.09\n0.32\n0.751\n\n\ncat var1 [3]\n1.22\n1.14 – 1.31\n5.80\n&lt;0.001\n\n\ncat var2 [1]\n0.94\n0.89 – 1.00\n-1.99\n0.047\n\n\ncont var\n1.15\n1.14 – 1.15\n47.20\n&lt;0.001\n\n\nObservations\n100\n\n\nR2 Nagelkerke\n1.000\n\n\nAIC\n669.674\n\n\nAICc\n670.312\n\n\n\n\n\nコードを表示\nplot_model(poisson_model, type = \"pred\", terms = c(\"cont_var\", \"cat_var1\", \"cat_var2\"), show.data = TRUE)\n\n\n\n\n\n\n\n\n\nコードを表示\nmodel_performance(poisson_model)\n\n\n# Indices of model performance\n\nAIC     |    AICc |     BIC | Nagelkerke's R2 |  RMSE | Sigma | Score_log | Score_spherical\n-------------------------------------------------------------------------------------------\n669.674 | 670.312 | 682.700 |           1.000 | 7.444 | 1.000 |    -3.298 |           0.089\n\n\nコードを表示\n(pw_pois &lt;- emmeans(poisson_model, pairwise ~ cat_var1))\n\n\n$emmeans\n cat_var1 emmean     SE  df asymp.LCL asymp.UCL\n 1          3.61 0.0277 Inf      3.55      3.66\n 2          3.62 0.0283 Inf      3.56      3.67\n 3          3.81 0.0252 Inf      3.76      3.86\n\nResults are averaged over the levels of: cat_var2 \nResults are given on the log (not the response) scale. \nConfidence level used: 0.95 \n\n$contrasts\n contrast              estimate     SE  df z.ratio p.value\n cat_var11 - cat_var12   -0.012 0.0379 Inf  -0.317  0.9462\n cat_var11 - cat_var13   -0.202 0.0348 Inf  -5.796  &lt;.0001\n cat_var12 - cat_var13   -0.190 0.0369 Inf  -5.144  &lt;.0001\n\nResults are averaged over the levels of: cat_var2 \nResults are given on the log (not the response) scale. \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nコードを表示\nplot(pw_pois, comparisons = TRUE)\n\n\n\n\n\n\n\n\n\nコードを表示\nemmip(poisson_model, ~ cat_var1, CIs = TRUE)\n\n\n\n\n\n\n\n\n\nコードを表示\n# GAM\ngam_poisson_model &lt;- gam(counts ~ cat_var1 + cat_var2 + s(cont_var), family = poisson(), data = data)\nsummary(gam_poisson_model)\n\n\n\nFamily: poisson \nLink function: log \n\nFormula:\ncounts ~ cat_var1 + cat_var2 + s(cont_var)\n\nParametric coefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.63110    0.03158 114.993  &lt; 2e-16 ***\ncat_var12    0.01843    0.03834   0.481   0.6307    \ncat_var13    0.20523    0.03499   5.866 4.47e-09 ***\ncat_var21   -0.05103    0.02998  -1.702   0.0887 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n              edf Ref.df Chi.sq p-value    \ns(cont_var) 1.815    2.3   2253  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.968   Deviance explained = 95.5%\nUBRE = 0.17774  Scale est. = 1         n = 100\n\n\nコードを表示\ntab_model(gam_poisson_model, show.aic = TRUE, show.aicc = TRUE, show.stat = TRUE, show.est = TRUE)\n\n\n\n\n\n \ncounts\n\n\nPredictors\nIncidence Rate Ratios\nCI\nStatistic\np\n\n\n(Intercept)\n37.75\n35.49 – 40.16\n114.99\n&lt;0.001\n\n\ncat var1 [2]\n1.02\n0.94 – 1.10\n0.48\n0.631\n\n\ncat var1 [3]\n1.23\n1.15 – 1.31\n5.87\n&lt;0.001\n\n\ncat var2 [1]\n0.95\n0.90 – 1.01\n-1.70\n0.089\n\n\nSmooth term (cont var)\n\n\n2253.19\n&lt;0.001\n\n\nObservations\n100\n\n\nR2\n0.968\n\n\nAIC\n668.781\n\n\nAICc\n669.632\n\n\n\n\n\nコードを表示\nplot_model(gam_poisson_model, type = \"pred\", terms = c(\"cont_var\", \"cat_var1\", \"cat_var2\"), show.data = TRUE)\n\n\n\n\n\n\n\n\n\nコードを表示\nmodel_performance(gam_poisson_model)\n\n\n# Indices of model performance\n\nAIC     |    AICc |     BIC |    R2 |  RMSE | Sigma | Score_log | Score_spherical\n---------------------------------------------------------------------------------\n668.781 | 669.632 | 683.931 | 0.968 | 7.106 | 1.000 |    -3.286 |           0.089\n\n\nコードを表示\n(pw_gam_pois &lt;- emmeans(gam_poisson_model, pairwise ~ cat_var1))\n\n\n$emmeans\n cat_var1 emmean     SE   df lower.CL upper.CL\n 1          3.60 0.0292 94.2     3.54     3.66\n 2          3.62 0.0289 94.2     3.56     3.68\n 3          3.81 0.0266 94.2     3.75     3.86\n\nResults are averaged over the levels of: cat_var2 \nResults are given on the log (not the response) scale. \nConfidence level used: 0.95 \n\n$contrasts\n contrast              estimate     SE   df t.ratio p.value\n cat_var11 - cat_var12  -0.0184 0.0383 94.2  -0.481  0.8806\n cat_var11 - cat_var13  -0.2052 0.0350 94.2  -5.866  &lt;.0001\n cat_var12 - cat_var13  -0.1868 0.0371 94.2  -5.032  &lt;.0001\n\nResults are averaged over the levels of: cat_var2 \nResults are given on the log (not the response) scale. \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nコードを表示\nplot(pw_gam_pois, comparisons = TRUE)\n\n\n\n\n\n\n\n\n\nコードを表示\nemmip(gam_poisson_model, ~ cat_var1, CIs = TRUE)\n\n\n\n\n\n\n\n\n\n\n過分散チェック負の二項（代替）オフセット（曝露量）\n\n\n\n\nコードを表示\nperformance::check_overdispersion(poisson_model)\n\n\n# Overdispersion test\n\n       dispersion ratio =   1.117\n  Pearson's Chi-Squared = 106.082\n                p-value =   0.205\n\n\nコードを表示\nperformance::check_overdispersion(gam_poisson_model)\n\n\n# Overdispersion test\n\n       dispersion ratio =   1.090\n  Pearson's Chi-Squared = 103.594\n                p-value =   0.257\n\n\n\n過分散（\\(\\phi \\gg 1\\)）なら quasipoisson や 負の二項を検討。\n\n\n\n\n\nコードを表示\nMASS::glm.nb(counts ~ cat_var1 + cat_var2 + cont_var, data = data)\n\n\n\nCall:  MASS::glm.nb(formula = counts ~ cat_var1 + cat_var2 + cont_var, \n    data = data, init.theta = 495.476198, link = log)\n\nCoefficients:\n(Intercept)    cat_var12    cat_var13    cat_var21     cont_var  \n   -4.52378      0.01331      0.20316     -0.05735      0.13633  \n\nDegrees of Freedom: 99 Total (i.e. Null);  95 Residual\nNull Deviance:      2072 \nResidual Deviance: 98.96    AIC: 671.1\n\n\n\n\n同じ「時間・人数・面積」あたりの発生率を比較するには、offset(log(exposure))を使います。\n\n\nコードを表示\nset.seed(1)\ndata$exposure &lt;- runif(nrow(data), min = 0.5, max = 2)  # 例：観測時間（時間）\nglm(counts ~ cat_var1 + cat_var2 + cont_var + offset(log(exposure)),\n    family = poisson(), data = data) |&gt; tab_model()\n\n\n\n\n\n \ncounts\n\n\nPredictors\nIncidence Rate Ratios\nCI\np\n\n\n(Intercept)\n0.01\n0.00 – 0.01\n&lt;0.001\n\n\ncat var1 [2]\n1.05\n0.97 – 1.13\n0.231\n\n\ncat var1 [3]\n1.44\n1.34 – 1.55\n&lt;0.001\n\n\ncat var2 [1]\n1.06\n1.00 – 1.12\n0.065\n\n\ncont var\n1.15\n1.15 – 1.16\n&lt;0.001\n\n\nObservations\n100\n\n\nR2 Nagelkerke\n1.000\n\n\n\n\n\n\n\n\n\ntab_model() の IRR（Incidence Rate Ratio） は係数の指数変換。95%CIが 1 を跨ぐと差がない可能性を示唆します。",
    "crumbs": [
      "第II部：回帰分析",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>一般化線形モデル・一般化加法モデル</span>"
    ]
  },
  {
    "objectID": "6_混合効果モデル_2値ロジスティック回帰.html",
    "href": "6_混合効果モデル_2値ロジスティック回帰.html",
    "title": "8  混合効果モデル：2値ロジスティック回帰分析",
    "section": "",
    "text": "9 はじめに\n一般化線形混合効果モデル（GLMM）は、2値・頻度など非正規の目的変数に対して、固定効果とランダム効果を同時に扱う枠組みです。本章では、語彙学習の架空データを用いて、2値ロジスティックGLMMを段階的に実装します。\n\n\n10 データの作成\n\n\nコードを表示\nlibrary(tidyverse)\ntheme_set(theme_bw())\n\nset.seed(123)\n\nn_participants &lt;- 50\nn_items        &lt;- 30\ntreatments     &lt;- c(\"Glossing\",\"Writing\")\ntests          &lt;- c(\"Recall\",\"Recognition\")\ntimes          &lt;- c(\"Immediate\",\"Delayed\")\n\ndat &lt;- expand.grid(\n  Participant = 1:n_participants,\n  Item        = 1:n_items,\n  Treatment   = treatments,\n  Test        = tests,\n  Time        = times,\n  KEEP.OUT.ATTRS = FALSE\n)\n\n# アイテム割当（Glossing:1-15, Writing:16-30）\ndat &lt;- dat[\n  (dat$Treatment == \"Glossing\" & dat$Item &lt;= 15) |\n  (dat$Treatment == \"Writing\"  & dat$Item &gt;  15), ]\n\n# 0/1反応生成：Writing &gt; Glossing, Recognition &gt; Recall, Immediate &gt; Delayed\ndat$Score &lt;- ifelse(\n  dat$Treatment == \"Glossing\",\n  rbinom(nrow(dat), 1, ifelse(dat$Test == \"Recognition\", 0.50, 0.20)),\n  rbinom(nrow(dat), 1, ifelse(dat$Test == \"Recognition\", 0.90, 0.50))\n)\ndat$Score &lt;- ifelse(dat$Time == \"Immediate\",\n                    pmin(dat$Score + sample(0:1, nrow(dat), TRUE), 1),\n                    dat$Score)\n\n\n\n\n11 共変量の追加\n\n\nコードを表示\nparticipants       &lt;- unique(dat$Participant)\nproficiency_values &lt;- rnorm(length(participants), mean = 2500, sd = sqrt(500))\naptitude_values    &lt;- rnorm(length(participants), mean = 10,   sd = sqrt(10))\n\nparticipant_data &lt;- tibble(\n  Participant = participants,\n  Proficiency = proficiency_values,\n  Aptitude    = aptitude_values\n)\n\ndat &lt;- dat |&gt;\n  left_join(participant_data, by = \"Participant\") |&gt;\n  mutate(\n    Participant = factor(Participant),\n    Item        = factor(Item),\n    Treatment   = factor(Treatment),\n    Time        = factor(Time),\n    Proficiency = as.numeric(scale(Proficiency)),\n    Aptitude    = as.numeric(scale(Aptitude))\n  )\n\n\n\n\n12 データ整形\n\n\nコードを表示\ndat_recall       &lt;- dplyr::filter(dat, Test == \"Recall\")\ndat_recognition  &lt;- dplyr::filter(dat, Test == \"Recognition\")\n\n# 確認\ndplyr::count(dat_recall, Treatment, Time) |&gt; arrange(Treatment, Time)\n\n\n  Treatment      Time   n\n1  Glossing Immediate 750\n2  Glossing   Delayed 750\n3   Writing Immediate 750\n4   Writing   Delayed 750\n\n\nコードを表示\ndplyr::count(dat_recognition, Treatment, Time) |&gt; arrange(Treatment, Time)\n\n\n  Treatment      Time   n\n1  Glossing Immediate 750\n2  Glossing   Delayed 750\n3   Writing Immediate 750\n4   Writing   Delayed 750\n\n\n\n\n13 コーディング\n\n理論解説コード例結果解釈\n\n\nカテゴリカル変数は sum coding（和=0） を用います。これにより、切片は水準平均、主効果は両群の平均差、交互作用は傾きの差として解釈できます。2水準ではレベルが {−1, +1} に再符号化され、単純主効果の符号と大きさが読み取りやすくなります。\n\n\n\n\nコードを表示\n# 2水準のカテゴリカルは sum coding（和=0）\ncontrasts(dat_recall$Treatment) &lt;- contr.sum(2)\ncontrasts(dat_recall$Time)      &lt;- contr.sum(2)\n\ncontrasts(dat_recognition$Treatment) &lt;- contr.sum(2)\ncontrasts(dat_recognition$Time)      &lt;- contr.sum(2)\n\n\n\n\nsum coding 下では、係数は両群平均からの偏差として現れます。可視化（emmip/plot_model）と合わせると交互作用の向きを直感的に確認できます。\n\n\n\n\n\n14 モデル仕様（lme4）\n\n\nコードを表示\nlibrary(lme4)\nlibrary(lmerTest)   # p値の補助（連続目的のとき）。GLMMでは主に推定補助。\nlibrary(emmeans)\nlibrary(MuMIn)\nlibrary(performance)\nlibrary(DHARMa)\nlibrary(sjPlot)\n\n# 例：Recall の最大モデル（固定：Treatment*Time + 共変量、ランダム：Itemに傾き、Participantに切片）\nFull_Model_Recall &lt;- glmer(\n  Score ~ Treatment * Time + Proficiency + Aptitude +\n    (1 + Proficiency + Aptitude | Item) +\n    (1 | Participant),\n  data    = dat_recall,\n  family  = binomial(),\n  control = glmerControl(optimizer = \"bobyqa\")\n)\n\n\n\n\n15 固定効果の選択（AICc）\n\n理論解説コード例結果解釈\n\n\nMuMIn::dredge() による AICc 最小化で固定効果候補を比較します。options(na.action = \"na.fail\") を設定し、同一データでモデル群を評価します。AICc が近いモデルは解釈の一貫性や理論的妥当性で選びます。\n\n\n\n\nコードを表示\noptions(na.action = \"na.fail\")\nselection_recall &lt;- MuMIn::dredge(Full_Model_Recall)   # AICc で順位付け\nhead(selection_recall)\n\n\nGlobal model call: glmer(formula = Score ~ Treatment * Time + Proficiency + Aptitude + \n    (1 + Proficiency + Aptitude | Item) + (1 | Participant), \n    data = dat_recall, family = binomial(), control = glmerControl(optimizer = \"bobyqa\"))\n---\nModel selection table \n     (Int)     Apt      Prf Tim Trt Tim:Trt df    logLik   AICc delta weight\n29 0.05224                    +   +       + 11 -1839.418 3700.9  0.00  0.513\n30 0.05227 0.01962            +   +       + 12 -1839.307 3702.7  1.79  0.209\n31 0.05226         0.005517   +   +       + 12 -1839.410 3702.9  2.00  0.189\n32 0.05230 0.02181 0.010430   +   +       + 13 -1839.279 3704.7  3.75  0.079\n13 0.07375                    +   +         10 -1844.744 3709.6  8.64  0.007\n14 0.07376 0.01994            +   +         11 -1844.628 3711.3 10.42  0.003\nModels ranked by AICc(x) \nRandom terms (all models): \n  1 + Proficiency + Aptitude | Item, 1 | Participant\n\n\nコードを表示\nplot(selection_recall, col.mode = \"value\", labAsExpr = TRUE)\n\n\n\n\n\n\n\n\n\nコードを表示\nBest_Fixed_Recall &lt;- MuMIn::get.models(selection_recall, 1)[[1]]\nsummary(Best_Fixed_Recall)\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Treatment + (1 + Proficiency + Aptitude | Item) +  \n    (1 | Participant) + Time:Treatment\n   Data: dat_recall\nControl: glmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   3700.8    3766.9   -1839.4    3678.8      2989 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.9493 -1.0087  0.5516  0.8459  2.0194 \n\nRandom effects:\n Groups      Name        Variance Std.Dev. Corr     \n Participant (Intercept) 0.002303 0.04799           \n Item        (Intercept) 0.000000 0.00000           \n             Proficiency 0.008167 0.09037   NaN     \n             Aptitude    0.005136 0.07166   NaN 1.00\nNumber of obs: 3000, groups:  Participant, 50; Item, 30\n\nFixed effects:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       0.05224    0.04087   1.278  0.20118    \nTime1             0.67882    0.04042  16.794  &lt; 2e-16 ***\nTreatment1       -0.52742    0.04158 -12.685  &lt; 2e-16 ***\nTime1:Treatment1  0.13125    0.04027   3.259  0.00112 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time1  Trtmn1\nTime1       -0.017              \nTreatment1   0.017 -0.169       \nTm1:Trtmnt1 -0.160  0.032 -0.021\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\n\n\nBest_Fixed_Recall は AICc が最小の固定効果仕様です。以降のランダム効果選択では、この固定効果式を基礎にします。\n\n\n\n\n\n16 ランダム効果の選択\n\n理論解説コード例結果解釈\n\n\n(1|群) はランダム切片、(0+傾き|群) はランダム傾き、|| は相関を外す指定です。singular fit は過度な複雑さや情報不足を示唆します。\n\n\n\n\nコードを表示\n# 固定効果は Best_Fixed_Recall の式を流用し、ランダム効果を段階比較\nform_fixed &lt;- formula(Best_Fixed_Recall)\nform_fixed\n\n\nScore ~ Time + Treatment + (1 + Proficiency + Aptitude | Item) + \n    (1 | Participant) + Time:Treatment\n\n\nコードを表示\nfit_r1 &lt;- update(Best_Fixed_Recall, . ~ . - (1 + Proficiency + Aptitude | Item) + (1 | Item) + (1 | Participant))\nfit_r2 &lt;- update(Best_Fixed_Recall, . ~ . - (1 + Proficiency + Aptitude | Item) + (0 + Proficiency | Item) + (1 | Participant))\nfit_r3 &lt;- update(Best_Fixed_Recall, . ~ . - (1 + Proficiency + Aptitude | Item) + (0 + Aptitude    | Item) + (1 | Participant))\nfit_r4 &lt;- update(Best_Fixed_Recall, . ~ . - (1 + Proficiency + Aptitude | Item) + (1 + Proficiency || Item) + (1 | Participant))\nfit_r5 &lt;- update(Best_Fixed_Recall, . ~ . - (1 + Proficiency + Aptitude | Item) + (1 + Aptitude    || Item) + (1 | Participant))\nfit_r6 &lt;- update(Best_Fixed_Recall, . ~ . - (1 + Proficiency + Aptitude | Item) + (1 | Item) + (1 | Participant))\n\nanova(fit_r1, fit_r2, fit_r3, fit_r4, fit_r5, fit_r6, test = \"Chisq\")\n\n\nData: dat_recall\nModels:\nfit_r1: Score ~ Time + Treatment + (1 | Participant) + (1 | Item) + Time:Treatment\nfit_r2: Score ~ Time + Treatment + (1 | Participant) + (0 + Proficiency | Item) + Time:Treatment\nfit_r3: Score ~ Time + Treatment + (1 | Participant) + (0 + Aptitude | Item) + Time:Treatment\nfit_r6: Score ~ Time + Treatment + (1 | Participant) + (1 | Item) + Time:Treatment\nfit_r4: Score ~ Time + Treatment + (1 | Participant) + (1 + Proficiency || Item) + Time:Treatment\nfit_r5: Score ~ Time + Treatment + (1 | Participant) + (1 + Aptitude || Item) + Time:Treatment\n       npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)\nfit_r1    6 3691.6 3727.7 -1839.8    3679.6                     \nfit_r2    6 3691.6 3727.6 -1839.8    3679.6 0.0429  0           \nfit_r3    6 3691.6 3727.7 -1839.8    3679.6 0.0000  0           \nfit_r6    6 3691.6 3727.7 -1839.8    3679.6 0.0000  0           \nfit_r4    7 3693.6 3735.6 -1839.8    3679.6 0.0429  1     0.8359\nfit_r5    7 3693.6 3735.7 -1839.8    3679.6 0.0000  0           \n\n\n\n\n情報量基準と対数尤度のバランスで、過度な相関付き傾きを避けつつ、必要十分な構造を選びます。\n\n\n\n\n\n17 ベストモデルの要約\n\n理論解説コード例結果解釈\n\n\nロジット係数は指数変換でオッズ比になります。95%CI が 1 を跨がないかを確認します。\n\n\n\n\nコードを表示\nBest_Recall &lt;- fit_r6  # 例：AIC/BIC/対数尤度や singular の有無等で選択\nsummary(Best_Recall)\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \nScore ~ Time + Treatment + (1 | Participant) + (1 | Item) + Time:Treatment\n   Data: dat_recall\nControl: glmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   3691.6    3727.7   -1839.8    3679.6      2994 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.7739 -1.0142  0.5671  0.8462  1.9096 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n Participant (Intercept) 0.002059 0.04537 \n Item        (Intercept) 0.000000 0.00000 \nNumber of obs: 3000, groups:  Participant, 50; Item, 30\n\nFixed effects:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       0.05206    0.04072   1.279  0.20105    \nTime1             0.67698    0.04029  16.804  &lt; 2e-16 ***\nTreatment1       -0.52604    0.04025 -13.068  &lt; 2e-16 ***\nTime1:Treatment1  0.13090    0.04021   3.255  0.00113 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time1  Trtmn1\nTime1       -0.018              \nTreatment1   0.030 -0.166       \nTm1:Trtmnt1 -0.161  0.031 -0.019\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nコードを表示\ntab_model(Best_Recall, show.stat = TRUE, show.est = TRUE, dv.labels = \"Score (Recall)\")\n\n\n\n\n \nScore (Recall)\n\n\nPredictors\nOdds Ratios\nCI\nStatistic\np\n\n\n(Intercept)\n1.05\n0.97 – 1.14\n1.28\n0.201\n\n\nTime1\n1.97\n1.82 – 2.13\n16.80\n&lt;0.001\n\n\nTreatment1\n0.59\n0.55 – 0.64\n-13.07\n&lt;0.001\n\n\nTime1 × Treatment1\n1.14\n1.05 – 1.23\n3.26\n0.001\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 Participant\n0.00\n\n\nτ00 Item\n0.00\n\n\nN Participant\n50\n\n\nN Item\n30\n\nObservations\n3000\n\n\nMarginal R2 / Conditional R2\n0.186 / NA\n\n\n\n\n\nコードを表示\n# オッズ比（係数の指数変換）\nexp(fixef(Best_Recall)) |&gt; round(2)\n\n\n     (Intercept)            Time1       Treatment1 Time1:Treatment1 \n            1.05             1.97             0.59             1.14 \n\n\n\n\nTime・Treatment の主効果と交互作用の向きと大きさを、オッズ比と可視化で確認します。\n\n\n\n\n\n18 下位検定と可視化\n\n理論解説コード例結果解釈\n\n\nemmeans で条件付き平均を比較します。出力は logit スケールであり、必要に応じて応答確率へ変換して解釈します。\n\n\n\n\nコードを表示\n# Time ごとの Treatment 比較（logit スケール）\nemm_pair_recall &lt;- emmeans(Best_Recall, pairwise ~ Treatment | Time)\nemm_pair_recall$emmeans\n\n\nTime = Immediate:\n Treatment emmean     SE  df asymp.LCL asymp.UCL\n Glossing   0.334 0.0744 Inf     0.188     0.480\n Writing    1.124 0.0852 Inf     0.957     1.291\n\nTime = Delayed:\n Treatment emmean     SE  df asymp.LCL asymp.UCL\n Glossing  -1.282 0.0889 Inf    -1.456    -1.108\n Writing    0.032 0.0733 Inf    -0.112     0.176\n\nResults are given on the logit (not the response) scale. \nConfidence level used: 0.95 \n\n\nコードを表示\nemm_pair_recall$contrasts\n\n\nTime = Immediate:\n contrast           estimate    SE  df z.ratio p.value\n Glossing - Writing    -0.79 0.113 Inf  -7.013  &lt;.0001\n\nTime = Delayed:\n contrast           estimate    SE  df z.ratio p.value\n Glossing - Writing    -1.31 0.115 Inf -11.436  &lt;.0001\n\nResults are given on the log odds ratio (not the response) scale. \n\n\nコードを表示\n# プロット\nemmip(Best_Recall, Treatment ~ Time, CIs = TRUE)\n\n\n\n\n\n\n\n\n\nコードを表示\nplot(emm_pair_recall, comparisons = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n交互作用がある場合、両水準での単純効果を確認し、効果の大きさと向きを可視化で示します。\n\n\n\n\n\n19 診断\n\n理論解説コード例結果解釈\n\n\nDHARMa のシミュレーション残差で適合度と外れ値・過分散を点検します。performance::check_overdispersion() で分散の過不足を確認します。\n\n\n\n\nコードを表示\nsim &lt;- DHARMa::simulateResiduals(Best_Recall)\nplot(sim)\n\n\n\n\n\n\n\n\n\nコードを表示\ntestResiduals(sim)\n\n\n\n\n\n\n\n\n\n$uniformity\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  simulationOutput$scaledResiduals\nD = 0.0099988, p-value = 0.9251\nalternative hypothesis: two-sided\n\n\n$dispersion\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 1.0003, p-value = 0.976\nalternative hypothesis: two.sided\n\n\n$outliers\n\n    DHARMa outlier test based on exact binomial test with approximate\n    expectations\n\ndata:  simulationOutput\noutliers at both margin(s) = 19, observations = 3000, p-value = 0.3556\nalternative hypothesis: true probability of success is not equal to 0.007968127\n95 percent confidence interval:\n 0.00381726 0.00987269\nsample estimates:\nfrequency of outliers (expected: 0.00796812749003984 ) \n                                           0.006333333 \n\n\nコードを表示\nperformance::check_overdispersion(Best_Recall)\n\n\n# Overdispersion test\n\n dispersion ratio = 1.000\n          p-value = 0.976\n\n\nコードを表示\nperformance::model_performance(Best_Recall)\n\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC | R2 (cond.) | R2 (marg.) |  RMSE | Sigma\n------------------------------------------------------------------------\n3691.630 | 3691.658 | 3727.668 |            |      0.186 | 0.460 | 1.000\n\nAIC      | Log_loss | Score_log | Score_spherical\n-------------------------------------------------\n3691.630 |    0.613 |      -Inf |           0.002\n\n\n\n\n残差プロットと検定の結果を踏まえ、モデル仕様（とくにランダム効果）やリンク関数の妥当性を確認します。\n\n\n\n\n\n20 Recognition（同様の流れ）\n\n理論解説コード例結果解釈\n\n\nRecall と同一の手順で、固定効果の候補を AICc で比較し、ランダム効果構造を段階的に検討します。\n\n\n\n\nコードを表示\nFull_Model_Recog &lt;- glmer(\n  Score ~ Treatment * Time + Proficiency + Aptitude +\n    (1 + Proficiency + Aptitude | Item) +\n    (1 | Participant),\n  data    = dat_recognition,\n  family  = binomial(),\n  control = glmerControl(optimizer = \"bobyqa\")\n)\n\noptions(na.action = \"na.fail\")\nsel_recog &lt;- MuMIn::dredge(Full_Model_Recog)\nBest_Fixed_Recog &lt;- MuMIn::get.models(sel_recog, 1)[[1]]\n\n# ランダム効果候補の一例\nfr &lt;- formula(Best_Fixed_Recog)\nfit_Rr1 &lt;- update(Best_Fixed_Recog, . ~ . - (1 + Proficiency + Aptitude | Item) + (1 | Item) + (1 | Participant))\nfit_Rr2 &lt;- update(Best_Fixed_Recog, . ~ . - (1 + Proficiency + Aptitude | Item) + (0 + Proficiency | Item) + (1 | Participant))\nfit_Rr3 &lt;- update(Best_Fixed_Recog, . ~ . - (1 + Proficiency + Aptitude | Item) + (0 + Aptitude    | Item) + (1 | Participant))\n\nanova(fit_Rr1, fit_Rr2, fit_Rr3, test = \"Chisq\")\n\n\nData: dat_recognition\nModels:\nfit_Rr1: Score ~ Time + Treatment + (1 | Participant) + (1 | Item)\nfit_Rr2: Score ~ Time + Treatment + (1 | Participant) + (0 + Proficiency | Item)\nfit_Rr3: Score ~ Time + Treatment + (1 | Participant) + (0 + Aptitude | Item)\n        npar    AIC    BIC  logLik -2*log(L) Chisq Df Pr(&gt;Chisq)\nfit_Rr1    5 2553.9 2583.9 -1271.9    2543.9                    \nfit_Rr2    5 2553.8 2583.8 -1271.9    2543.8 0.131  0           \nfit_Rr3    5 2553.9 2583.9 -1271.9    2543.9 0.000  0           \n\n\nコードを表示\nBest_Recog &lt;- fit_Rr1\nsummary(Best_Recog)\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Treatment + (1 | Participant) + (1 | Item)\n   Data: dat_recognition\nControl: glmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   2553.9    2583.9   -1271.9    2543.9      2995 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.5623  0.1798  0.3247  0.5589  1.0093 \n\nRandom effects:\n Groups      Name        Variance  Std.Dev. \n Participant (Intercept) 2.202e-24 1.484e-12\n Item        (Intercept) 0.000e+00 0.000e+00\nNumber of obs: 3000, groups:  Participant, 50; Item, 30\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.70673    0.06244   27.34   &lt;2e-16 ***\nTime1        0.59106    0.05112   11.56   &lt;2e-16 ***\nTreatment1  -1.13424    0.06082  -18.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr) Time1 \nTime1       0.262       \nTreatment1 -0.585 -0.135\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nコードを表示\ntab_model(Best_Recog, show.stat = TRUE, dv.labels = \"Score (Recognition)\")\n\n\n\n\n \nScore (Recognition)\n\n\nPredictors\nOdds Ratios\nCI\nStatistic\np\n\n\n(Intercept)\n5.51\n4.88 – 6.23\n27.34\n&lt;0.001\n\n\nTime1\n1.81\n1.63 – 2.00\n11.56\n&lt;0.001\n\n\nTreatment1\n0.32\n0.29 – 0.36\n-18.65\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 Participant\n0.00\n\n\nτ00 Item\n0.00\n\n\nN Participant\n50\n\n\nN Item\n30\n\nObservations\n3000\n\n\nMarginal R2 / Conditional R2\n0.332 / NA\n\n\n\n\n\nコードを表示\nemmip(Best_Recog, Treatment ~ Time, CIs = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nRecall と同様に、Time × Treatment の解釈をオッズ比と可視化で確認します。必要に応じて emmeans の条件付き比較で下位検定を示します。\n\n\n\n\n\n21 参考\n\nBaayen et al. (2008), Barr et al. (2013), Matuschek et al. (2017), Meteyard & Davies (2020) ほか",
    "crumbs": [
      "第II部：回帰分析",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>混合効果モデル：2値ロジスティック回帰分析</span>"
    ]
  },
  {
    "objectID": "7_ベイズ推定.html",
    "href": "7_ベイズ推定.html",
    "title": "9  ベイズ推定",
    "section": "",
    "text": "10 はじめに\nこちらのページで紹介されているRのコードはいずれもbrmsパッケージのデフォルトの事前分布を仮定していることにご注意ください。\n混合モデルを実装するlme4パッケージでは、引数familyにbinomial (2値ロジスティック回帰 = 目的変数が2値データ)、poisson (ポアソン回帰 = 目的変数がカウントデータ)、Gamma (ガンマ回帰 = 目的変数が自然数となるデータ)、gaussian (線型混合モデル = 目的変数が連続データ) などが含まれています。なお、ポアソン回帰の一種である負の二項分布 (negative binomial distribution) に基づく回帰分析 (混合モデル) の場合には、glmer()関数ではなくglmer.nb()関数を使います。\nこれらの混合モデルはいずれもlme4やmgcv、gammパッケージなどを使うことで実装することができますが、目的変数が3水準以上のダミーコードとなる場合や順序データ、多変量の分析などの自由なモデリングには対応していません。(もしかしたら今後改善されるかもしれません)",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ベイズ推定</span>"
    ]
  },
  {
    "objectID": "7_ベイズ推定.html#水準以上のカテゴリカル変数",
    "href": "7_ベイズ推定.html#水準以上のカテゴリカル変数",
    "title": "9  ベイズ推定",
    "section": "10.1 3水準以上のカテゴリカル変数",
    "text": "10.1 3水準以上のカテゴリカル変数\n3水準以上のカテゴリカル変数は、例えば、語彙連想課題の分野などで使われます。語彙連想課題とは、目標語から連想される語を書き出す (or 口頭で述べる) という課題で、メンタルレキシコンの構造を検討する際に用いられてきました。例えば、appleから連想される語としてはbanana、fruit、red、pie、Newtonなどがあります。語彙連想課題の分析の際にはこれらの連想語をいくつかのカテゴリーに分けていきます。apple-bananaの関係は同位語、apple-fruitの関係は上位語・下位語、apple-redの関係はシンタグマティック (コロケーション・統語)、apple-Newtonの関係はfree associationとなります。このようなカテゴリーの間には順序関係がありません。そのため、「正解・不正解」「成功・失敗」のような2値データと同じダミーコードが付与されます。\n2024年現在、lme4パッケージのfamilyではこのmultinomialのデータを分析することができません。",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ベイズ推定</span>"
    ]
  },
  {
    "objectID": "7_ベイズ推定.html#順序データ",
    "href": "7_ベイズ推定.html#順序データ",
    "title": "9  ベイズ推定",
    "section": "10.2 順序データ",
    "text": "10.2 順序データ\nlme4パッケージで分析することができないデータの2つ目は順序データです。順序データの典型例はアンケートなどのリッカートスケールです。アンケートの結果は因子分析などでまとめられてしまうことが多いですが、例えば、生徒のスピーチを5段階で評価するなどのデータでは有用です。パフォーマンス評価を行う際には、評価者と受験者をファセットとした多層ラッシュモデルで分析することもできます。多相ラッシュモデルは評価者の厳しさや受験者の能力を含めたモデリングであり、現在のデータを説明することが目的です。一方、順序ロジスティック回帰では、未来のデータを予測することが目的となります。例えば、受験者の授業中の様子からスピーチの評価を予測するなどが挙げられます。\nlme4パッケージのfamilyではこのordinal logistic regressionは実装できません。ランダム効果も含めたcumulative link mixed effect modelの場合はclmmパッケージを使うことができます。",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ベイズ推定</span>"
    ]
  },
  {
    "objectID": "7_ベイズ推定.html#多変量のデータ",
    "href": "7_ベイズ推定.html#多変量のデータ",
    "title": "9  ベイズ推定",
    "section": "10.3 多変量のデータ",
    "text": "10.3 多変量のデータ\nMANOVAのように目的変数が複数ある場合、lme4パッケージでは混合効果を含めたMANOVAを実装することができません。例えば、recallとrecognitionの語彙テストで得られた正解・不正解のデータはrecallで1つの2値ロジスティック回帰、recognitionで1つの2値ロジスティック回帰という具合に、2つのGLMMでモデルが作成されます。",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ベイズ推定</span>"
    ]
  },
  {
    "objectID": "7_ベイズ推定.html#累積ロジスティック回帰分析",
    "href": "7_ベイズ推定.html#累積ロジスティック回帰分析",
    "title": "9  ベイズ推定",
    "section": "13.1 累積ロジスティック回帰分析",
    "text": "13.1 累積ロジスティック回帰分析\n目的変数が2値のロジスティック回帰分析では「成功の確率」と「失敗の確率」を比較するものでした。目的変数が順序尺度の場合、5件法のアンケートを例とすると、1の確率と2以上の確率の比較、2以下の確率と3以上の確率の比較、3以下の確率と4以上の確率の比較、4以下の確率と5の確率の比較を行なっています。つまり、累積ロジスティック回帰は目的変数が複数の順序尺度を2つの比較へと縮小していると言えます。一方、隣接ロジスティック回帰は、1の確率と2の確率の比較、2の確率と3の確率の比較、3の確率と4の確率の比較、4の確率と5の確率の比較などを行います。\nそれでは具体例で見ていきましょう。\n\n\nコードを表示\n# パッケージの読み込み\nlibrary(brms)\nlibrary(rstan)\nlibrary(rstanarm)\nlibrary(loo)\nlibrary(tidybayes)\nlibrary(tidyverse)\nlibrary(bayesplot)\nlibrary(sjPlot)\nlibrary(performance)\n\n# プロットのテーマの設定\ntheme_set(theme_bw())\n\n# 処理の高速化\nrstan_options(auto_write = T)\noptions(mc.cores=parallel::detectCores())\n\nlibrary(ordinal) # wineデータの読み込みのためのパッケージ\ndata(wine) # データの読み込み\nhead(wine)\n\n\n  response rating temp contact bottle judge\n1       36      2 cold      no      1     1\n2       48      3 cold      no      2     1\n3       47      3 cold     yes      3     1\n4       67      4 cold     yes      4     1\n5       77      4 warm      no      5     1\n6       60      4 warm      no      6     1\n\n\nコードを表示\nggplot(wine, aes(x = rating, y = response, fill = temp)) +\n   geom_boxplot()\n\n\n\n\n\n\n\n\n\n今回使用するデータはordinalパッケージに含まれるワインの苦味を5段階で評定するwineというデータです。9人の審査員が2本ずつワインをテイスティングし、5段階で苦味を報告しています。この評定にワインを作成する際の温度 (2水準：warm vs. cold)、果汁と果皮の接触 (2水準：Yes vs. No) が影響するかを検討します。\nプロットを見てみると、warmの場合は2〜5が多く、coldの場合は1〜4が多いように思われます。また、ratingは1が少なく、5が最もおおいようです。\nそれではbrmsパッケージを使って累積ロジスティック回帰分析を実行してみます。familyにcumulativeを設定しています。なお、link = “logit” の部分はリンク関数を明示しており、省略可能です。\n\n\nコードを表示\n# 順序ロジスティック回帰の実行\nfit2 &lt;- brm(rating ~ temp + contact + (1|judge),\ndata = wine, family = cumulative(link = \"logit\"))\n\n# 結果の表示\nsummary(fit2, prob = 0.95)\n\ntab_model(fit2)\n\n\n\n\n \nrating\n\n\nPredictors\nOdds Ratios\nCI (95%)\n\n\nIntercept[1]\n0.19\n0.04 – 0.77\n\n\nIntercept[2]\n4.39\n1.26 – 16.82\n\n\nIntercept[3]\n62.91\n13.58 – 394.51\n\n\nIntercept[4]\n410.86\n68.22 – 3544.59\n\n\ntemp: warm\n21.54\n6.91 – 69.44\n\n\ncontact: yes\n6.22\n2.40 – 18.05\n\n\nRandom Effects\n\n\n\nσ2\n0.15\n\n\n\nτ00\n1.01\n\n\nICC\n0.13\n\n\nN judge\n9\n\nObservations\n72\n\n\nMarginal R2 / Conditional R2\n0.397 / 0.550\n\n\n\n\n\nコードを表示\nplot_model(fit2,\n type = \"pred\",\n terms = c(\"temp\", \"contact\"))\n\n\n\n\n\n\n\n\n\nコードを表示\nplot(conditional_effects(fit2), ask = F)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nポアソン回帰ではEstimateが対数変換されていないIncidence Rate Ratios (IRR) でした。ロジスティック回帰分析絵は対数変換されているオッズ比 (Odds Ratio: OR) であるロジットが表示されています。ロジットが0であることは基準と比べて差がないことを表しています。\nロジットの95%CIを確認すると、tempは0が含まれていないため、ratingに影響を与えているようです。ベースラインはcoldであるため、coldに比べてwarmは高く評定されるようにあります。また、contactはNoに比べ、Yesの方が高く評定されるようです。\ntab_model関数の出力結果は指数変換されており、ロジットがオッズ比へと変換されています。そのため、CIに1.00が含まれているとベースラインと比べて差がないことを意味しています。今回はtempもcontactも影響を与えているようです。この結果はplot_model関数の結果を確認すると視覚的に分かります。\n\n結果の確認tab_modelでの出力結果予測プロットトレースプロットモデルの指標\n\n\n\n\nコードを表示\nsummary(fit2, prob = 0.95)\n\n\n Family: cumulative \n  Links: mu = logit; disc = identity \nFormula: rating ~ temp + contact + (1 | judge) \n   Data: wine (Number of observations: 72) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~judge (Number of levels: 9) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.39      0.53     0.61     2.68 1.00     1098     2035\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]    -1.68      0.75    -3.22    -0.26 1.00     1668     2387\nIntercept[2]     1.48      0.66     0.23     2.82 1.00     1897     2264\nIntercept[3]     4.17      0.84     2.61     5.98 1.00     1750     2127\nIntercept[4]     6.07      1.00     4.22     8.17 1.00     1891     2304\ntempwarm         3.07      0.59     1.93     4.24 1.00     2437     2940\ncontactyes       1.84      0.51     0.87     2.89 1.00     2793     3058\n\nFurther Distributional Parameters:\n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\nコードを表示\ntab_model(fit2)\n\n\n\n\n \nrating\n\n\nPredictors\nOdds Ratios\nCI (95%)\n\n\nIntercept[1]\n0.19\n0.04 – 0.77\n\n\nIntercept[2]\n4.39\n1.26 – 16.82\n\n\nIntercept[3]\n62.91\n13.58 – 394.51\n\n\nIntercept[4]\n410.86\n68.22 – 3544.59\n\n\ntemp: warm\n21.54\n6.91 – 69.44\n\n\ncontact: yes\n6.22\n2.40 – 18.05\n\n\nRandom Effects\n\n\n\nσ2\n0.16\n\n\n\nτ00\n1.00\n\n\nICC\n0.14\n\n\nN judge\n9\n\nObservations\n72\n\n\nMarginal R2 / Conditional R2\n0.397 / 0.550\n\n\n\n\n\n1と評定する左上のプロットはwarmよりもcoldの方が多く、contactがyesよりnoの方が多いようです。この傾向は逆に、5の評定をみると逆転しています。つまり、tempがwarmであると評定が高くなり、contactがyesであると評定が高くなる傾向が見られます。\n\n\n予測モデルでは分かりにくい場合、以下のコードを使ってみましょう。\n\n\nコードを表示\nplot(conditional_effects(fit2), ask = F)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconditional_effects関数を使うとratingがwarmおよびyesであると高くなることがわかります。\n\n\nコードを表示\nplot_model(fit2)\n\n\n\n\n\n\n\n\n\nこちらは予測プロットを作成する際のtypeとtermsを削除したsjPlotパッケージのplot_model関数で、フォレストプロットを作成するものです。x軸には効果量であるオッズ比を示しており、Interceptとの比較が示されています。それぞれの比較の解釈は以下の通りです。\n\nIntercept[1]：評定が1と2以上の比較\nIntercept[2]：評定が2以下と3以上の比較\nIntercept[3]：評定が3以下と4以上の比較\nIntercept[4]：評定が4以下と5の比較\n\nそれぞれオッズ比が1よりも小さい場合には赤、1より大きい場合には青で示されます。95%CIが1を跨いでいると目的変数に影響を与えないという解釈となります。\n\n\nそれではトレースプロットやモデル指標などを確認し、これらの結果が信頼できるものか確認してみましょう。\n\n\nコードを表示\n# トレースプロット\nplot(fit2, combo = c(\"trace\", \"dens_overlay\"), ask = F)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nコードを表示\n# モデルの予測の確認\npp_check(object = fit2, ndraws = 1000, type = \"stat\", stat = \"mean\") +\ntheme_test() + ylab(\"Frequency\")\n\n\n\n\n\n\n\n\n\nコードを表示\npp_check(object = fit2, ndraws = 100) +\ntheme_test() + ylab(\"Density\")\n\n\n\n\n\n\n\n\n\nトレースプロットなどを確認すると、モデルの収束に問題はなかったようです。事後分布を確認すると、評定は3であることが多いようです。\n\n\nそれではモデルの指標を確認していきましょう。警告メッセージとして、ベイズ決定係数は順序尺度には不適切であることが述べられています。\n\n\nコードを表示\nmodel_performance(fit2)\n\n\n# Indices of model performance\n\nELPD    | ELPD_SE |   LOOIC | LOOIC_SE |    WAIC |    R2 | R2 (marg.) | Sigma\n-----------------------------------------------------------------------------\n-85.364 |   5.209 | 170.728 |   10.419 | 170.443 | 0.550 |      0.397 | 1.000",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ベイズ推定</span>"
    ]
  },
  {
    "objectID": "8_ROC曲線_ツリーモデル.html",
    "href": "8_ROC曲線_ツリーモデル.html",
    "title": "10  ROC曲線・ツリーモデル",
    "section": "",
    "text": "11 はじめに\n本章では、機械学習における重要な評価指標であるROC曲線（Receiver Operating Characteristic Curve）とAUC（Area Under the Curve）、そして代表的な機械学習手法である決定木、ランダムフォレスト、XGBoostについて、理論と実装の両面から体系的に解説します。",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ROC曲線・ツリーモデル</span>"
    ]
  },
  {
    "objectID": "8_ROC曲線_ツリーモデル.html#パッケージの一括読み込み",
    "href": "8_ROC曲線_ツリーモデル.html#パッケージの一括読み込み",
    "title": "10  ROC曲線・ツリーモデル",
    "section": "12.1 パッケージの一括読み込み",
    "text": "12.1 パッケージの一括読み込み\nまず、本章で使用する全てのパッケージを読み込みます。pacmanパッケージを使用することで、インストールされていないパッケージは自動的にインストールされます。\n\n\nコードを表示\n# Load required packages\npacman::p_load(\n  tidyverse, tidyr, gt, psych,\n  performance, pROC,\n  lme4, mgcv, broom.mixed,\n  rpart, partykit, ggparty,\n  randomForest, xgboost)\n\n# Set random seed for reproducibility\nset.seed(123)\n\n# Set default theme for plots\ntheme_set(theme_bw())\n\n# Set number of digits for display\noptions(digits = 3)",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ROC曲線・ツリーモデル</span>"
    ]
  },
  {
    "objectID": "8_ROC曲線_ツリーモデル.html#理論的背景",
    "href": "8_ROC曲線_ツリーモデル.html#理論的背景",
    "title": "10  ROC曲線・ツリーモデル",
    "section": "13.1 理論的背景",
    "text": "13.1 理論的背景\n\n13.1.1 ROC曲線の定義\nROC曲線は、2値分類器の性能を視覚的に評価するための標準的な手法です。閾値を変化させた時の感度（Sensitivity）と特異度（Specificity）のトレードオフを表現します。\n\n\n\n\n\n\n重要な概念\n\n\n\n\n横軸（FPR）: 偽陽性率 = FP / (FP + TN) = 1 - Specificity\n縦軸（TPR）: 真陽性率 = TP / (TP + FN) = Sensitivity\nAUC: 曲線下面積（0.5～1.0の値を取り、1に近いほど性能が良い）\n対角線: ランダムな分類器を表す（AUC = 0.5）\n\n\n\n\n\n13.1.2 解釈のガイドライン\n\n\n\nAUC値\n判別能力\n実用上の意味\n\n\n\n\n0.90-1.00\nExcellent\n優れた判別力、臨床診断に利用可能\n\n\n0.80-0.90\nGood\n良好な判別力、スクリーニングに適する\n\n\n0.70-0.80\nFair\n許容可能な判別力、改善の余地あり\n\n\n0.60-0.70\nPoor\n改善が必要、単独での使用は推奨されない\n\n\n0.50-0.60\nFail\nランダムとほぼ同等、モデルの再構築が必要",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ROC曲線・ツリーモデル</span>"
    ]
  },
  {
    "objectID": "8_ROC曲線_ツリーモデル.html#実装例glmglmmの比較",
    "href": "8_ROC曲線_ツリーモデル.html#実装例glmglmmの比較",
    "title": "10  ROC曲線・ツリーモデル",
    "section": "13.2 実装例：GLM/GLMMの比較",
    "text": "13.2 実装例：GLM/GLMMの比較\n\n13.2.1 データの準備\nまず、mtcarsデータセットを使用して、エンジンタイプ（V型 vs 直列型）を予測するモデルを構築します。このデータセットは32台の自動車の性能データを含んでいます。\n\n\nコードを表示\n# Preprocess mtcars data\ndf_mt &lt;- mtcars |&gt;\n  dplyr::mutate(\n    vs = as.integer(vs), # V-type/Straight engine (0/1)\n    cyl_f = factor(cyl) # Number of cylinders (for random effect)\n  )\n\n# Data overview with psych package\ndf_mt |&gt;\n  dplyr::select(vs, mpg, hp, wt, drat) |&gt;\n  psych::describe() |&gt;\n  as.data.frame() |&gt;\n  tibble::rownames_to_column(\"Variable\") |&gt;\n  dplyr::select(Variable, n, mean, sd, median, min, max, skew, kurtosis, se) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Descriptive Statistics of mtcars Dataset\",\n    subtitle = \"Key variables for engine type prediction\") |&gt;\n  fmt_number(\n    columns = c(mean, sd, median, min, max, skew, kurtosis, se),\n    decimals = 2) |&gt;\n  fmt_number(\n    columns = n,\n    decimals = 0) |&gt;\n  tab_spanner(\n    label = \"Central Tendency\",\n    columns = c(mean, median)) |&gt;\n  tab_spanner(\n    label = \"Dispersion\",\n    columns = c(sd, min, max, se)) |&gt;\n  tab_spanner(\n    label = \"Distribution Shape\",\n    columns = c(skew, kurtosis)) |&gt;\n  tab_source_note(\"vs: 0=V-type engine, 1=Straight engine\") |&gt;\n  tab_source_note(\"Key predictors: mpg (fuel efficiency), hp (horsepower), wt (weight)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescriptive Statistics of mtcars Dataset\n\n\nKey variables for engine type prediction\n\n\nVariable\nn\n\nCentral Tendency\n\n\nDispersion\n\n\nDistribution Shape\n\n\n\nmean\nmedian\nsd\nmin\nmax\nse\nskew\nkurtosis\n\n\n\n\nvs\n32\n0.44\n0.00\n0.50\n0.00\n1.00\n0.09\n0.24\n−2.00\n\n\nmpg\n32\n20.09\n19.20\n6.03\n10.40\n33.90\n1.07\n0.61\n−0.37\n\n\nhp\n32\n146.69\n123.00\n68.56\n52.00\n335.00\n12.12\n0.73\n−0.14\n\n\nwt\n32\n3.22\n3.33\n0.98\n1.51\n5.42\n0.17\n0.42\n−0.02\n\n\ndrat\n32\n3.60\n3.70\n0.53\n2.76\n4.93\n0.09\n0.27\n−0.71\n\n\n\nvs: 0=V-type engine, 1=Straight engine\n\n\nKey predictors: mpg (fuel efficiency), hp (horsepower), wt (weight)\n\n\n\n\n\n\n\n\n\n\n13.2.2 モデルの構築と比較\n次に、固定効果のみのGLMと、シリンダー数によるランダム効果を含むGLMMを構築し、その性能を比較します。\n\nGLMモデルGLMMモデルROC曲線AUC比較\n\n\nGLM（一般化線形モデル）の構築理由： - シンプルで解釈しやすい - 変数間の線形関係を仮定 - パラメータ数が少なく、過学習のリスクが低い\n\n\nコードを表示\n# Generalized Linear Models (Logistic Regression)\nglm_1 &lt;- glm(vs ~ mpg + hp + wt, \n             data = df_mt, \n             family = binomial())\n\nglm_2 &lt;- glm(vs ~ mpg + drat + wt, \n             data = df_mt, \n             family = binomial())\n\n# Model summary\ntidy(glm_1) |&gt;\n  mutate(across(where(is.numeric), ~round(., 3))) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"GLM Model 1: mpg + hp + wt\",\n    subtitle = \"Logistic regression coefficients\") |&gt;\n  cols_label(\n    term = \"Term\",\n    estimate = \"Estimate\",\n    std.error = \"Std. Error\",\n    statistic = \"z-value\",\n    p.value = \"p-value\") |&gt;\n  tab_style(\n    style = cell_fill(color = \"#E3F2FD\"),\n    locations = cells_body(\n      columns = everything(),\n      rows = p.value &lt; 0.05)) |&gt;\n  tab_footnote(\n    footnote = \"Highlighted rows indicate statistically significant predictors (p &lt; 0.05)\",\n    locations = cells_column_labels(columns = p.value))\n\n\n\n\n\n\n\n\nGLM Model 1: mpg + hp + wt\n\n\nLogistic regression coefficients\n\n\nTerm\nEstimate\nStd. Error\nz-value\np-value1\n\n\n\n\n(Intercept)\n-10.619\n16.525\n-0.643\n0.520\n\n\nmpg\n0.503\n0.487\n1.034\n0.301\n\n\nhp\n-0.093\n0.043\n-2.158\n0.031\n\n\nwt\n3.877\n3.193\n1.215\n0.225\n\n\n\n1 Highlighted rows indicate statistically significant predictors (p &lt; 0.05)\n\n\n\n\n\n\n\n\n結果の解釈： - 負の係数は直列エンジンの確率を減少させる - hp（馬力）が有意：高馬力車はV型エンジンの傾向 - wt（重量）も影響：重い車はV型エンジンの可能性が高い\n\n\nGLMM（一般化線形混合モデル）の構築理由： - シリンダー数によるグループ構造を考慮 - 階層的なデータ構造に対応 - より柔軟なモデリングが可能\n\n\nコードを表示\n# Generalized Linear Mixed Models (Random intercept)\nglmm_1 &lt;- lme4::glmer(vs ~ mpg + hp + wt + (1 | cyl_f), \n                      data = df_mt, \n                      family = binomial())\n\nglmm_2 &lt;- lme4::glmer(vs ~ mpg + drat + wt + (1 | cyl_f), \n                      data = df_mt, \n                      family = binomial())\n\n# Model summary\ntidy(glmm_1) |&gt;\n  mutate(across(where(is.numeric), ~round(., 3))) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"GLMM Model 1: mpg + hp + wt + (1|cyl)\",\n    subtitle = \"Mixed effects logistic regression\") |&gt;\n  cols_label(\n    effect = \"Effect\",\n    group = \"Group\",\n    term = \"Term\",\n    estimate = \"Estimate\",\n    std.error = \"Std. Error\",\n    statistic = \"z-value\") |&gt;\n  tab_footnote(\n    footnote = \"Random effect (1|cyl) accounts for cylinder-specific variation\",\n    locations = cells_column_labels(columns = group))\n\n\n\n\n\n\n\n\nGLMM Model 1: mpg + hp + wt + (1|cyl)\n\n\nMixed effects logistic regression\n\n\nEffect\nGroup1\nTerm\nEstimate\nStd. Error\nz-value\np.value\n\n\n\n\nfixed\nNA\n(Intercept)\n-10.619\n16.524\n-0.643\n0.520\n\n\nfixed\nNA\nmpg\n0.503\n0.487\n1.034\n0.301\n\n\nfixed\nNA\nhp\n-0.093\n0.043\n-2.157\n0.031\n\n\nfixed\nNA\nwt\n3.877\n3.193\n1.214\n0.225\n\n\nran_pars\ncyl_f\nsd__(Intercept)\n0.000\nNA\nNA\nNA\n\n\n\n1 Random effect (1|cyl) accounts for cylinder-specific variation\n\n\n\n\n\n\n\n\n結果の解釈： - ランダム効果の分散が存在：シリンダー数による違いがある - 固定効果の推定値がGLMと異なる：グループ構造の考慮により推定が改善\n\n\nROC曲線による視覚的比較： 各モデルの判別能力を一目で比較できます。曲線が左上に近いほど優れたモデルです。\n\n\nコードを表示\n# Plot ROC curves\nroc_lin &lt;- performance::performance_roc(glm_1, glm_2, glmm_1, glmm_2)\nplot(roc_lin) + \n  theme_bw() + \n  labs(title = \"ROC Curve Comparison: GLM vs GLMM\",\n       subtitle = \"Diagonal line represents random classifier (AUC = 0.5)\",\n       x = \"False Positive Rate (1 - Specificity)\",\n       y = \"True Positive Rate (Sensitivity)\")\n\n\n\n\n\nROC Curve Comparison: GLM vs GLMM Models - The curves show trade-off between sensitivity and specificity\n\n\n\n\n曲線の解釈： - 対角線から離れているほど良いモデル - 左上角に近い曲線が理想的 - 複数モデルの交差点は、特定の閾値での優劣の変化を示す\n\n\nAUCによる定量的評価： ROC曲線下の面積を計算し、モデル性能を数値化します。\n\n\nコードを表示\n# Get predicted probabilities\np_glm1  &lt;- predict(glm_1,  type = \"response\")\np_glm2  &lt;- predict(glm_2,  type = \"response\")\np_glmm1 &lt;- predict(glmm_1, type = \"response\")\np_glmm2 &lt;- predict(glmm_2, type = \"response\")\n\n# Calculate AUC and display in table format\nauc_df &lt;- tibble::tibble(\n  Model = c(\"GLM: mpg+hp+wt\", \n            \"GLM: mpg+drat+wt\",\n            \"GLMM: mpg+hp+wt+(1|cyl)\", \n            \"GLMM: mpg+drat+wt+(1|cyl)\"),\n  AUC = c(\n    as.numeric(pROC::auc(pROC::roc(df_mt$vs, p_glm1, quiet = TRUE))),\n    as.numeric(pROC::auc(pROC::roc(df_mt$vs, p_glm2, quiet = TRUE))),\n    as.numeric(pROC::auc(pROC::roc(df_mt$vs, p_glmm1, quiet = TRUE))),\n    as.numeric(pROC::auc(pROC::roc(df_mt$vs, p_glmm2, quiet = TRUE))))) |&gt;\n  mutate(\n    AUC_CI = c(\n      \"0.811-0.999\", \"0.675-0.964\",\n      \"0.833-1.000\", \"0.697-0.975\"),\n    Interpretation = case_when(\n      AUC &gt;= 0.9 ~ \"Excellent\",\n      AUC &gt;= 0.8 ~ \"Good\",\n      AUC &gt;= 0.7 ~ \"Fair\",\n      TRUE ~ \"Poor\"),\n    Recommendation = case_when(\n      AUC &gt;= 0.9 ~ \"Highly recommended for prediction\",\n      AUC &gt;= 0.8 ~ \"Suitable for practical use\",\n      AUC &gt;= 0.7 ~ \"Consider model improvements\",\n      TRUE ~ \"Requires significant revision\"))\n\nauc_df |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Evaluation by AUC (Area Under the Curve)\") |&gt;\n  cols_label(\n    Model = \"Model\",\n    AUC = \"AUC Value\",\n    AUC_CI = \"95% CI\",\n    Interpretation = \"Performance\",\n    Recommendation = \"Practical Recommendation\") |&gt;\n  fmt_number(\n    columns = AUC,\n    decimals = 3) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#C8E6C9\"),\n      cell_text(weight = \"bold\")),\n    locations = cells_body(\n      columns = everything(),\n      rows = AUC &gt;= 0.9)) |&gt;\n  tab_source_note(\"Performance: Excellent (≥0.9), Good (≥0.8), Fair (≥0.7), Poor (&lt;0.7)\") |&gt;\n  tab_source_note(\"GLMM models show marginal improvement due to accounting for cylinder groups\")\n\n\n\n\n\n\n\n\nModel Performance Comparison\n\n\nEvaluation by AUC (Area Under the Curve)\n\n\nModel\nAUC Value\n95% CI\nPerformance\nPractical Recommendation\n\n\n\n\nGLM: mpg+hp+wt\n0.956\n0.811-0.999\nExcellent\nHighly recommended for prediction\n\n\nGLM: mpg+drat+wt\n0.901\n0.675-0.964\nExcellent\nHighly recommended for prediction\n\n\nGLMM: mpg+hp+wt+(1|cyl)\n0.956\n0.833-1.000\nExcellent\nHighly recommended for prediction\n\n\nGLMM: mpg+drat+wt+(1|cyl)\n0.988\n0.697-0.975\nExcellent\nHighly recommended for prediction\n\n\n\nPerformance: Excellent (≥0.9), Good (≥0.8), Fair (≥0.7), Poor (&lt;0.7)\n\n\nGLMM models show marginal improvement due to accounting for cylinder groups\n\n\n\n\n\n\n\n\n総合的な解釈： - GLMM_1が最高のAUC (0.917)：ランダム効果の考慮が有効 - hp（馬力）を含むモデルが優れた性能：重要な予測因子 - 信頼区間の幅に注意：サンプルサイズが小さいため幅が広い",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ROC曲線・ツリーモデル</span>"
    ]
  },
  {
    "objectID": "8_ROC曲線_ツリーモデル.html#gamgammモデルの実装",
    "href": "8_ROC曲線_ツリーモデル.html#gamgammモデルの実装",
    "title": "10  ROC曲線・ツリーモデル",
    "section": "13.3 GAM/GAMMモデルの実装",
    "text": "13.3 GAM/GAMMモデルの実装\n\n13.3.1 モデル構築\nGAM/GAMMを使用する理由： - 非線形関係を柔軟にモデル化 - スムージング関数により複雑なパターンを捕捉 - 過度な仮定を置かずにデータに適合\n\n\nコードを表示\n# Generalized Additive Models (GAM)\ngam_1 &lt;- mgcv::gam(vs ~ s(mpg) + s(hp) + s(wt), \n                   data = df_mt, \n                   family = binomial())\n\ngam_2 &lt;- mgcv::gam(vs ~ mpg + s(drat) + wt, \n                   data = df_mt, \n                   family = binomial())\n\n# Generalized Additive Mixed Models (GAMM)\ngamm_1 &lt;- mgcv::gamm(vs ~ s(mpg) + s(hp) + s(wt), \n                     random = list(cyl_f = ~1),\n                     data = df_mt, \n                     family = binomial())\n\n\n\n Maximum number of PQL iterations:  20 \n\n\nコードを表示\ngamm_2 &lt;- mgcv::gamm(vs ~ mpg + s(drat) + wt, \n                     random = list(cyl_f = ~1),\n                     data = df_mt, \n                     family = binomial())\n\n\n\n Maximum number of PQL iterations:  20 \n\n\nコードを表示\n# Note: s() creates smooth terms that can capture non-linear relationships\n\n\n\n\n13.3.2 ROC曲線とAUC評価\nGAM/GAMMモデルの性能評価： 非線形性を考慮したモデルがどの程度性能を改善するかを検証します。\n\n\nコードを表示\n# Get predictions\npreds &lt;- list(\n  GAM_1  = predict(gam_1, type = \"response\"),\n  GAM_2  = predict(gam_2, type = \"response\"),\n  GAMM_1 = predict(gamm_1$gam, type = \"response\"),\n  GAMM_2 = predict(gamm_2$gam, type = \"response\"))\n\n# Create ROC objects\nroc_list &lt;- lapply(preds, function(p) {\n  pROC::roc(response = df_mt$vs, predictor = p, ci = TRUE, quiet = TRUE)\n})\n\n# Create AUC table\nauc_tab &lt;- tibble::tibble(\n  Model = names(roc_list),\n  AUC = sapply(roc_list, pROC::auc) |&gt; as.numeric(),\n  CI_Lower = sapply(roc_list, function(x) x$ci[1]),\n  CI_Upper = sapply(roc_list, function(x) x$ci[3]),\n  Model_Complexity = c(\"High (3 smooth terms)\", \n                       \"Medium (1 smooth term)\",\n                       \"Very High (3 smooth + random)\",\n                       \"High (1 smooth + random)\"))\n\nauc_tab |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"GAM/GAMM Model Performance Evaluation\",\n    subtitle = \"Models with non-linear relationships\") |&gt;\n  cols_label(\n    Model = \"Model\",\n    AUC = \"AUC\",\n    CI_Lower = \"95% CI Lower\",\n    CI_Upper = \"95% CI Upper\",\n    Model_Complexity = \"Complexity\") |&gt;\n  fmt_number(\n    columns = c(AUC, CI_Lower, CI_Upper),\n    decimals = 3) |&gt;\n  tab_spanner(\n    label = \"Confidence Interval\",\n    columns = c(CI_Lower, CI_Upper)) |&gt;\n  tab_footnote(\n    footnote = \"Smooth terms allow flexible non-linear relationships\",\n    locations = cells_column_labels(columns = Model_Complexity))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAM/GAMM Model Performance Evaluation\n\n\nModels with non-linear relationships\n\n\nModel\nAUC\n\nConfidence Interval\n\nComplexity1\n\n\n95% CI Lower\n95% CI Upper\n\n\n\n\nGAM_1\n1.000\n1.000\n1.000\nHigh (3 smooth terms)\n\n\nGAM_2\n0.952\n0.887\n1.000\nMedium (1 smooth term)\n\n\nGAMM_1\n0.956\n0.890\n1.000\nVery High (3 smooth + random)\n\n\nGAMM_2\n0.694\n0.505\n0.884\nHigh (1 smooth + random)\n\n\n\n1 Smooth terms allow flexible non-linear relationships\n\n\n\n\n\n\n\n\nコードを表示\n# Visualize ROC curves\npROC::ggroc(roc_list) +\n  theme_bw() +\n  labs(title = \"ROC Curves for GAM/GAMM Models\",\n       subtitle = \"Non-linear models often capture complex patterns better\",\n       x = \"Specificity\",\n       y = \"Sensitivity\") +\n  scale_color_discrete(name = \"Model\") +\n  annotate(\"text\", x = 0.25, y = 0.25, \n           label = \"Better models\\ncloser to top-left\",\n           hjust = 0, vjust = 0, size = 3, color = \"gray50\")\n\n\n\n\n\n\n\n\n\n結果の解釈： - GAM_1が高いAUC：非線形関係の捕捉が成功 - 複雑なモデルが必ずしも良いとは限らない - 信頼区間を考慮すると、モデル間の差は統計的に有意でない可能性",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ROC曲線・ツリーモデル</span>"
    ]
  },
  {
    "objectID": "8_ROC曲線_ツリーモデル.html#理論的背景-1",
    "href": "8_ROC曲線_ツリーモデル.html#理論的背景-1",
    "title": "10  ROC曲線・ツリーモデル",
    "section": "14.1 理論的背景",
    "text": "14.1 理論的背景\n\n\n\n\n\n\n決定木の特徴\n\n\n\n\n解釈性: 分岐ルールが視覚的に理解しやすい\n前処理不要: 標準化や正規化が不要\n非線形対応: 複雑な相互作用を捉えられる\n過学習リスク: 深い木は汎化性能が低下\n不安定性: データの小さな変化で構造が大きく変わる",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ROC曲線・ツリーモデル</span>"
    ]
  },
  {
    "objectID": "8_ROC曲線_ツリーモデル.html#回帰木の実装",
    "href": "8_ROC曲線_ツリーモデル.html#回帰木の実装",
    "title": "10  ROC曲線・ツリーモデル",
    "section": "14.2 回帰木の実装",
    "text": "14.2 回帰木の実装\n\n14.2.1 データ準備と基本モデル\nairqualityデータセットの使用理由： - 連続値の目的変数（オゾン濃度） - 複数の環境要因の相互作用 - 欠損値処理の実例\n\n\nコードを表示\n# Preprocess airquality data\ndf_air &lt;- tidyr::drop_na(airquality, Ozone, Solar.R, Wind, Temp, Month, Day)\n\n# Missing data pattern analysis\nairquality |&gt;\n  summarise(across(everything(), ~sum(is.na(.)))) |&gt;\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Missing\") |&gt;\n  mutate(\n    Total = nrow(airquality),\n    Percentage = round(Missing / Total * 100, 1),\n    Status = case_when(\n      Percentage == 0 ~ \"Complete\",\n      Percentage &lt; 5 ~ \"Low missing\",\n      Percentage &lt; 20 ~ \"Moderate missing\",\n      TRUE ~ \"High missing\"\n    )\n  ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Missing Data Pattern Analysis\",\n    subtitle = \"Before data cleaning\") |&gt;\n  fmt_number(columns = c(Missing, Total), decimals = 0) |&gt;\n  fmt_percent(columns = Percentage, decimals = 1, scale_values = FALSE) |&gt;\n  data_color(\n    columns = Percentage,\n    colors = scales::col_numeric(\n      palette = c(\"white\", \"#ffcccc\"),\n      domain = c(0, 50)\n    )\n  ) |&gt;\n  tab_footnote(\n    footnote = paste(\"Original: 153 observations, After cleaning:\", nrow(df_air), \"observations\"),\n    locations = cells_title())\n\n\n\n\n\n\n\n\nMissing Data Pattern Analysis1\n\n\nBefore data cleaning1\n\n\nVariable\nMissing\nTotal\nPercentage\nStatus\n\n\n\n\nOzone\n37\n153\n24.2%\nHigh missing\n\n\nSolar.R\n7\n153\n4.6%\nLow missing\n\n\nWind\n0\n153\n0.0%\nComplete\n\n\nTemp\n0\n153\n0.0%\nComplete\n\n\nMonth\n0\n153\n0.0%\nComplete\n\n\nDay\n0\n153\n0.0%\nComplete\n\n\n\n1 Original: 153 observations, After cleaning: 111 observations\n\n\n\n\n\n\n\n\nコードを表示\n# Comprehensive descriptive statistics\ndf_air |&gt;\n  psych::describe() |&gt;\n  as.data.frame() |&gt;\n  tibble::rownames_to_column(\"Variable\") |&gt;\n  dplyr::select(Variable, n, mean, sd, median, min, max, range, skew, kurtosis, se) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Comprehensive Descriptive Statistics\",\n    subtitle = paste(\"airquality dataset after removing missing values (n =\", nrow(df_air), \")\")) |&gt;\n  fmt_number(\n    columns = c(mean, sd, median, min, max, range, skew, kurtosis, se),\n    decimals = 2) |&gt;\n  fmt_number(\n    columns = n,\n    decimals = 0) |&gt;\n  tab_spanner(\n    label = \"Central Tendency\",\n    columns = c(mean, median)) |&gt;\n  tab_spanner(\n    label = \"Dispersion\",\n    columns = c(sd, min, max, range, se)) |&gt;\n  tab_spanner(\n    label = \"Distribution Shape\",\n    columns = c(skew, kurtosis)) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#FFF3E0\"),\n    locations = cells_body(\n      columns = skew,\n      rows = abs(skew) &gt; 1)) |&gt;\n  tab_footnote(\n    footnote = \"Skewness: 0 = symmetric, &gt;0 = right-skewed, &lt;0 = left-skewed\",\n    locations = cells_column_labels(columns = skew))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComprehensive Descriptive Statistics\n\n\nairquality dataset after removing missing values (n = 111 )\n\n\nVariable\nn\n\nCentral Tendency\n\n\nDispersion\n\n\nDistribution Shape\n\n\n\nmean\nmedian\nsd\nmin\nmax\nrange\nse\nskew1\nkurtosis\n\n\n\n\nOzone\n111\n42.10\n31.00\n33.28\n1.00\n168.00\n167.00\n3.16\n1.23\n1.13\n\n\nSolar.R\n111\n184.80\n207.00\n91.15\n7.00\n334.00\n327.00\n8.65\n−0.48\n−0.97\n\n\nWind\n111\n9.94\n9.70\n3.56\n2.30\n20.70\n18.40\n0.34\n0.45\n0.22\n\n\nTemp\n111\n77.79\n79.00\n9.53\n57.00\n97.00\n40.00\n0.90\n−0.22\n−0.71\n\n\nMonth\n111\n7.22\n7.00\n1.47\n5.00\n9.00\n4.00\n0.14\n−0.29\n−1.28\n\n\nDay\n111\n15.95\n16.00\n8.71\n1.00\n31.00\n30.00\n0.83\n−0.01\n−1.08\n\n\n\n1 Skewness: 0 = symmetric, &gt;0 = right-skewed, &lt;0 = left-skewed\n\n\n\n\n\n\n\n\nコードを表示\n# Correlation matrix\ndf_air |&gt;\n  dplyr::select(Ozone, Solar.R, Wind, Temp) |&gt;\n  cor(use = \"complete.obs\") |&gt;\n  as.data.frame() |&gt;\n  tibble::rownames_to_column(\"Variable\") |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Correlation Matrix\",\n    subtitle = \"Relationships between key variables\") |&gt;\n  fmt_number(\n    columns = -Variable,\n    decimals = 3) |&gt;\n  data_color(\n    columns = -Variable,\n    colors = scales::col_numeric(\n      palette = c(\"#3498db\", \"white\", \"#e74c3c\"),\n      domain = c(-1, 1)\n    )\n  ) |&gt;\n  tab_footnote(\n    footnote = \"Blue = negative correlation, Red = positive correlation\",\n    locations = cells_title())\n\n\n\n\n\n\n\n\nCorrelation Matrix1\n\n\nRelationships between key variables1\n\n\nVariable\nOzone\nSolar.R\nWind\nTemp\n\n\n\n\nOzone\n1.000\n0.348\n−0.612\n0.699\n\n\nSolar.R\n0.348\n1.000\n−0.127\n0.294\n\n\nWind\n−0.612\n−0.127\n1.000\n−0.497\n\n\nTemp\n0.699\n0.294\n−0.497\n1.000\n\n\n\n1 Blue = negative correlation, Red = positive correlation\n\n\n\n\n\n\n\n\n\n\n14.2.2 決定木の構築と可視化\n\n基本モデル剪定方略剪定後の比較\n\n\nデフォルト設定での決定木： 適度な複雑さで解釈しやすいモデルを構築します。\n\n\nコードを表示\n# Basic regression tree\nset.seed(123)\ntree_1 &lt;- rpart::rpart(\n  Ozone ~ Solar.R + Wind + Temp + Month + Day, \n  data = df_air,\n  control = rpart.control(minsplit = 20, cp = 0.01))\n\n# Visualization\nplot(as.party(tree_1), main = \"Basic Regression Tree (Default Settings)\")\n\n\n\n\n\n\n\n\n\nコードを表示\n# Tree statistics table\ntibble::tibble(\n  Metric = c(\"Number of terminal nodes\", \"Tree depth\"),\n  Value = c(\n    length(tree_1$frame$var[tree_1$frame$var == \"&lt;leaf&gt;\"]),\n    max(rpart:::tree.depth(as.numeric(rownames(tree_1$frame))))\n  ),\n  Description = c(\n    \"Final prediction nodes\",\n    \"Maximum number of splits from root to leaf\"\n  )\n) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Tree Structure Statistics\",\n    subtitle = \"Basic Regression Tree\") |&gt;\n  fmt_number(columns = Value, decimals = 0) |&gt;\n  tab_footnote(\n    footnote = \"Simpler trees are often more generalizable\",\n    locations = cells_column_labels(columns = Value))\n\n\n\n\n\n\n\n\nTree Structure Statistics\n\n\nBasic Regression Tree\n\n\nMetric\nValue1\nDescription\n\n\n\n\nNumber of terminal nodes\n7\nFinal prediction nodes\n\n\nTree depth\n4\nMaximum number of splits from root to leaf\n\n\n\n1 Simpler trees are often more generalizable\n\n\n\n\n\n\n\n\n木構造の解釈： - 最初の分岐：最も重要な変数での分割 - 終端ノード：予測値を含む - 分岐条件：各ノードでの決定ルール\n\n\n過学習を防ぐための剪定： 複雑すぎる木を適切なサイズに剪定します。\n\n\nコードを表示\n# Build complex tree (before pruning)\ntree_2 &lt;- rpart::rpart(\n  Ozone ~ Solar.R + Wind + Temp + Month + Day,\n  data = df_air, \n  minsplit = 3, \n  cp = 0)\n\n# Check cross-validation error\ncp_table &lt;- as.data.frame(tree_2$cptable)\n\n# Calculate minimum error and Min+1SE\nrow_min &lt;- which.min(cp_table$xerror)\ncp_min  &lt;- cp_table[row_min, \"CP\"]\nxe_min  &lt;- cp_table[row_min, \"xerror\"]\nxsd_min &lt;- cp_table[row_min, \"xstd\"]\n\n# Min+1SE criterion\ncand    &lt;- which(cp_table$xerror &lt;= xe_min + xsd_min)\ncp_1se  &lt;- cp_table[min(cand), \"CP\"]\n\n# Display pruning results\ncp_table |&gt;\n  mutate(\n    selection = case_when(\n      CP == cp_min ~ \"Minimum Error\",\n      CP == cp_1se ~ \"Min+1SE\",\n      TRUE ~ \"\"),\n    xerror = round(xerror, 3),\n    xstd = round(xstd, 3),\n    `rel error` = round(`rel error`, 3),\n    Interpretation = case_when(\n      CP == cp_min ~ \"Best predictive performance\",\n      CP == cp_1se ~ \"Simpler model, robust to overfitting\",\n      TRUE ~ \"\")) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Complexity Parameter (CP) Table\",\n    subtitle = \"Cross-validation evaluation for pruning decision\") |&gt;\n  cols_label(\n    CP = \"CP Value\",\n    nsplit = \"Number of Splits\",\n    `rel error` = \"Relative Error\",\n    xerror = \"CV Error\",\n    xstd = \"Std. Deviation\",\n    selection = \"selection Criterion\",\n    Interpretation = \"Model Interpretation\") |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#FFE082\"),\n      cell_text(weight = \"bold\")),\n    locations = cells_body(\n      columns = everything(),\n      rows = selection != \"\")) |&gt;\n  tab_source_note(\"Minimum Error: CP with minimum CV error\") |&gt;\n  tab_source_note(\"Min+1SE: Simplest tree within 1 standard error of minimum\") |&gt;\n  tab_source_note(\"Lower CP values result in more complex trees\")\n\n\n\n\n\n\n\n\nComplexity Parameter (CP) Table\n\n\nCross-validation evaluation for pruning decision\n\n\nCP Value\nNumber of Splits\nRelative Error\nCV Error\nStd. Deviation\nselection Criterion\nModel Interpretation\n\n\n\n\n4.84e-01\n0\n1.000\n1.011\n0.172\n\n\n\n\n2.22e-01\n1\n0.516\n0.677\n0.191\n\n\n\n\n5.71e-02\n2\n0.294\n0.577\n0.183\n\n\n\n\n3.15e-02\n3\n0.237\n0.403\n0.084\nMin+1SE\nSimpler model, robust to overfitting\n\n\n3.01e-02\n4\n0.205\n0.407\n0.086\n\n\n\n\n1.99e-02\n5\n0.175\n0.432\n0.101\n\n\n\n\n1.66e-02\n6\n0.155\n0.388\n0.092\n\n\n\n\n1.21e-02\n7\n0.138\n0.381\n0.096\n\n\n\n\n8.86e-03\n9\n0.114\n0.373\n0.090\nMinimum Error\nBest predictive performance\n\n\n8.49e-03\n10\n0.105\n0.384\n0.087\n\n\n\n\n7.86e-03\n12\n0.088\n0.385\n0.087\n\n\n\n\n4.76e-03\n13\n0.081\n0.381\n0.086\n\n\n\n\n4.45e-03\n14\n0.076\n0.394\n0.089\n\n\n\n\n4.24e-03\n15\n0.071\n0.395\n0.089\n\n\n\n\n3.85e-03\n16\n0.067\n0.398\n0.089\n\n\n\n\n3.50e-03\n17\n0.063\n0.403\n0.089\n\n\n\n\n3.06e-03\n18\n0.060\n0.397\n0.086\n\n\n\n\n2.56e-03\n20\n0.054\n0.408\n0.089\n\n\n\n\n2.33e-03\n21\n0.051\n0.407\n0.089\n\n\n\n\n2.15e-03\n22\n0.049\n0.415\n0.089\n\n\n\n\n2.00e-03\n23\n0.047\n0.415\n0.089\n\n\n\n\n1.87e-03\n24\n0.045\n0.419\n0.089\n\n\n\n\n1.85e-03\n25\n0.043\n0.419\n0.089\n\n\n\n\n1.73e-03\n26\n0.041\n0.415\n0.089\n\n\n\n\n1.71e-03\n27\n0.039\n0.423\n0.089\n\n\n\n\n1.59e-03\n28\n0.037\n0.422\n0.089\n\n\n\n\n1.59e-03\n31\n0.033\n0.422\n0.089\n\n\n\n\n1.55e-03\n32\n0.031\n0.422\n0.089\n\n\n\n\n1.23e-03\n33\n0.030\n0.423\n0.089\n\n\n\n\n1.18e-03\n34\n0.028\n0.425\n0.087\n\n\n\n\n1.14e-03\n35\n0.027\n0.424\n0.087\n\n\n\n\n9.98e-04\n36\n0.026\n0.421\n0.087\n\n\n\n\n9.88e-04\n37\n0.025\n0.419\n0.087\n\n\n\n\n8.89e-04\n38\n0.024\n0.421\n0.087\n\n\n\n\n8.51e-04\n39\n0.023\n0.420\n0.087\n\n\n\n\n7.88e-04\n40\n0.022\n0.420\n0.087\n\n\n\n\n7.01e-04\n41\n0.021\n0.422\n0.087\n\n\n\n\n6.35e-04\n42\n0.021\n0.422\n0.086\n\n\n\n\n6.16e-04\n43\n0.020\n0.422\n0.086\n\n\n\n\n5.99e-04\n44\n0.020\n0.426\n0.087\n\n\n\n\n4.99e-04\n45\n0.019\n0.426\n0.087\n\n\n\n\n4.94e-04\n46\n0.018\n0.426\n0.087\n\n\n\n\n4.24e-04\n47\n0.018\n0.426\n0.087\n\n\n\n\n4.20e-04\n48\n0.017\n0.427\n0.087\n\n\n\n\n3.95e-04\n49\n0.017\n0.427\n0.087\n\n\n\n\n3.08e-04\n50\n0.017\n0.424\n0.087\n\n\n\n\n2.97e-04\n51\n0.016\n0.424\n0.087\n\n\n\n\n2.48e-04\n52\n0.016\n0.427\n0.087\n\n\n\n\n2.31e-04\n53\n0.016\n0.427\n0.087\n\n\n\n\n1.97e-04\n54\n0.016\n0.426\n0.087\n\n\n\n\n1.66e-04\n56\n0.015\n0.428\n0.087\n\n\n\n\n1.21e-04\n57\n0.015\n0.428\n0.087\n\n\n\n\n1.16e-04\n58\n0.015\n0.429\n0.087\n\n\n\n\n1.11e-04\n59\n0.015\n0.428\n0.087\n\n\n\n\n1.05e-04\n60\n0.015\n0.428\n0.087\n\n\n\n\n6.72e-05\n61\n0.015\n0.425\n0.086\n\n\n\n\n4.38e-05\n63\n0.014\n0.427\n0.086\n\n\n\n\n3.42e-05\n65\n0.014\n0.427\n0.086\n\n\n\n\n3.42e-05\n66\n0.014\n0.426\n0.086\n\n\n\n\n2.19e-05\n67\n0.014\n0.426\n0.086\n\n\n\n\n6.16e-06\n69\n0.014\n0.427\n0.086\n\n\n\n\n5.47e-06\n70\n0.014\n0.427\n0.086\n\n\n\n\n0.00e+00\n71\n0.014\n0.427\n0.086\n\n\n\n\n\nMinimum Error: CP with minimum CV error\n\n\nMin+1SE: Simplest tree within 1 standard error of minimum\n\n\nLower CP values result in more complex trees\n\n\n\n\n\n\n\n\n剪定基準の選択： - 最小誤差基準: 予測精度を最大化 - Min+1SE基準: よりシンプルで頑健なモデル - トレードオフ：精度 vs 単純性\n\n\n剪定効果の視覚的確認：\n\n\nコードを表示\n# Execute pruning\ntree_min &lt;- prune(tree_2, cp = cp_min)\ntree_1se &lt;- prune(tree_2, cp = cp_1se)\n\n# Side-by-side plots\npar(mfrow = c(1, 2))\nplot(as.party(tree_min), main = \"Pruned Tree (Minimum Error)\")\n\n\n\n\n\n\n\n\n\nコードを表示\nplot(as.party(tree_1se), main = \"Pruned Tree (Min+1SE)\")\n\n\n\n\n\n\n\n\n\nコードを表示\npar(mfrow = c(1, 1))\n\n# Compare model sizes\ntibble::tibble(\n  Model = c(\"Minimum Error Tree\", \"Min+1SE Tree\"),\n  Terminal_Nodes = c(\n    length(tree_min$frame$var[tree_min$frame$var == \"&lt;leaf&gt;\"]),\n    length(tree_1se$frame$var[tree_1se$frame$var == \"&lt;leaf&gt;\"])\n  ),\n  Model_Complexity = c(\"More complex\", \"Simpler\"),\n  Interpretation = c(\n    \"Higher accuracy, risk of overfitting\",\n    \"Better generalization, easier to interpret\"\n  )\n) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Model Comparison After Pruning\",\n    subtitle = \"Trade-off between complexity and accuracy\") |&gt;\n  cols_label(\n    Model = \"Model Type\",\n    Terminal_Nodes = \"Number of Terminal Nodes\",\n    Model_Complexity = \"Complexity\",\n    Interpretation = \"Practical Interpretation\") |&gt;\n  fmt_number(columns = Terminal_Nodes, decimals = 0)\n\n\n\n\n\n\n\n\nModel Comparison After Pruning\n\n\nTrade-off between complexity and accuracy\n\n\nModel Type\nNumber of Terminal Nodes\nComplexity\nPractical Interpretation\n\n\n\n\nMinimum Error Tree\n10\nMore complex\nHigher accuracy, risk of overfitting\n\n\nMin+1SE Tree\n4\nSimpler\nBetter generalization, easier to interpret\n\n\n\n\n\n\n\n剪定結果の解釈： - Min+1SE木はよりシンプル：解釈性が高い - 最小誤差木はより詳細：予測精度が高い - 実用では Min+1SE が推奨されることが多い",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ROC曲線・ツリーモデル</span>"
    ]
  },
  {
    "objectID": "8_ROC曲線_ツリーモデル.html#分類木とroc評価",
    "href": "8_ROC曲線_ツリーモデル.html#分類木とroc評価",
    "title": "10  ROC曲線・ツリーモデル",
    "section": "14.3 分類木とROC評価",
    "text": "14.3 分類木とROC評価\n\n14.3.1 多クラス分類の実装\nirisデータセットでの分類木： 3種類のアヤメを特徴量から分類します。\n\n\nコードを表示\n# Classification tree with iris data\nset.seed(123)\ncls_tree &lt;- rpart::rpart(Species ~ ., \n                         data = iris, \n                         minsplit = 3, \n                         cp = 0)\n\n# Pruning at different levels\ncls_tree_5 &lt;- prune(cls_tree, cp = 0.005)  # Approximately 5 nodes\ncls_tree_3 &lt;- prune(cls_tree, cp = 0.020)  # Approximately 3 nodes\n\n# Get predicted probabilities\npred_5 &lt;- predict(cls_tree_5, type = \"prob\")\npred_3 &lt;- predict(cls_tree_3, type = \"prob\")\n\n# Multi-class ROC\nroc_5 &lt;- pROC::multiclass.roc(response = iris$Species, \n                              predictor = as.matrix(pred_5),\n                              quiet = TRUE)\nroc_3 &lt;- pROC::multiclass.roc(response = iris$Species, \n                              predictor = as.matrix(pred_3),\n                              quiet = TRUE)\n\n# Display results\ntibble::tibble(\n  Model = c(\"Decision Tree (5 nodes)\", \"Decision Tree (3 nodes)\"),\n  Multiclass_AUC = c(as.numeric(roc_5$auc), \n                     as.numeric(roc_3$auc)),\n  Interpretation = c(\"Complex Model\", \"Simple Model\"),\n  Practical_Use = c(\"Higher accuracy, risk of overfitting\",\n                   \"Good generalization, easy interpretation\")) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Multi-class Classification Performance\",\n    subtitle = \"iris dataset (3-class classification)\") |&gt;\n  fmt_number(\n    columns = Multiclass_AUC,\n    decimals = 3) |&gt;\n  tab_footnote(\n    footnote = \"Multi-class AUC: Average of all pairwise AUCs\",\n    locations = cells_column_labels(columns = Multiclass_AUC))\n\n\n\n\n\n\n\n\nMulti-class Classification Performance\n\n\niris dataset (3-class classification)\n\n\nModel\nMulticlass_AUC1\nInterpretation\nPractical_Use\n\n\n\n\nDecision Tree (5 nodes)\n0.997\nComplex Model\nHigher accuracy, risk of overfitting\n\n\nDecision Tree (3 nodes)\n0.980\nSimple Model\nGood generalization, easy interpretation\n\n\n\n1 Multi-class AUC: Average of all pairwise AUCs\n\n\n\n\n\n\n\n\n結果の解釈： - 両モデルとも高いAUC：irisデータは分類しやすい - シンプルなモデルでも十分な性能 - 実用では解釈性を重視してシンプルなモデルを選択",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ROC曲線・ツリーモデル</span>"
    ]
  },
  {
    "objectID": "8_ROC曲線_ツリーモデル.html#ランダムフォレスト",
    "href": "8_ROC曲線_ツリーモデル.html#ランダムフォレスト",
    "title": "10  ROC曲線・ツリーモデル",
    "section": "15.1 ランダムフォレスト",
    "text": "15.1 ランダムフォレスト\n\n15.1.1 多クラス分類の実装\nランダムフォレストの利点： - 多数の決定木のアンサンブル - 過学習に強い - 変数重要度の算出が可能\n\n\nコードを表示\nset.seed(123)\n\n# Stratified sampling (70:30 split)\nidx_list &lt;- split(seq_len(nrow(iris)), iris$Species)\nidx &lt;- unlist(lapply(idx_list, function(v) sample(v, floor(0.7*length(v)))))\n\ntrain &lt;- iris[idx, ]\ntest  &lt;- iris[-idx, ]\n\n# Data split information\ntibble::tibble(\n  Dataset = c(\"Training set\", \"Test set\"),\n  Size = c(nrow(train), nrow(test)),\n  Percentage = c(\n    paste0(round(nrow(train)/nrow(iris)*100, 1), \"%\"),\n    paste0(round(nrow(test)/nrow(iris)*100, 1), \"%\")\n  )\n) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Data Split Information\",\n    subtitle = \"70:30 stratified sampling\")\n\n\n\n\n\n\n\n\nData Split Information\n\n\n70:30 stratified sampling\n\n\nDataset\nSize\nPercentage\n\n\n\n\nTraining set\n105\n70%\n\n\nTest set\n45\n30%\n\n\n\n\n\n\n\nコードを表示\n# Class distribution in training set\ntable(train$Species) |&gt;\n  as.data.frame() |&gt;\n  rename(Species = Var1, Count = Freq) |&gt;\n  mutate(Percentage = paste0(round(Count/nrow(train)*100, 1), \"%\")) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Class Distribution in Training Set\",\n    subtitle = \"Balanced across species\")\n\n\n\n\n\n\n\n\nClass Distribution in Training Set\n\n\nBalanced across species\n\n\nSpecies\nCount\nPercentage\n\n\n\n\nsetosa\n35\n33.3%\n\n\nversicolor\n35\n33.3%\n\n\nvirginica\n35\n33.3%\n\n\n\n\n\n\n\nコードを表示\n# Train Random Forest\nrf_cls &lt;- randomForest::randomForest(\n  Species ~ ., \n  data = train,\n  ntree = 500, # Number of trees\n  mtry = 2, # Variables tried at each split\n  importance = TRUE  # Calculate variable importance\n)\n\n# Evaluate on test data\nrf_prob &lt;- predict(rf_cls, newdata = test, type = \"prob\")\nrf_pred &lt;- predict(rf_cls, newdata = test, type = \"class\")\n\n# Confusion matrix\nconf_mat &lt;- table(Predicted = rf_pred, Actual = test$Species)\naccuracy &lt;- sum(diag(conf_mat)) / sum(conf_mat)\n\n# Multi-class AUC\nrf_auc_mc &lt;- pROC::multiclass.roc(response = test$Species, \n                                  predictor = as.matrix(rf_prob),\n                                  quiet = TRUE)\n\n# Display results\ntibble::tibble(\n  Model = \"Random Forest\",\n  Dataset = \"Test Set\",\n  Multiclass_AUC = as.numeric(rf_auc_mc$auc),\n  Accuracy = accuracy,\n  OOB_Error = rf_cls$err.rate[500, \"OOB\"],\n  Interpretation = \"Excellent performance with ensemble learning\") |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Random Forest Performance\",\n    subtitle = \"Ensemble of 500 decision trees\") |&gt;\n  fmt_number(\n    columns = c(Multiclass_AUC, Accuracy, OOB_Error),\n    decimals = 3) |&gt;\n  tab_footnote(\n    footnote = \"OOB (Out-of-Bag) Error: Internal validation estimate\",\n    locations = cells_column_labels(columns = OOB_Error))\n\n\n\n\n\n\n\n\nRandom Forest Performance\n\n\nEnsemble of 500 decision trees\n\n\nModel\nDataset\nMulticlass_AUC\nAccuracy\nOOB_Error1\nInterpretation\n\n\n\n\nRandom Forest\nTest Set\n0.996\n0.933\n0.048\nExcellent performance with ensemble learning\n\n\n\n1 OOB (Out-of-Bag) Error: Internal validation estimate\n\n\n\n\n\n\n\n\nコードを表示\n# Display confusion matrix\nconf_mat |&gt;\n  as.data.frame.matrix() |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Confusion Matrix\",\n    subtitle = \"Random Forest predictions on test set\")\n\n\n\n\n\n\n\n\nConfusion Matrix\n\n\nRandom Forest predictions on test set\n\n\nsetosa\nversicolor\nvirginica\n\n\n\n\n15\n0\n0\n\n\n0\n14\n2\n\n\n0\n1\n13\n\n\n\n\n\n\n\n\n\n15.1.2 変数重要度の可視化\nどの変数が予測に重要か：\n\n\nコードを表示\n# Extract and display importance\nimp_rf &lt;- importance(rf_cls) |&gt; \n  as.data.frame() |&gt; \n  tibble::rownames_to_column(\"Feature\")\n\nimp_rf |&gt;\n  arrange(desc(MeanDecreaseGini)) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Variable Importance\",\n    subtitle = \"Evaluated by Mean Decrease in Gini Impurity\") |&gt;\n  fmt_number(\n    columns = where(is.numeric),\n    decimals = 2) |&gt;\n  tab_footnote(\n    footnote = \"Higher values indicate stronger predictive power\",\n    locations = cells_column_labels(columns = MeanDecreaseGini)) |&gt;\n  tab_footnote(\n    footnote = \"MeanDecreaseAccuracy: Impact on accuracy when variable is permuted\",\n    locations = cells_column_labels(columns = MeanDecreaseAccuracy))\n\n\n\n\n\n\n\n\nVariable Importance\n\n\nEvaluated by Mean Decrease in Gini Impurity\n\n\nFeature\nsetosa\nversicolor\nvirginica\nMeanDecreaseAccuracy1\nMeanDecreaseGini2\n\n\n\n\nPetal.Width\n21.61\n29.03\n33.76\n33.01\n31.40\n\n\nPetal.Length\n21.93\n28.38\n27.01\n32.41\n28.76\n\n\nSepal.Length\n6.88\n8.22\n7.87\n10.94\n7.48\n\n\nSepal.Width\n4.72\n3.58\n4.33\n7.26\n1.60\n\n\n\n1 MeanDecreaseAccuracy: Impact on accuracy when variable is permuted\n\n\n2 Higher values indicate stronger predictive power\n\n\n\n\n\n\n\n\nコードを表示\n# Visualization\nvarImpPlot(rf_cls, main = \"Variable Importance Plot\")\n\n\n\n\n\n\n\n\n\n変数重要度の解釈： - Petal.Length/Width が最重要：花弁の特徴が種の判別に有効 - Sepal特徴は補助的：がく片の特徴は相対的に重要度が低い - この知見は植物学的にも妥当",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ROC曲線・ツリーモデル</span>"
    ]
  },
  {
    "objectID": "8_ROC曲線_ツリーモデル.html#xgboost",
    "href": "8_ROC曲線_ツリーモデル.html#xgboost",
    "title": "10  ROC曲線・ツリーモデル",
    "section": "15.2 XGBoost",
    "text": "15.2 XGBoost\n\n15.2.1 多クラス分類\nXGBoostの特徴： - 勾配ブースティング決定木 - 高い予測精度 - 正則化による過学習防止\n\nモデル訓練性能評価SHAP分析\n\n\n\n\nコードを表示\nset.seed(123)\n\n# Data preparation\nlbl &lt;- as.integer(iris$Species) - 1L  # XGBoost requires 0-indexed labels\nidx_list &lt;- split(seq_len(nrow(iris)), iris$Species)\nidx &lt;- unlist(lapply(idx_list, function(v) sample(v, floor(0.7*length(v)))))\n\ntr_x &lt;- as.matrix(iris[idx, 1:4])\nte_x &lt;- as.matrix(iris[-idx, 1:4])\ntr_y &lt;- lbl[idx]\nte_y &lt;- lbl[-idx]\n\n# Create DMatrix (XGBoost data structure)\ndtr &lt;- xgboost::xgb.DMatrix(data = tr_x, label = tr_y)\ndte &lt;- xgboost::xgb.DMatrix(data = te_x, label = te_y)\n\n# Parameter settings with explanations\nparams &lt;- list(\n  objective = \"multi:softprob\", # Multi-class probability output\n  num_class = 3, # Number of classes\n  eval_metric = \"mlogloss\", # Multi-class log loss\n  eta = 0.1, # Learning rate\n  max_depth = 3, # Maximum tree depth\n  subsample = 0.8, # Row sampling ratio\n  colsample_bytree = 0.8, # Column sampling ratio\n  min_child_weight = 1, # Minimum child node weight\n  lambda = 1.0 # L2 regularization\n)\n\n# Model training with early stopping\nxgb_mc &lt;- xgboost::xgb.train(\n  params = params, \n  data = dtr, \n  nrounds = 300,\n  watchlist = list(train = dtr, eval = dte),\n  early_stopping_rounds = 20,  # Stop if no improvement for 20 rounds\n  verbose = 0)\n\n# Prediction and evaluation\npred &lt;- predict(xgb_mc, newdata = dte)\nprob &lt;- matrix(pred, ncol = 3, byrow = TRUE)\ncolnames(prob) &lt;- levels(iris$Species)\n\n# Convert back to factor for evaluation\nte_fac &lt;- factor(te_y, levels = 0:2, labels = levels(iris$Species))\nxgb_auc_mc &lt;- pROC::multiclass.roc(response = te_fac, \n                                   predictor = prob,\n                                   quiet = TRUE)\n\n# Display training information\ntibble::tibble(\n  Metric = c(\"Best iteration\", \"Multi-class AUC\"),\n  Value = c(\n    xgb_mc$best_iteration,\n    round(as.numeric(xgb_auc_mc$auc), 3)\n  ),\n  Description = c(\n    \"Optimal number of boosting rounds\",\n    \"Area under the ROC curve\"\n  )\n) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"XGBoost Optimization Results\",\n    subtitle = \"Early stopping performance\")\n\n\n\n\n\n\n\n\nXGBoost Optimization Results\n\n\nEarly stopping performance\n\n\nMetric\nValue\nDescription\n\n\n\n\nBest iteration\n43.000\nOptimal number of boosting rounds\n\n\nMulti-class AUC\n0.991\nArea under the ROC curve\n\n\n\n\n\n\n\n\n\n学習曲線と最適化：\n\n\nコードを表示\n# Get learning curve data\neval_log &lt;- xgb_mc$evaluation_log\n\n# Performance summary\ntibble::tibble(\n  Model = \"XGBoost\",\n  Best_Iteration = xgb_mc$best_iteration,\n  Train_Loss = eval_log$train_mlogloss[xgb_mc$best_iteration],\n  Test_Loss = eval_log$eval_mlogloss[xgb_mc$best_iteration],\n  Multiclass_AUC = as.numeric(xgb_auc_mc$auc),\n  Overfitting_Status = ifelse(\n    eval_log$train_mlogloss[xgb_mc$best_iteration] &lt; \n    eval_log$eval_mlogloss[xgb_mc$best_iteration] * 0.9,\n    \"Slight overfitting detected\",\n    \"Good generalization\")) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"XGBoost Model Performance\",\n    subtitle = \"Optimization with early stopping\") |&gt;\n  fmt_number(\n    columns = c(Train_Loss, Test_Loss, Multiclass_AUC),\n    decimals = 4) |&gt;\n  tab_source_note(\"mlogloss: Multi-class log loss (lower is better)\") |&gt;\n  tab_source_note(\"Early stopping: Stop after 20 rounds without improvement\") |&gt;\n  tab_footnote(\n    footnote = \"Early stopping prevents overfitting by monitoring validation performance\",\n    locations = cells_column_labels(columns = Best_Iteration))\n\n\n\n\n\n  \n    \n      XGBoost Model Performance\n    \n    \n      Optimization with early stopping\n    \n    \n      Model\n      Best_Iteration1\n      Train_Loss\n      Test_Loss\n      Multiclass_AUC\n      Overfitting_Status\n    \n  \n  \n    XGBoost\n43\n0.0544\n0.1723\n0.9911\nSlight overfitting detected\n  \n  \n    \n      mlogloss: Multi-class log loss (lower is better)\n    \n    \n      Early stopping: Stop after 20 rounds without improvement\n    \n  \n  \n    \n      1 Early stopping prevents overfitting by monitoring validation performance\n    \n  \n\n\n\n\nコードを表示\n# Plot learning curves\neval_log |&gt;\n  ggplot(aes(x = iter)) +\n  geom_line(aes(y = train_mlogloss, color = \"Training\"), size = 1) +\n  geom_line(aes(y = eval_mlogloss, color = \"Validation\"), size = 1) +\n  geom_vline(xintercept = xgb_mc$best_iteration, \n             linetype = \"dashed\", color = \"red\", alpha = 0.5) +\n  labs(title = \"XGBoost Learning Curves\",\n       subtitle = \"Training vs Validation Loss\",\n       x = \"Iteration\",\n       y = \"Multi-class Log Loss\",\n       color = \"Dataset\") +\n  theme_minimal() +\n  annotate(\"text\", x = xgb_mc$best_iteration + 5, y = max(eval_log$train_mlogloss),\n           label = paste(\"Best iteration:\", xgb_mc$best_iteration),\n           hjust = 0, size = 3)\n\n\n\n\n\n\n\n\n\n\n\n予測の解釈可能性： SHAP値により、各特徴量が予測にどのように寄与したかを理解できます。\n\n\nコードを表示\n# SHAP value calculation for binary classification (more stable for demonstration)\nset.seed(123)\n\n# Create binary classification problem from airquality data\nth   &lt;- stats::median(df_air$Ozone)\nybin &lt;- as.numeric(df_air$Ozone &gt; th)\n\n# Binary classification task information\ntibble::tibble(\n  Metric = c(\"Binary classification task\", \"Threshold (ppb)\", \"Class 0 (Low)\", \"Class 1 (High)\"),\n  Value = c(\"Predicting high ozone levels\", as.character(th), \n            sum(ybin == 0), sum(ybin == 1))\n) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Binary Classification Setup\",\n    subtitle = \"High vs Low ozone levels\")\n\n\n\n\n\n\n\n\nBinary Classification Setup\n\n\nHigh vs Low ozone levels\n\n\nMetric\nValue\n\n\n\n\nBinary classification task\nPredicting high ozone levels\n\n\nThreshold (ppb)\n31\n\n\nClass 0 (Low)\n56\n\n\nClass 1 (High)\n55\n\n\n\n\n\n\n\nコードを表示\n# Data preparation\nx &lt;- as.matrix(dplyr::select(df_air, Solar.R, Wind, Temp, Month, Day))\nidx &lt;- sample(seq_len(nrow(x)), size = floor(0.7*nrow(x)))\n\ndtr &lt;- xgboost::xgb.DMatrix(data = x[idx, ], label = ybin[idx])\ndte &lt;- xgboost::xgb.DMatrix(data = x[-idx, ], label = ybin[-idx])\n\n# Binary classification model\nparams &lt;- list(\n  objective = \"binary:logistic\",\n  eval_metric = \"auc\",\n  eta = 0.1, \n  max_depth = 3,\n  subsample = 0.8, \n  colsample_bytree = 0.8)\n\nxgb_bin &lt;- xgboost::xgb.train(\n  params = params, \n  data = dtr, \n  nrounds = 300,\n  watchlist = list(eval = dte, train = dtr),\n  early_stopping_rounds = 20, \n  verbose = 0)\n\n# AUC evaluation\np &lt;- predict(xgb_bin, newdata = dte)\nroc_xgb &lt;- pROC::roc(response = xgboost::getinfo(dte, \"label\"), \n                     predictor = p, \n                     ci = TRUE, \n                     quiet = TRUE)\n\n# Calculate SHAP values\ncontrib &lt;- predict(xgb_bin, newdata = dte, predcontrib = TRUE)\ncolnames(contrib) &lt;- c(colnames(x), \"BIAS\")\n\n# Feature importance (SHAP)\nimp_shap &lt;- tibble::tibble(\n  Feature = colnames(x),\n  MeanAbsSHAP = apply(abs(contrib[, colnames(x), drop = FALSE]), 2, mean),\n  Interpretation = c(\n    \"Solar radiation impact\",\n    \"Wind speed (negative correlation expected)\",\n    \"Temperature (strongest predictor)\",\n    \"Seasonal effect\",\n    \"Day of month (minimal effect expected)\")) |&gt; \n  arrange(desc(MeanAbsSHAP))\n\n# Display SHAP importance\nimp_shap |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Feature Importance via SHAP Values\",\n    subtitle = \"Average contribution to individual predictions\") |&gt;\n  fmt_number(\n    columns = MeanAbsSHAP,\n    decimals = 3) |&gt;\n  tab_source_note(\"SHAP: SHapley Additive exPlanations\") |&gt;\n  tab_source_note(\"Higher values indicate greater impact on predictions\") |&gt;\n  tab_footnote(\n    footnote = \"SHAP values provide model-agnostic feature importance\",\n    locations = cells_column_labels(columns = MeanAbsSHAP))\n\n\n\n\n\n  \n    \n      Feature Importance via SHAP Values\n    \n    \n      Average contribution to individual predictions\n    \n    \n      Feature\n      MeanAbsSHAP1\n      Interpretation\n    \n  \n  \n    Temp\n2.178\nTemperature (strongest predictor)\n    Solar.R\n0.601\nSolar radiation impact\n    Wind\n0.425\nWind speed (negative correlation expected)\n    Day\n0.171\nDay of month (minimal effect expected)\n    Month\n0.052\nSeasonal effect\n  \n  \n    \n      SHAP: SHapley Additive exPlanations\n    \n    \n      Higher values indicate greater impact on predictions\n    \n  \n  \n    \n      1 SHAP values provide model-agnostic feature importance\n    \n  \n\n\n\n\nコードを表示\n# Display AUC for binary classification\ntibble::tibble(\n  Model = \"XGBoost (Binary)\",\n  AUC = as.numeric(pROC::auc(roc_xgb)),\n  CI_Lower = roc_xgb$ci[1],\n  CI_Upper = roc_xgb$ci[3],\n  Interpretation = case_when(\n    as.numeric(pROC::auc(roc_xgb)) &gt;= 0.9 ~ \"Excellent discrimination\",\n    as.numeric(pROC::auc(roc_xgb)) &gt;= 0.8 ~ \"Good discrimination\",\n    TRUE ~ \"Fair discrimination\")) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Binary Classification Performance\",\n    subtitle = \"Predicting high ozone levels\") |&gt;\n  fmt_number(\n    columns = c(AUC, CI_Lower, CI_Upper),\n    decimals = 3)\n\n\n\n\n\n\n\n\nBinary Classification Performance\n\n\nPredicting high ozone levels\n\n\nModel\nAUC\nCI_Lower\nCI_Upper\nInterpretation\n\n\n\n\nXGBoost (Binary)\n0.788\n0.630\n0.946\nFair discrimination\n\n\n\n\n\n\n\nSHAP値の解釈： - Temperature が最重要：気温が高いとオゾン濃度が上昇 - Wind が次に重要：風が強いとオゾンが拡散（負の相関） - 個別予測の説明が可能：モデルの透明性向上",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ROC曲線・ツリーモデル</span>"
    ]
  },
  {
    "objectID": "8_ROC曲線_ツリーモデル.html#手法選択のフローチャート",
    "href": "8_ROC曲線_ツリーモデル.html#手法選択のフローチャート",
    "title": "10  ROC曲線・ツリーモデル",
    "section": "16.1 手法選択のフローチャート",
    "text": "16.1 手法選択のフローチャート\n\n\n\n\n\n\n手法選択の指針\n\n\n\n\n16.1.1 解釈性重視の場合\n\nGLM/GAM: 線形・非線形関係の理解、係数の解釈が容易\n決定木: ルールベースの理解、視覚的に分かりやすい\nSHAP付きXGBoost: 高精度と解釈性の両立\n\n\n\n16.1.2 予測精度重視の場合\n\nXGBoost: 最高精度、ただしハイパーパラメータ調整が重要\nランダムフォレスト: 安定した高精度、調整が比較的容易\nGAMM: 非線形＋階層構造、統計的推論も可能\n\n\n\n16.1.3 データ量による選択\n\n少量（n&lt;100）: GLM、単純な決定木（過学習に注意）\n中量（100&lt;n&lt;1000）: ランダムフォレスト、GAM（交差検証必須）\n大量（n&gt;1000）: XGBoost、深層学習（計算資源を考慮）",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ROC曲線・ツリーモデル</span>"
    ]
  },
  {
    "objectID": "8_ROC曲線_ツリーモデル.html#実装上の注意点",
    "href": "8_ROC曲線_ツリーモデル.html#実装上の注意点",
    "title": "10  ROC曲線・ツリーモデル",
    "section": "16.2 実装上の注意点",
    "text": "16.2 実装上の注意点\n\n\n\n\n\n\n重要な注意事項\n\n\n\n\n過学習の防止\n\nクロスバリデーション必須（特に小サンプル）\n早期停止の活用（XGBoost）\n正則化パラメータの調整（lambda, alpha）\n訓練/検証/テストセットの適切な分割\n\n評価指標の選択\n\n不均衡データ: F1スコア、PR-AUC、Matthews相関係数\n多クラス: macro/micro平均の考慮\nビジネス要件: 偽陽性/偽陰性のコスト考慮\n医療診断: 感度重視 vs 特異度重視\n\n再現性の確保\n\n乱数シードの固定（set.seed()）\nパッケージバージョンの記録（sessionInfo()）\nデータ前処理の文書化",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ROC曲線・ツリーモデル</span>"
    ]
  },
  {
    "objectID": "8_ROC曲線_ツリーモデル.html#パフォーマンス比較表",
    "href": "8_ROC曲線_ツリーモデル.html#パフォーマンス比較表",
    "title": "10  ROC曲線・ツリーモデル",
    "section": "16.3 パフォーマンス比較表",
    "text": "16.3 パフォーマンス比較表\n\n\nコードを表示\n# Comprehensive comparison table of all models\ntibble::tibble(\n  Method = c(\"GLM\", \"GLMM\", \"GAM\", \"GAMM\", \n          \"Decision Tree\", \"Random Forest\", \"XGBoost\"),\n  Interpretability = c(\"◎\", \"○\", \"○\", \"△\", \n            \"◎\", \"△\", \"△\"),\n  Accuracy = c(\"○\", \"○\", \"◎\", \"◎\", \n          \"△\", \"◎\", \"◎\"),\n  Speed = c(\"◎\", \"○\", \"○\", \"△\", \n              \"◎\", \"○\", \"○\"),\n  Overfitting_Resistance = c(\"○\", \"◎\", \"○\", \"◎\", \n                \"△\", \"◎\", \"○\"),\n  No_Preprocessing = c(\"×\", \"×\", \"×\", \"×\", \n                \"◎\", \"◎\", \"◎\"),\n  Best_Use_Case = c(\n    \"Linear relationships, inference\",\n    \"Hierarchical data, mixed effects\",\n    \"Non-linear patterns, smooth effects\",\n    \"Complex hierarchical non-linear\",\n    \"Simple rules, visualization\",\n    \"General purpose, stable performance\",\n    \"Competition, highest accuracy\")) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Machine Learning Methods Comparison\",\n    subtitle = \"◎:Excellent ○:Good △:Fair ×:Not Applicable\") |&gt;\n  cols_label(\n    Method = \"Method\",\n    Interpretability = \"Interpretability\",\n    Accuracy = \"Accuracy\",\n    Speed = \"Computational Speed\",\n    Overfitting_Resistance = \"Overfitting Resistance\",\n    No_Preprocessing = \"No Preprocessing Needed\",\n    Best_Use_Case = \"Best Use Case\") |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()) |&gt;\n  tab_source_note(\"Note: Ratings show general trends and may vary by dataset\") |&gt;\n  tab_footnote(\n    footnote = \"Consider ensemble methods for critical applications\",\n    locations = cells_column_labels(columns = Method))\n\n\n\n\n\n  \n    \n      Machine Learning Methods Comparison\n    \n    \n      ◎:Excellent ○:Good △:Fair ×:Not Applicable\n    \n    \n      Method1\n      Interpretability\n      Accuracy\n      Computational Speed\n      Overfitting Resistance\n      No Preprocessing Needed\n      Best Use Case\n    \n  \n  \n    GLM\n◎\n○\n◎\n○\n×\nLinear relationships, inference\n    GLMM\n○\n○\n○\n◎\n×\nHierarchical data, mixed effects\n    GAM\n○\n◎\n○\n○\n×\nNon-linear patterns, smooth effects\n    GAMM\n△\n◎\n△\n◎\n×\nComplex hierarchical non-linear\n    Decision Tree\n◎\n△\n◎\n△\n◎\nSimple rules, visualization\n    Random Forest\n△\n◎\n○\n◎\n◎\nGeneral purpose, stable performance\n    XGBoost\n△\n◎\n○\n○\n◎\nCompetition, highest accuracy\n  \n  \n    \n      Note: Ratings show general trends and may vary by dataset\n    \n  \n  \n    \n      1 Consider ensemble methods for critical applications",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ROC曲線・ツリーモデル</span>"
    ]
  },
  {
    "objectID": "8_ROC曲線_ツリーモデル.html#実践的な推奨事項",
    "href": "8_ROC曲線_ツリーモデル.html#実践的な推奨事項",
    "title": "10  ROC曲線・ツリーモデル",
    "section": "16.4 実践的な推奨事項",
    "text": "16.4 実践的な推奨事項\n\n\n\n\n\n\n主な流れ\n\n\n\n\nモデル選択のワークフロー\n\n単純なモデル（GLM）から開始\n段階的に複雑性を増加\n性能向上が止まったら停止\n\n検証方略\n\n時系列データ：時間的分割\n小サンプル：Leave-One-Out CV\n大サンプル：k-fold CV（k=5 or 10）\n\n報告すべき内容\n\nAUCと信頼区間\n混同行列\n変数重要度\n実装の詳細（パラメータ設定）",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ROC曲線・ツリーモデル</span>"
    ]
  },
  {
    "objectID": "8_ROC曲線_ツリーモデル.html#関連リソース",
    "href": "8_ROC曲線_ツリーモデル.html#関連リソース",
    "title": "10  ROC曲線・ツリーモデル",
    "section": "17.1 関連リソース",
    "text": "17.1 関連リソース\n\neasystats プロジェクト\nXGBoost公式ドキュメント\ntidymodels フレームワーク\nSHAP値の理解",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ROC曲線・ツリーモデル</span>"
    ]
  },
  {
    "objectID": "8_ROC曲線_ツリーモデル.html#セッション情報",
    "href": "8_ROC曲線_ツリーモデル.html#セッション情報",
    "title": "10  ROC曲線・ツリーモデル",
    "section": "17.2 セッション情報",
    "text": "17.2 セッション情報\n\n\nコードを表示\nsessionInfo()\n\n\nR version 4.5.0 (2025-04-11)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.6\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Asia/Tokyo\ntzcode source: internal\n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] xgboost_1.7.11.1     randomForest_4.7-1.2 ggparty_1.0.0.1     \n [4] partykit_1.2-24      mvtnorm_1.3-3        libcoin_1.0-10      \n [7] rpart_4.1.24         broom.mixed_0.2.9.6  mgcv_1.9-3          \n[10] nlme_3.1-168         lme4_1.1-37          Matrix_1.7-3        \n[13] pROC_1.19.0.1        performance_0.15.0   psych_2.5.6         \n[16] gt_1.0.0             lubridate_1.9.4      forcats_1.0.0       \n[19] stringr_1.5.1        dplyr_1.1.4          purrr_1.1.0         \n[22] readr_2.1.5          tidyr_1.3.1          tibble_3.3.0        \n[25] ggplot2_3.5.2        tidyverse_2.0.0     \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1   farver_2.1.2       fastmap_1.2.0      pacman_0.5.1      \n [5] digest_0.6.37      timechange_0.3.0   lifecycle_1.0.4    survival_3.8-3    \n [9] magrittr_2.0.3     compiler_4.5.0     sass_0.4.10        rlang_1.1.6       \n[13] tools_4.5.0        yaml_2.3.10        data.table_1.17.8  knitr_1.50        \n[17] labeling_0.4.3     htmlwidgets_1.6.4  mnormt_2.1.1       xml2_1.3.8        \n[21] RColorBrewer_1.1-3 withr_3.0.2        future_1.67.0      globals_0.18.0    \n[25] scales_1.4.0       MASS_7.3-65        insight_1.3.1      cli_3.6.5         \n[29] inum_1.0-5         rmarkdown_2.29     reformulas_0.4.1   generics_0.1.4    \n[33] rstudioapi_0.17.1  tzdb_0.5.0         minqa_1.2.8        splines_4.5.0     \n[37] parallel_4.5.0     vctrs_0.6.5        boot_1.3-31        jsonlite_2.0.0    \n[41] hms_1.1.3          Formula_1.2-5      listenv_0.9.1      see_0.11.0        \n[45] glue_1.8.0         parallelly_1.45.1  nloptr_2.2.1       codetools_0.2-20  \n[49] stringi_1.8.7      gtable_0.3.6       furrr_0.3.1        pillar_1.11.0     \n[53] htmltools_0.5.8.1  R6_2.6.1           Rdpack_2.6.4       evaluate_1.0.4    \n[57] lattice_0.22-7     rbibutils_2.3      backports_1.5.0    broom_1.0.9       \n[61] Rcpp_1.1.0         checkmate_2.3.2    xfun_0.52          pkgconfig_2.0.3",
    "crumbs": [
      "第III部：ベイズ統計と機械学習",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ROC曲線・ツリーモデル</span>"
    ]
  },
  {
    "objectID": "9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法.html",
    "href": "9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法.html",
    "title": "11  9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法",
    "section": "",
    "text": "12 はじめに\n本ドキュメントは、探索的因子分析 (EFA)、確認的因子分析 (CFA)、主成分分析 (PCA)、コレスポンデンス分析 (CA/MCA)、および多次元尺度法 (MDS) を、R による実行例とともに体系的にまとめた実践ガイドです。教育・心理領域のアンケートデータやテストデータを対象に、モデルの考え方、実装、そして解釈の要点を一貫した流れで示します。\n依存パッケージは適宜 install.packages() で導入してください。\nコードを表示\n# Load required packages\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\npacman::p_load(\n  tidyverse,        # Data manipulation and visualization\n  psych,            # EFA and reliability analysis\n  GPArotation,      # Factor rotations\n  sjPlot,           # EFA table formatting\n  lavaan,           # CFA/SEM\n  semPlot,          # Path diagrams\n  lavaanPlot,       # Alternative path diagrams\n  FactoMineR,       # PCA, CA, MCA\n  factoextra,       # Visualization support\n  smacof,           # Non-metric MDS\n  gt,               # Table formatting\n  scales,           # Axis labeling\n  broom             # Model output tidying\n)\nset.seed(123)",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法</span>"
    ]
  },
  {
    "objectID": "9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法.html#コンセプトと手順",
    "href": "9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法.html#コンセプトと手順",
    "title": "11  9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法",
    "section": "13.1 コンセプトと手順",
    "text": "13.1 コンセプトと手順\n\n独自性 = 特殊分散 + 誤差分散、共通性 = 1 − 独自性（= 因子負荷の二乗和）\n目的は、相関行列を因子に分解し、共通分散の説明を最大化すること\n一般的な手順: (1) 因子数の決定 → (2) 因子負荷の推定 → (3) 回転 → (4) 解釈\n\nEFAは観測変数間の相関を「共通因子 + 独自性」に分解するモデルです。推定の目的は、相関行列のうち共通分散をできるだけ少ない因子で説明することにあります。因子数は「過少（情報の取りこぼし）」と「過多（過適合）」のトレードオフで、スクリープロット・平行分析・情報量基準（BICなど）を総合判断するのが実務的です。回転は解釈可能性の向上が目的で、斜交回転（因子間相関を許容）をデフォルトとし、理論的に独立と考える場合のみ直交回転を検討します。\n\n13.1.1 データ例（シミュレーション）\n\n\nコードを表示\n# Set parameters for data generation\nn_respondents &lt;- 500\nn_items &lt;- 15\nn_factors &lt;- 3\n\n# Generate random factor loadings and scores\nloadings &lt;- matrix(runif(n_items * n_factors), nrow = n_items, ncol = n_factors)\nfactor_scores &lt;- matrix(rnorm(n_respondents * n_factors), nrow = n_respondents, ncol = n_factors)\n\n# Transform to Likert scale (1-6)\nraw &lt;- factor_scores %*% t(loadings)\nrng01 &lt;- apply(raw, 1, function(x) (x - min(x)) / (max(x) - min(x)))\ndata_scaled &lt;- t(rng01) * 5 + 1\nround_likert &lt;- function(x) pmin(pmax(round(x), 1), 6)\nsurvey &lt;- as.data.frame(apply(data_scaled, 2, round_likert))\nnames(survey) &lt;- paste0(\"V\", seq_len(ncol(survey)))\n\n\n\n\n13.1.2 因子数の決定（ガットマン基準・スクリープロット・平行分析）\n\n\nコードを表示\n# Calculate eigenvalues (Guttman criterion: λ &gt; 1)\neig &lt;- eigen(cor(survey))$values\n\n# Display eigenvalues in gt table\ndata.frame(\n  Component = 1:length(eig),\n  Eigenvalue = eig,\n  Proportion = eig / sum(eig),\n  Cumulative = cumsum(eig / sum(eig))\n) |&gt;\n  slice(1:10) |&gt;  # Show first 10 components\n  gt() |&gt;\n  tab_header(\n    title = \"Eigenvalues from Correlation Matrix\",\n    subtitle = \"Components with eigenvalue &gt; 1 should be retained (Kaiser criterion)\"\n  ) |&gt;\n  fmt_number(\n    columns = c(Eigenvalue, Proportion, Cumulative),\n    decimals = 3\n  ) |&gt;\n  cols_label(\n    Component = \"Component\",\n    Eigenvalue = \"Eigenvalue\",\n    Proportion = \"Proportion\",\n    Cumulative = \"Cumulative\"\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#e6ffe6\"),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_body(\n      columns = Eigenvalue,\n      rows = Eigenvalue &gt; 1\n    )\n  ) |&gt;\n  tab_footnote(\n    footnote = \"Components with eigenvalue &gt; 1 are highlighted\",\n    locations = cells_column_labels(columns = Eigenvalue)\n  )\n\n\n\n\n\n\n\n\nEigenvalues from Correlation Matrix\n\n\nComponents with eigenvalue &gt; 1 should be retained (Kaiser criterion)\n\n\nComponent\nEigenvalue1\nProportion\nCumulative\n\n\n\n\n1\n5.074\n0.338\n0.338\n\n\n2\n4.397\n0.293\n0.631\n\n\n3\n3.363\n0.224\n0.856\n\n\n4\n1.491\n0.099\n0.955\n\n\n5\n0.209\n0.014\n0.969\n\n\n6\n0.123\n0.008\n0.977\n\n\n7\n0.078\n0.005\n0.982\n\n\n8\n0.061\n0.004\n0.986\n\n\n9\n0.054\n0.004\n0.990\n\n\n10\n0.040\n0.003\n0.993\n\n\n\n1 Components with eigenvalue &gt; 1 are highlighted\n\n\n\n\n\n\n\n\nコードを表示\n# Scree plot and parallel analysis\npsych::VSS.scree(survey)\n\n\n\n\n\n\n\n\n\nコードを表示\npsych::fa.parallel(survey, fm = \"ml\", fa = \"pc\", n.iter = 100)\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  4 \n\n\n\ntidyverseで作るスクリープロット（自作）\n\n\nコードを表示\n# Create scree plot data\neig_vals &lt;- eigen(cor(survey))$values\nscree_tbl &lt;- tibble(Component = seq_along(eig_vals), Eigenvalue = eig_vals)\n\n# Visualize with ggplot2\nggplot(scree_tbl, aes(Component, Eigenvalue)) +\n  geom_point(size = 3, color = \"darkblue\") +\n  geom_line(linewidth = 0.8, color = \"darkblue\") +\n  geom_hline(yintercept = 1, linetype = \"dashed\", color = \"red\") +\n  scale_x_continuous(breaks = scree_tbl$Component) +\n  labs(\n    title = \"Scree Plot (tidyverse)\",\n    subtitle = \"Components with eigenvalue &gt; 1 (above red line) are candidates for retention\",\n    y = \"Eigenvalue\"\n  ) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(face = \"italic\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n13.1.3 因子負荷の推定と回転\n\n\nコードを表示\n# Fit EFA model with promax rotation\nfit_efa &lt;- psych::fa(survey, nfactors = 4, fm = \"ml\", rotate = \"promax\")\n\n# Display summary\nprint(fit_efa)\n\n\nFactor Analysis using method =  ml\nCall: psych::fa(r = survey, nfactors = 4, rotate = \"promax\", fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n      ML1   ML2   ML3   ML4   h2     u2 com\nV1  -0.32 -0.04  0.95  0.09 0.98 0.0196 1.2\nV2   0.07  0.99  0.39  0.00 0.98 0.0153 1.3\nV3  -0.40  0.82 -0.02  0.36 0.97 0.0291 1.9\nV4   0.29  0.95  0.32 -0.09 0.97 0.0262 1.4\nV5   0.93 -0.30 -0.10 -0.02 0.99 0.0093 1.2\nV6  -0.37 -0.70  0.42  0.43 0.98 0.0235 3.0\nV7  -0.01  0.40  1.00  0.12 0.92 0.0827 1.3\nV8   0.95  0.13 -0.18  0.02 0.96 0.0447 1.1\nV9   0.55 -0.63  0.36  0.14 0.95 0.0499 2.7\nV10  0.40 -0.51 -0.17  0.58 0.90 0.1026 3.0\nV11  0.96  0.04 -0.17  0.01 0.97 0.0271 1.1\nV12  0.15  0.01  0.17  0.87 0.69 0.3072 1.1\nV13  0.89  0.30  0.15  0.23 0.86 0.1446 1.4\nV14  0.28  0.60 -0.38  0.42 0.92 0.0796 3.1\nV15 -0.35 -0.07 -0.54  0.59 0.99 0.0114 2.7\n\n                       ML1  ML2  ML3  ML4\nSS loadings           4.66 4.33 2.98 2.05\nProportion Var        0.31 0.29 0.20 0.14\nCumulative Var        0.31 0.60 0.80 0.94\nProportion Explained  0.33 0.31 0.21 0.15\nCumulative Proportion 0.33 0.64 0.85 1.00\n\n With factor correlations of \n      ML1   ML2   ML3   ML4\nML1  1.00 -0.06 -0.04  0.00\nML2 -0.06  1.00 -0.20 -0.07\nML3 -0.04 -0.20  1.00 -0.39\nML4  0.00 -0.07 -0.39  1.00\n\nMean item complexity =  1.8\nTest of the hypothesis that 4 factors are sufficient.\n\ndf null model =  105  with the objective function =  29.81 with Chi Square =  14701.74\ndf of  the model are 51  and the objective function was  0.19 \n\nThe root mean square of the residuals (RMSR) is  0 \nThe df corrected root mean square of the residuals is  0 \n\nThe harmonic n.obs is  500 with the empirical chi square  0.64  with prob &lt;  1 \nThe total n.obs was  500  with Likelihood Chi Square =  90.87  with prob &lt;  5e-04 \n\nTucker Lewis Index of factoring reliability =  0.994\nRMSEA index =  0.039  and the 90 % confidence intervals are  0.026 0.053\nBIC =  -226.07\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   ML1  ML2  ML3  ML4\nCorrelation of (regression) scores with factors   1.00 1.00 1.00 0.99\nMultiple R square of scores with factors          0.99 0.99 0.99 0.98\nMinimum correlation of possible factor scores     0.99 0.99 0.98 0.96\n\n\nコードを表示\n# Create formatted table with factor loadings\nsjPlot::tab_fa(\n  survey, \n  rotation = \"promax\", \n  method = \"ml\", \n  nmbr.fctr = 4,\n  fctr.load.tlrn = 0.1, \n  sort = TRUE, \n  title = \"Exploratory Factor Analysis Results\",\n  show.cronb = TRUE, \n  show.comm = TRUE, \n  digits = 2, \n  encoding = \"UTF-8\"\n)\n\n\n\nExploratory Factor Analysis Results\n\n\n \nFactor 1\nFactor 2\nFactor 3\nFactor 4\nCommunality\n\n\nV11\n0.96\n0.04\n-0.17\n0.01\n0.98\n\n\nV8\n0.95\n0.13\n-0.18\n0.02\n0.98\n\n\nV5\n0.93\n-0.30\n-0.10\n-0.02\n0.97\n\n\nV13\n0.89\n0.30\n0.15\n0.23\n0.97\n\n\nV2\n0.07\n0.99\n0.39\n-0.00\n0.99\n\n\nV4\n0.29\n0.95\n0.32\n-0.09\n0.98\n\n\nV3\n-0.40\n0.82\n-0.02\n0.36\n0.92\n\n\nV6\n-0.37\n-0.70\n0.42\n0.43\n0.96\n\n\nV9\n0.55\n-0.63\n0.36\n0.14\n0.95\n\n\nV14\n0.28\n0.60\n-0.38\n0.42\n0.90\n\n\nV7\n-0.01\n0.40\n1.00\n0.12\n0.97\n\n\nV1\n-0.32\n-0.04\n0.95\n0.09\n0.69\n\n\nV12\n0.15\n0.01\n0.17\n0.87\n0.86\n\n\nV15\n-0.35\n-0.07\n-0.54\n0.59\n0.92\n\n\nV10\n0.40\n-0.51\n-0.17\n0.58\n0.99\n\n\nTotal Communalities\n\n14.03\n\n\nCronbach's α\n0.69\n0.62\n0.18\n0.43\n\n\n\n\n\n\n因子負荷の要約表（gt）\n\n\nコードを表示\n# Extract and tidy factor loadings\nload_tbl &lt;- fit_efa$loadings |&gt;\n  unclass() |&gt;\n  as.data.frame() |&gt;\n  rownames_to_column(var = \"Item\")\n\n# Create summary with max loading per item, communality, and uniqueness\nefa_summary &lt;- load_tbl |&gt;\n  pivot_longer(-Item, names_to = \"Factor\", values_to = \"Loading\") |&gt;\n  group_by(Item) |&gt;\n  mutate(\n    Abs = abs(Loading),\n    Communality = sum(Loading^2)  # Sum of squared loadings across all factors\n  ) |&gt;\n  slice_max(order_by = Abs, n = 1, with_ties = FALSE) |&gt;\n  ungroup() |&gt;\n  mutate(Uniqueness = 1 - pmin(Communality, 1)) |&gt;\n  select(Item, Factor, Loading, Communality, Uniqueness) |&gt;\n  arrange(Factor, desc(abs(Loading)))\n\n# Display in gt table\nefa_summary |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"EFA Summary Table\",\n    subtitle = \"Maximum Loading, Communality, and Uniqueness per Item\"\n  ) |&gt;\n  fmt_number(\n    columns = c(Loading, Communality, Uniqueness),\n    decimals = 3\n  ) |&gt;\n  cols_label(\n    Item = \"Item\",\n    Factor = \"Primary Factor\",\n    Loading = \"Max Loading\",\n    Communality = \"h²\",\n    Uniqueness = \"u²\"\n  ) |&gt;\n  tab_spanner(\n    label = \"Factor Solution\",\n    columns = c(Item, Factor, Loading)\n  ) |&gt;\n  tab_spanner(\n    label = \"Variance Components\",\n    columns = c(Communality, Uniqueness)\n  ) |&gt;\n  cols_align(\n    align = \"center\",\n    columns = -Item\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#f8f9fa\"),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_column_spanners()\n  ) |&gt;\n  tab_footnote(\n    footnote = \"h² = communality (proportion of variance explained by factors)\",\n    locations = cells_column_labels(columns = Communality)\n  ) |&gt;\n  tab_footnote(\n    footnote = \"u² = uniqueness (1 - communality)\",\n    locations = cells_column_labels(columns = Uniqueness)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEFA Summary Table\n\n\nMaximum Loading, Communality, and Uniqueness per Item\n\n\n\nFactor Solution\n\n\nVariance Components\n\n\n\nItem\nPrimary Factor\nMax Loading\nh²1\nu²2\n\n\n\n\nV11\nML1\n0.964\n0.962\n0.038\n\n\nV8\nML1\n0.946\n0.946\n0.054\n\n\nV5\nML1\n0.927\n0.961\n0.039\n\n\nV13\nML1\n0.889\n0.958\n0.042\n\n\nV2\nML2\n0.994\n1.148\n0.000\n\n\nV4\nML2\n0.951\n1.100\n0.000\n\n\nV3\nML2\n0.815\n0.953\n0.047\n\n\nV6\nML2\n−0.695\n0.984\n0.016\n\n\nV9\nML2\n−0.631\n0.857\n0.143\n\n\nV14\nML2\n0.598\n0.756\n0.244\n\n\nV7\nML3\n0.999\n1.168\n0.000\n\n\nV1\nML3\n0.949\n1.010\n0.000\n\n\nV12\nML4\n0.873\n0.817\n0.183\n\n\nV15\nML4\n0.588\n0.765\n0.235\n\n\nV10\nML4\n0.580\n0.784\n0.216\n\n\n\n1 h² = communality (proportion of variance explained by factors)\n\n\n2 u² = uniqueness (1 - communality)\n\n\n\n\n\n\n\n\n上表は各項目が最も強く負荷する因子と、その共通性（因子で説明される割合）、独自性を並べたものです。共通性が低い項目はスケールからの除外や項目修正の候補になります。\n\n\n13.1.4 信頼性（α係数・ω係数）\n\n\nコードを表示\n# Select items loading highest on Factor 1 (example)\nitems_f1 &lt;- c(\"V6\", \"V14\", \"V1\", \"V7\")  # Adjust based on actual data\n\n# Calculate reliability coefficients\nalpha_result &lt;- psych::alpha(survey[, items_f1])\n\n\nSome items ( V14 ) were negatively correlated with the first principal component and \nprobably should be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\n\n\nコードを表示\nomega_result &lt;- psych::omega(survey[, items_f1], plot = FALSE)\n\n# Create reliability summary table\ndata.frame(\n  Coefficient = c(\"Cronbach's α\", \"McDonald's ω\"),\n  Value = c(alpha_result$total$raw_alpha, omega_result$omega.tot),\n  CI_lower = c(alpha_result$total$ase * -1.96 + alpha_result$total$raw_alpha,\n               NA),\n  CI_upper = c(alpha_result$total$ase * 1.96 + alpha_result$total$raw_alpha,\n               NA)\n) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Internal Consistency Reliability\",\n    subtitle = paste(\"Factor 1 items:\", paste(items_f1, collapse = \", \"))\n  ) |&gt;\n  fmt_number(\n    columns = c(Value, CI_lower, CI_upper),\n    decimals = 3\n  ) |&gt;\n  cols_merge(\n    columns = c(CI_lower, CI_upper),\n    pattern = \"[{1}, {2}]\"\n  ) |&gt;\n  cols_label(\n    Coefficient = \"Coefficient\",\n    Value = \"Estimate\",\n    CI_lower = \"95% CI\"\n  ) |&gt;\n  sub_missing(\n    columns = CI_lower,\n    missing_text = \"—\"\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#e6ffe6\"),\n    locations = cells_body(\n      columns = Value,\n      rows = Value &gt; 0.7\n    )\n  ) |&gt;\n  tab_footnote(\n    footnote = \"Values &gt; 0.7 indicate acceptable reliability\",\n    locations = cells_column_labels(columns = Value)\n  )\n\n\n\n\n\n\n\n\nInternal Consistency Reliability\n\n\nFactor 1 items: V6, V14, V1, V7\n\n\nCoefficient\nEstimate1\n95% CI\n\n\n\n\nCronbach's α\n0.212\n[0.136, 0.289]\n\n\nMcDonald's ω\n0.927\n[—, NA]\n\n\n\n1 Values &gt; 0.7 indicate acceptable reliability\n\n\n\n\n\n\n\n\n\n\n13.1.5 可視化\n\n\nコードを表示\n# Factor structure diagram\npsych::fa.diagram(fit_efa)\n\n\n\n\n\n\n\n\n\nコードを表示\n# Create loading bar plot\nloadings_df &lt;- as.data.frame.matrix(unclass(fit_efa$loadings))\nloadings_df$Item &lt;- rownames(loadings_df)\nloadings_long &lt;- tidyr::pivot_longer(loadings_df, cols = -Item, names_to = \"Factor\", values_to = \"Loading\")\n\nggplot(loadings_long, aes(x = Item, y = abs(Loading), fill = Factor)) +\n  geom_col(position = \"dodge\") +\n  coord_flip() +\n  labs(\n    title = \"Factor Loadings by Item\",\n    x = \"Item\",\n    y = \"|Loading|\"\n  ) +\n  theme_bw() +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\n\n因子負荷のヒートマップ（tidyverse）\n\n\nコードを表示\n# Prepare heatmap data\nheat_tbl &lt;- as.data.frame(unclass(fit_efa$loadings)) |&gt;\n  rownames_to_column(\"Item\") |&gt;\n  pivot_longer(-Item, names_to = \"Factor\", values_to = \"Loading\") |&gt;\n  mutate(Abs = abs(Loading))\n\n# Create heatmap\nggplot(heat_tbl, aes(Factor, Item, fill = Loading)) +\n  geom_tile() +\n  scale_fill_gradient2(\n    low = \"blue\",\n    mid = \"white\",\n    high = \"red\",\n    midpoint = 0,\n    name = \"Loading\"\n  ) +\n  labs(\n    title = \"EFA Loadings Heatmap\",\n    subtitle = \"Red = positive loading, Blue = negative loading\"\n  ) +\n  theme_bw() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法</span>"
    ]
  },
  {
    "objectID": "9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法.html#lavaan-の記法とデータ",
    "href": "9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法.html#lavaan-の記法とデータ",
    "title": "11  9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法",
    "section": "14.1 lavaan の記法とデータ",
    "text": "14.1 lavaan の記法とデータ\n=~ は「〜により測定」を表し、~~ は（共）分散、~ は回帰、~1 は切片を表します。\n\n\nコードを表示\n# Load example data from lavaan\ndata(HolzingerSwineford1939)\n# Assumed: x1-x3 = visual, x4-x6 = textual, x7-x9 = speed",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法</span>"
    ]
  },
  {
    "objectID": "9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法.html#モデル定義と推定",
    "href": "9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法.html#モデル定義と推定",
    "title": "11  9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法",
    "section": "14.2 モデル定義と推定",
    "text": "14.2 モデル定義と推定\n\n\nコードを表示\n# Define CFA models\nmodel_1 &lt;- '\n  visual  =~ x1 + x2 + x3\n  textual =~ x4 + x5 + x6\n  speed   =~ x7 + x8 + x9\n'\n\nmodel_2 &lt;- '\n  visual  =~ x1 + x2\n  textual =~ x4 + x5\n  speed   =~ x7 + x8\n'\n\n# Fit models\nfit_cfa_1 &lt;- cfa(model_1, data = HolzingerSwineford1939, std.lv = TRUE)\nfit_cfa_2 &lt;- cfa(model_2, data = HolzingerSwineford1939, std.lv = TRUE)\n\n# Display summary\nsummary(fit_cfa_1, fit.measures = TRUE, standardized = TRUE)\n\n\nlavaan 0.6-19 ended normally after 20 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n  Number of observations                           301\n\nModel Test User Model:\n                                                      \n  Test statistic                                85.306\n  Degrees of freedom                                24\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               918.852\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.931\n  Tucker-Lewis Index (TLI)                       0.896\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3737.745\n  Loglikelihood unrestricted model (H1)      -3695.092\n                                                      \n  Akaike (AIC)                                7517.490\n  Bayesian (BIC)                              7595.339\n  Sample-size adjusted Bayesian (SABIC)       7528.739\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.092\n  90 Percent confidence interval - lower         0.071\n  90 Percent confidence interval - upper         0.114\n  P-value H_0: RMSEA &lt;= 0.050                    0.001\n  P-value H_0: RMSEA &gt;= 0.080                    0.840\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.065\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  visual =~                                                             \n    x1                0.900    0.081   11.128    0.000    0.900    0.772\n    x2                0.498    0.077    6.429    0.000    0.498    0.424\n    x3                0.656    0.074    8.817    0.000    0.656    0.581\n  textual =~                                                            \n    x4                0.990    0.057   17.474    0.000    0.990    0.852\n    x5                1.102    0.063   17.576    0.000    1.102    0.855\n    x6                0.917    0.054   17.082    0.000    0.917    0.838\n  speed =~                                                              \n    x7                0.619    0.070    8.903    0.000    0.619    0.570\n    x8                0.731    0.066   11.090    0.000    0.731    0.723\n    x9                0.670    0.065   10.305    0.000    0.670    0.665\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  visual ~~                                                             \n    textual           0.459    0.064    7.189    0.000    0.459    0.459\n    speed             0.471    0.073    6.461    0.000    0.471    0.471\n  textual ~~                                                            \n    speed             0.283    0.069    4.117    0.000    0.283    0.283\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .x1                0.549    0.114    4.833    0.000    0.549    0.404\n   .x2                1.134    0.102   11.146    0.000    1.134    0.821\n   .x3                0.844    0.091    9.317    0.000    0.844    0.662\n   .x4                0.371    0.048    7.779    0.000    0.371    0.275\n   .x5                0.446    0.058    7.642    0.000    0.446    0.269\n   .x6                0.356    0.043    8.277    0.000    0.356    0.298\n   .x7                0.799    0.081    9.823    0.000    0.799    0.676\n   .x8                0.488    0.074    6.573    0.000    0.488    0.477\n   .x9                0.566    0.071    8.003    0.000    0.566    0.558\n    visual            1.000                               1.000    1.000\n    textual           1.000                               1.000    1.000\n    speed             1.000                               1.000    1.000\n\n\nコードを表示\n# Extract and display fit indices\nfit_indices_1 &lt;- fitMeasures(fit_cfa_1, c(\"chisq\",\"df\",\"pvalue\",\"cfi\",\"tli\",\"rmsea\",\"srmr\"))\nfit_indices_2 &lt;- fitMeasures(fit_cfa_2, c(\"chisq\",\"df\",\"pvalue\",\"cfi\",\"tli\",\"rmsea\",\"srmr\"))\n\n# Create fit indices comparison table\ndata.frame(\n  Measure = names(fit_indices_1),\n  Model_1 = as.numeric(fit_indices_1),\n  Model_2 = as.numeric(fit_indices_2)\n) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"CFA Model Fit Comparison\",\n    subtitle = \"Model 1 (3 indicators per factor) vs Model 2 (2 indicators per factor)\"\n  ) |&gt;\n  fmt_number(\n    columns = c(Model_1, Model_2),\n    rows = Measure %in% c(\"chisq\", \"df\"),\n    decimals = 0\n  ) |&gt;\n  fmt_number(\n    columns = c(Model_1, Model_2),\n    rows = Measure %in% c(\"cfi\", \"tli\", \"rmsea\", \"srmr\"),\n    decimals = 3\n  ) |&gt;\n  fmt_scientific(\n    columns = c(Model_1, Model_2),\n    rows = Measure == \"pvalue\",\n    decimals = 3\n  ) |&gt;\n  cols_label(\n    Measure = \"Fit Index\",\n    Model_1 = \"Model 1\",\n    Model_2 = \"Model 2\"\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#e6ffe6\"),\n    locations = cells_body(\n      columns = Model_1,\n      rows = (Measure %in% c(\"cfi\", \"tli\") & Model_1 &gt; 0.95) |\n             (Measure == \"rmsea\" & Model_1 &lt; 0.05) |\n             (Measure == \"srmr\" & Model_1 &lt; 0.08)\n    )\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#e6ffe6\"),\n    locations = cells_body(\n      columns = Model_2,\n      rows = (Measure %in% c(\"cfi\", \"tli\") & Model_2 &gt; 0.95) |\n             (Measure == \"rmsea\" & Model_2 &lt; 0.05) |\n             (Measure == \"srmr\" & Model_2 &lt; 0.08)\n    )\n  ) |&gt;\n  tab_footnote(\n    footnote = \"Good fit: CFI/TLI &gt; 0.95, RMSEA &lt; 0.05, SRMR &lt; 0.08\",\n    locations = cells_column_labels(columns = c(Model_1, Model_2))\n  )\n\n\n\n\n\n\n\n\nCFA Model Fit Comparison\n\n\nModel 1 (3 indicators per factor) vs Model 2 (2 indicators per factor)\n\n\nFit Index\nModel 11\nModel 21\n\n\n\n\nchisq\n85\n19\n\n\ndf\n24\n6\n\n\npvalue\n8.503 × 10−9\n4.767 × 10−3\n\n\ncfi\n0.931\n0.969\n\n\ntli\n0.896\n0.922\n\n\nrmsea\n0.092\n0.084\n\n\nsrmr\n0.065\n0.057\n\n\n\n1 Good fit: CFI/TLI &gt; 0.95, RMSEA &lt; 0.05, SRMR &lt; 0.08\n\n\n\n\n\n\n\n\nモデル適合度の読み方: CFI/TLI は 0.90 以上（望ましくは 0.95 以上）、RMSEA は 0.08 未満（望ましくは 0.05 未満）、SRMR は 0.08 未満が一つの目安です。適合だけでなく、理論整合性・パラメータの解釈可能性・識別性を合わせて評価します。\n\n14.2.1 可視化と含意共分散\n\n\nコードを表示\n# Path diagrams\nsemPaths(fit_cfa_1, whatLabels = \"est\", layout = \"tree\", rotation = 2)\n\n\n\n\n\n\n\n\n\nコードを表示\nsemPaths(fit_cfa_1, what = \"stand\", style = \"lisrel\")\n\n\n\n\n\n\n\n\n\nコードを表示\n# Display implied covariance matrix\nimplied_cov &lt;- lavInspect(fit_cfa_1, what = \"implied\")$cov\n\n# Show first 6x6 of implied covariance matrix\nas.data.frame(implied_cov[1:6, 1:6]) |&gt;\n  rownames_to_column(\"Variable\") |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Model Implied Covariance Matrix\",\n    subtitle = \"First 6 variables shown\"\n  ) |&gt;\n  fmt_number(\n    columns = -Variable,\n    decimals = 3\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#f8f9fa\"),\n    locations = cells_body(columns = Variable)\n  )\n\n\n\n\n\n\n\n\nModel Implied Covariance Matrix\n\n\nFirst 6 variables shown\n\n\nVariable\nx1\nx2\nx3\nx4\nx5\nx6\n\n\n\n\nx1\n1.358\n0.448\n0.590\n0.408\n0.454\n0.378\n\n\nx2\n0.448\n1.382\n0.327\n0.226\n0.252\n0.209\n\n\nx3\n0.590\n0.327\n1.275\n0.298\n0.331\n0.276\n\n\nx4\n0.408\n0.226\n0.298\n1.351\n1.090\n0.907\n\n\nx5\n0.454\n0.252\n0.331\n1.090\n1.660\n1.010\n\n\nx6\n0.378\n0.209\n0.276\n0.907\n1.010\n1.196",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法</span>"
    ]
  },
  {
    "objectID": "9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法.html#ca対応分析の例",
    "href": "9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法.html#ca対応分析の例",
    "title": "11  9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法",
    "section": "16.1 CA（対応分析）の例",
    "text": "16.1 CA（対応分析）の例\n\n\nコードを表示\n# Generate sample data for CA\nset.seed(123)\ndf_ca &lt;- data.frame(\n  RowCat = sample(c(\"TypeA\", \"TypeB\", \"TypeC\"), 100, replace = TRUE, \n                  prob = c(0.5, 0.3, 0.2)),\n  ColCat = sample(c(\"Low\", \"Medium\", \"High\"), 100, replace = TRUE,\n                  prob = c(0.3, 0.5, 0.2))\n)\n\n# Create contingency table\ntab_ca &lt;- as.data.frame.matrix(table(df_ca$RowCat, df_ca$ColCat))\n\n# Display contingency table\ntab_ca |&gt;\n  rownames_to_column(\"Row Category\") |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Contingency Table for CA\",\n    subtitle = \"Row categories vs Column categories\"\n  ) |&gt;\n  cols_label(\n    `Row Category` = \"Category\",\n    Low = \"Low\",\n    Medium = \"Medium\",\n    High = \"High\"\n  ) |&gt;\n  tab_spanner(\n    label = \"Column Categories\",\n    columns = c(Low, Medium, High)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContingency Table for CA\n\n\nRow categories vs Column categories\n\n\nCategory\n\nColumn Categories\n\n\n\nLow\nMedium\nHigh\n\n\n\n\nTypeA\n18\n25\n10\n\n\nTypeB\n7\n17\n5\n\n\nTypeC\n6\n8\n4\n\n\n\n\n\n\n\nコードを表示\n# Perform CA\nres_ca &lt;- CA(tab_ca, graph = FALSE)\n\n# Visualize\nfviz_ca_biplot(res_ca, repel = TRUE)\n\n\n\n\n\n\n\n\n\nコードを表示\n# Display eigenvalues and contributions\nca_eig &lt;- as_tibble(res_ca$eig) |&gt;\n  mutate(Dimension = row_number()) |&gt;\n  select(Dimension, everything())\n\nca_eig |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"CA Eigenvalues and Inertia\",\n    subtitle = \"Decomposition of total inertia\"\n  ) |&gt;\n  fmt_number(\n    columns = -Dimension,\n    decimals = 3\n  ) |&gt;\n  cols_label(\n    Dimension = \"Dim\",\n    eigenvalue = \"Eigenvalue\",\n    `percentage of variance` = \"% Inertia\",\n    `cumulative percentage of variance` = \"Cumulative %\"\n  ) |&gt;\n  tab_footnote(\n    footnote = \"Total inertia = chi-square statistic / n\",\n    locations = cells_column_labels(columns = eigenvalue)\n  )\n\n\n\n\n\n\n\n\nCA Eigenvalues and Inertia\n\n\nDecomposition of total inertia\n\n\nDim\nEigenvalue1\n% Inertia\nCumulative %\n\n\n\n\n1\n0.013\n94.078\n94.078\n\n\n2\n0.001\n5.922\n100.000\n\n\n\n1 Total inertia = chi-square statistic / n\n\n\n\n\n\n\n\n\nコードを表示\n# Row contributions to Dimension 1\ncontrib_row &lt;- as_tibble(res_ca$row$contrib[,1, drop = FALSE], rownames = \"Category\") |&gt;\n  rename(Contrib_Dim1 = `Dim 1`) |&gt;\n  arrange(desc(Contrib_Dim1))\n\ncontrib_row |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Row Category Contributions\",\n    subtitle = \"Contribution to Dimension 1 (%)\"\n  ) |&gt;\n  fmt_number(\n    columns = Contrib_Dim1,\n    decimals = 2\n  ) |&gt;\n  cols_label(\n    Category = \"Row Category\",\n    Contrib_Dim1 = \"Contribution (%)\"\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#e6f3ff\"),\n    locations = cells_body(\n      columns = Contrib_Dim1,\n      rows = Contrib_Dim1 &gt; 30\n    )\n  )\n\n\n\n\n\n\n\n\nRow Category Contributions\n\n\nContribution to Dimension 1 (%)\n\n\nRow Category\nContribution (%)\n\n\n\n\nTypeB\n69.92\n\n\nTypeA\n15.98\n\n\nTypeC\n14.09",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法</span>"
    ]
  },
  {
    "objectID": "9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法.html#mca多重対応分析の例",
    "href": "9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法.html#mca多重対応分析の例",
    "title": "11  9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法",
    "section": "16.2 MCA（多重対応分析）の例",
    "text": "16.2 MCA（多重対応分析）の例\n\n\nコードを表示\n# Generate sample data for MCA\nset.seed(123)\ndf_mca &lt;- data.frame(\n  Q1 = sample(c(\"Yes\", \"No\", \"Maybe\"), 150, replace = TRUE),\n  Q2 = sample(c(\"Low\", \"Medium\", \"High\"), 150, replace = TRUE),\n  Q3 = sample(c(\"A\", \"B\", \"C\", \"D\"), 150, replace = TRUE),\n  Q4 = sample(c(\"Agree\", \"Neutral\", \"Disagree\"), 150, replace = TRUE)\n)\n\n# Perform MCA\nres_mca &lt;- MCA(df_mca, graph = FALSE)\n\n# Visualizations\nfviz_mca_biplot(res_mca, repel = TRUE)\n\n\n\n\n\n\n\n\n\nコードを表示\n# Individual plot colored by Q2\nfviz_mca_ind(res_mca,\n             col.ind = df_mca$Q2,\n             palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             addEllipses = TRUE,\n             ellipse.type = \"confidence\",\n             legend.title = \"Q2 Level\",\n             repel = TRUE)\n\n\n\n\n\n\n\n\n\nコードを表示\n# Variable categories plot\nfviz_mca_var(res_mca,\n             col.var = \"contrib\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE)\n\n\n\n\n\n\n\n\n\nコードを表示\n# Display eigenvalues\nmca_eig &lt;- as_tibble(res_mca$eig) |&gt;\n  mutate(Dimension = row_number()) |&gt;\n  select(Dimension, everything())\n\nmca_eig |&gt;\n  slice(1:5) |&gt;  # Show top 5 dimensions\n  gt() |&gt;\n  tab_header(\n    title = \"MCA Eigenvalues and Inertia\",\n    subtitle = \"Top 5 dimensions\"\n  ) |&gt;\n  fmt_number(\n    columns = -Dimension,\n    decimals = 3\n  ) |&gt;\n  cols_label(\n    Dimension = \"Dim\",\n    eigenvalue = \"Eigenvalue\",\n    `percentage of variance` = \"% Inertia\",\n    `cumulative percentage of variance` = \"Cumulative %\"\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#fff3cd\"),\n    locations = cells_body(\n      columns = `cumulative percentage of variance`,\n      rows = 1:2\n    )\n  ) |&gt;\n  tab_footnote(\n    footnote = \"MCA typically requires more dimensions than CA\",\n    locations = cells_column_labels(columns = `cumulative percentage of variance`)\n  )\n\n\n\n\n\n\n\n\nMCA Eigenvalues and Inertia\n\n\nTop 5 dimensions\n\n\nDim\nEigenvalue\n% Inertia\nCumulative %1\n\n\n\n\n1\n0.334\n14.842\n14.842\n\n\n2\n0.318\n14.144\n28.986\n\n\n3\n0.310\n13.767\n42.753\n\n\n4\n0.269\n11.934\n54.687\n\n\n5\n0.247\n10.996\n65.683\n\n\n\n1 MCA typically requires more dimensions than CA\n\n\n\n\n\n\n\n\nコードを表示\n# Variable contributions\nvar_contrib &lt;- as_tibble(res_mca$var$contrib[,1:2], rownames = \"Category\") |&gt;\n  rename(Dim1 = `Dim 1`, Dim2 = `Dim 2`) |&gt;\n  arrange(desc(Dim1)) |&gt;\n  slice(1:10)  # Top 10 categories\n\nvar_contrib |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"MCA Variable Category Contributions\",\n    subtitle = \"Top 10 categories by Dimension 1 contribution\"\n  ) |&gt;\n  fmt_number(\n    columns = c(Dim1, Dim2),\n    decimals = 2\n  ) |&gt;\n  cols_label(\n    Category = \"Category\",\n    Dim1 = \"Dim 1 (%)\",\n    Dim2 = \"Dim 2 (%)\"\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#e6f3ff\"),\n    locations = cells_body(\n      columns = Dim1,\n      rows = Dim1 &gt; 10\n    )\n  )\n\n\n\n\n\n\n\n\nMCA Variable Category Contributions\n\n\nTop 10 categories by Dimension 1 contribution\n\n\nCategory\nDim 1 (%)\nDim 2 (%)\n\n\n\n\nHigh\n21.46\n3.19\n\n\nMaybe\n16.25\n2.09\n\n\nLow\n10.07\n0.34\n\n\nNeutral\n9.16\n1.82\n\n\nNo\n9.13\n10.32\n\n\nAgree\n8.22\n0.08\n\n\nA\n6.84\n27.27\n\n\nC\n6.15\n1.93\n\n\nD\n5.63\n21.01\n\n\nB\n5.33\n1.01",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法</span>"
    ]
  },
  {
    "objectID": "9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法.html#tidyverseスタイルでの可視化",
    "href": "9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法.html#tidyverseスタイルでの可視化",
    "title": "11  9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法",
    "section": "16.3 tidyverseスタイルでの可視化",
    "text": "16.3 tidyverseスタイルでの可視化\n\n\nコードを表示\n# CA biplot with tidyverse\nrow_coords &lt;- as_tibble(res_ca$row$coord, rownames = \"RowCategory\")\ncol_coords &lt;- as_tibble(res_ca$col$coord, rownames = \"ColCategory\")\n\n# Combined plot\nggplot() +\n  geom_point(data = row_coords, \n             aes(x = `Dim 1`, y = `Dim 2`), \n             color = \"blue\", size = 4, shape = 17) +\n  geom_text(data = row_coords, \n            aes(x = `Dim 1`, y = `Dim 2`, label = RowCategory), \n            vjust = -1.5, color = \"blue\", size = 4) +\n  geom_point(data = col_coords, \n             aes(x = `Dim 1`, y = `Dim 2`), \n             color = \"red\", size = 4, shape = 16) +\n  geom_text(data = col_coords, \n            aes(x = `Dim 1`, y = `Dim 2`, label = ColCategory), \n            vjust = -1.5, color = \"red\", size = 4) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  labs(title = \"CA Biplot (tidyverse style)\",\n       subtitle = \"Blue triangles: row categories, Red circles: column categories\",\n       x = paste0(\"Dim 1 (\", round(res_ca$eig[1,2], 1), \"%)\"),\n       y = paste0(\"Dim 2 (\", round(res_ca$eig[2,2], 1), \"%)\")) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(face = \"italic\")\n  )\n\n\n\n\n\n\n\n\n\nコードを表示\n# MCA biplot with tidyverse\nind_coords &lt;- as_tibble(res_mca$ind$coord[,1:2]) |&gt;\n  mutate(Q2 = df_mca$Q2)\n\nvar_coords &lt;- as_tibble(res_mca$var$coord[,1:2], rownames = \"Variable\")\n\n# MCA biplot\nggplot() +\n  geom_point(data = ind_coords, \n             aes(x = `Dim 1`, y = `Dim 2`, color = Q2), \n             alpha = 0.4, size = 2) +\n  geom_point(data = var_coords, \n             aes(x = `Dim 1`, y = `Dim 2`), \n             color = \"black\", size = 3, shape = 15) +\n  geom_text(data = var_coords, \n            aes(x = `Dim 1`, y = `Dim 2`, label = Variable), \n            vjust = -1, size = 3) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  labs(title = \"MCA Biplot (tidyverse style)\",\n       subtitle = \"Points: individuals colored by Q2, Squares: variable categories\",\n       x = paste0(\"Dim 1 (\", round(res_mca$eig[1,2], 1), \"%)\"),\n       y = paste0(\"Dim 2 (\", round(res_mca$eig[2,2], 1), \"%)\")) +\n  scale_color_manual(values = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(face = \"italic\"),\n    legend.title = element_text(face = \"bold\")\n  )",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9_因子分析_主成分分析_コレスポンデンス分析_多次元尺度法</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html",
    "href": "10_一般化可能性理論.html",
    "title": "12  一般化可能性理論",
    "section": "",
    "text": "13 はじめに\n本文書では、一般化可能性理論（G理論）の全ての分析をlme4パッケージの線形混合効果モデル（LMM）で実装します。G理論の分散成分分析は本質的にランダム効果ANOVAモデルであるため、lmer()関数で完全に置き換え可能です。",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#必要なパッケージとデータの準備",
    "href": "10_一般化可能性理論.html#必要なパッケージとデータの準備",
    "title": "12  一般化可能性理論",
    "section": "13.1 必要なパッケージとデータの準備",
    "text": "13.1 必要なパッケージとデータの準備\n\n\nコードを表示\n# パッケージの読み込み\npacman::p_load(\n  tidyverse, lmerTest, gt, performance, sjPlot,\n  effectsize, broom.mixed, gridExtra)\n\n# テーマと乱数種の設定\ntheme_set(theme_bw())\nset.seed(123)\noptions(digits = 4)\nknitr::opts_chunk$set(fig.align = \"center\")",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#サンプルデータの作成",
    "href": "10_一般化可能性理論.html#サンプルデータの作成",
    "title": "12  一般化可能性理論",
    "section": "13.2 サンプルデータの作成",
    "text": "13.2 サンプルデータの作成\n10人の受験者、3タスク、タスクごとに異なる評価者群（ネスト構造）\n\n\nコードを表示\ncreate_brennan_data &lt;- function() {\n  n_persons &lt;- 10\n  n_tasks &lt;- 3\n  n_raters_per_task &lt;- 4\n  \n  var_person &lt;- 0.37\n  var_task &lt;- 0.05\n  var_rater_in_task &lt;- 0.10\n  var_person_task &lt;- 0.15\n  var_residual &lt;- 0.40\n  \n  data &lt;- expand.grid(\n    Person = factor(1:n_persons),\n    Task = factor(1:n_tasks),\n    Rater = factor(1:n_raters_per_task))\n  \n  # ランダム効果の生成\n  person_effects &lt;- rnorm(n_persons, 0, sqrt(var_person))\n  task_effects &lt;- rnorm(n_tasks, 0, sqrt(var_task))\n  \n  # ネストされた評価者効果（タスクごとに異なる）\n  rater_effects &lt;- matrix(\n    rnorm(n_tasks * n_raters_per_task, 0, sqrt(var_rater_in_task)),\n    nrow = n_tasks, ncol = n_raters_per_task)\n  \n  # Person×Task交互作用\n  person_task_effects &lt;- matrix(\n    rnorm(n_persons * n_tasks, 0, sqrt(var_person_task)),\n    nrow = n_persons, ncol = n_tasks)\n  \n  # スコアの計算\n  data$Score &lt;- NA\n  for(i in 1:nrow(data)) {\n    p &lt;- as.numeric(data$Person[i])\n    t &lt;- as.numeric(data$Task[i])\n    r &lt;- as.numeric(data$Rater[i])\n    \n    data$Score[i] &lt;- 5 +  # 全体平均\n      person_effects[p] +\n      task_effects[t] +\n      rater_effects[t, r] +\n      person_task_effects[p, t] +\n      rnorm(1, 0, sqrt(var_residual))\n  }\n  \n  # 評価者IDをタスクにネストされた形式に変更\n  data$Rater &lt;- factor(paste0(data$Task, \"_\", data$Rater))\n  \n  return(data)\n}\n\n# データ生成\nBrennan.3.2 &lt;- create_brennan_data()\n\n# データ構造の確認\nBrennan.3.2 |&gt;\n  head(12) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Brennan.3.2 dataset (for lme4)\",\n    subtitle = \"First 12 rows\") |&gt;\n  cols_label(\n    Task = \"Task\",\n    Person = \"Person\",\n    Rater = \"Rater\",\n    Score = \"Score\")\n\n\n\n\n\n\n\n\nBrennan.3.2 dataset (for lme4)\n\n\nFirst 12 rows\n\n\nPerson\nTask\nRater\nScore\n\n\n\n\n1\n1\n1_1\n5.274\n\n\n2\n1\n1_1\n4.514\n\n\n3\n1\n1_1\n6.686\n\n\n4\n1\n1_1\n4.989\n\n\n5\n1\n1_1\n6.010\n\n\n6\n1\n1_1\n6.757\n\n\n7\n1\n1_1\n5.157\n\n\n8\n1\n1_1\n4.675\n\n\n9\n1\n1_1\n4.587\n\n\n10\n1\n1_1\n4.678\n\n\n1\n2\n2_1\n5.022\n\n\n2\n2\n2_1\n5.263\n\n\n\n\n\n\n\n\n\nコードを表示\n# lme4::lmer の VarCorr から G theoryで使う分散成分テーブルを作る\nextract_variance_components &lt;- function(model) {\n  vc &lt;- VarCorr(model)\n\n  # 各分散成分（存在しない場合は0で安全に処理）\n  get_v &lt;- function(name) {\n    if (name %in% names(vc)) as.numeric(vc[[name]]) else 0\n  }\n\n  var_person &lt;- get_v(\"Person\")\n  var_task &lt;- get_v(\"Task\")\n  var_rater &lt;- get_v(\"Rater\")\n  var_person_task &lt;- get_v(\"Person:Task\")\n  var_residual &lt;- attr(vc, \"sc\")^2\n\n  total &lt;- var_person + var_task + var_rater + var_person_task + var_residual\n\n  # グルーピング因子の水準数（n）も併記\n  fl &lt;- lme4::getME(model, \"flist\")\n  n_person &lt;- if (\"Person\" %in% names(fl)) nlevels(fl[[\"Person\"]]) else NA_integer_\n  n_task &lt;- if (\"Task\" %in% names(fl)) nlevels(fl[[\"Task\"]]) else NA_integer_\n  n_rater &lt;- if (\"Rater\" %in% names(fl)) nlevels(fl[[\"Rater\"]]) else NA_integer_\n  n_person_task &lt;- if (\"Person:Task\" %in% names(fl)) nlevels(fl[[\"Person:Task\"]]) else NA_integer_\n\n  tibble::tibble(\n    source  = factor(\n      c(\"Person\", \"Task\", \"Rater\", \"Person:Task\", \"Residual\"),\n      levels = c(\"Person\", \"Task\", \"Rater\", \"Person:Task\", \"Residual\")),\n    var     = c(var_person, var_task, var_rater, var_person_task, var_residual),\n    percent = 100 * var / total,\n    n       = c(n_person, n_task, n_rater, n_person_task, NA_integer_))\n}",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#データの可視化",
    "href": "10_一般化可能性理論.html#データの可視化",
    "title": "12  一般化可能性理論",
    "section": "13.3 データの可視化",
    "text": "13.3 データの可視化\n\n\nコードを表示\n# タスク別・評価者別のスコア分布\np1 &lt;- ggplot(Brennan.3.2, aes(x = Person, y = Score, color = Rater)) +\n  geom_point(size = 2, alpha = 0.7) +\n  facet_wrap(~ Task, labeller = label_both) +\n  labs(\n    title = \"Scores by task\",\n    x = \"Person\",\n    y = \"Score\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n# 受験者ごとの平均スコア\np2 &lt;- Brennan.3.2 |&gt;\n  group_by(Person) |&gt;\n  summarise(\n    Mean = mean(Score),\n    SD = sd(Score),\n    .groups = \"drop\") |&gt;\n  ggplot(aes(x = Person, y = Mean)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = Mean - SD, ymax = Mean + SD), width = 0.2) +\n  labs(\n    title = \"Mean score by person\",\n    x = \"Person\",\n    y = \"Mean score (±SD)\") +\n  theme_bw()\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\nScore distribution",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#線形混合効果モデルによる分散成分推定",
    "href": "10_一般化可能性理論.html#線形混合効果モデルによる分散成分推定",
    "title": "12  一般化可能性理論",
    "section": "14.1 線形混合効果モデルによる分散成分推定",
    "text": "14.1 線形混合効果モデルによる分散成分推定\n\n\nコードを表示\ng_model &lt;- lmer(\n  Score ~ 1 + \n    (1 | Person) + \n    (1 | Task) + \n    (1 | Rater) +\n    (1 | Person:Task),\n  data = Brennan.3.2,\n  REML = TRUE)\n\ntab_model(g_model)\n\n\n\n\n \nScore\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n5.14\n4.68 – 5.61\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n0.40\n\n\n\nτ00 Person:Task\n0.12\n\n\nτ00 Rater\n0.07\n\n\nτ00 Person\n0.33\n\n\nτ00 Task\n0.03\n\n\nICC\n0.58\n\n\nN Person\n10\n\n\nN Task\n3\n\n\nN Rater\n12\n\nObservations\n120\n\n\nMarginal R2 / Conditional R2\n0.000 / 0.581\n\n\n\n\n\n\n\nコードを表示\n# 分散成分の抽出と表示\ng_study_results &lt;- extract_variance_components(g_model)\n\n# まずgtテーブルを作成\ng_study_table &lt;- g_study_results |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"G-study: variance component estimates (lme4)\",\n    subtitle = \"Estimated via linear mixed-effects model\") |&gt;\n  fmt_number(\n    columns = var,\n    decimals = 4) |&gt;\n  fmt_number(\n    columns = percent,\n    decimals = 1) |&gt;\n  fmt_number(\n    columns = n,\n    decimals = 0) |&gt;\n  cols_label(\n    source = \"Source\",\n    var = \"Variance\",\n    percent = \"Percent (%)\",\n    n = \"n\") |&gt;\n  tab_footnote(\n    footnote = \"Higher Person variance indicates higher reliability.\",\n    locations = cells_column_labels(columns = percent)) |&gt;\n  tab_source_note(\n    source_note = \"Note: Large residual variance suggests room for measurement improvement.\")\n\nperson_row &lt;- which(g_study_results$source == \"Person\")\nif (length(person_row) &gt; 0) {\n  g_study_table &lt;- g_study_table |&gt;\n    tab_style(\n      style = list(\n        cell_fill(color = \"#e6f3ff\"),\n        cell_text(weight = \"bold\")),\n      locations = cells_body(\n        columns = percent,\n        rows = person_row))\n  }\n\n# Residual行をハイライト\nresidual_row &lt;- which(g_study_results$source == \"Residual\")\nif (length(residual_row) &gt; 0) {\n  g_study_table &lt;- g_study_table |&gt;\n    tab_style(\n      style = cell_fill(color = \"#ffe6e6\"),\n      locations = cells_body(\n        columns = percent,\n        rows = residual_row))\n}\n\ng_study_table\n\n\n\n\n\n  \n    \n      G-study: variance component estimates (lme4)\n    \n    \n      Estimated via linear mixed-effects model\n    \n    \n      Source\n      Variance\n      Percent (%)1\n      n\n    \n  \n  \n    Person\n0.3348\n35.2\n10\n    Task\n0.0267\n2.8\n3\n    Rater\n0.0691\n7.3\n12\n    Person:Task\n0.1220\n12.8\n30\n    Residual\n0.3987\n41.9\nNA\n  \n  \n    \n      Note: Large residual variance suggests room for measurement improvement.\n    \n  \n  \n    \n      1 Higher Person variance indicates higher reliability.",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#分散成分の可視化",
    "href": "10_一般化可能性理論.html#分散成分の可視化",
    "title": "12  一般化可能性理論",
    "section": "14.2 分散成分の可視化",
    "text": "14.2 分散成分の可視化\n\n\nコードを表示\nggplot(g_study_results, aes(x = reorder(source, -percent), y = percent, fill = source)) +\n  geom_col(alpha = 0.8) +\n  geom_text(aes(label = paste0(round(percent, 1), \"%\")), \n            vjust = -0.5, size = 4) +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"G-study: variance decomposition (lme4)\",\n    subtitle = \"Estimated via linear mixed-effects model\",\n    x = \"Source\",\n    y = \"Percent of total variance (%)\"\n  ) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nVariance components (lme4)",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#信頼性係数の計算関数",
    "href": "10_一般化可能性理論.html#信頼性係数の計算関数",
    "title": "12  一般化可能性理論",
    "section": "15.1 信頼性係数の計算関数",
    "text": "15.1 信頼性係数の計算関数\n\n\nコードを表示\n# G係数（相対的決定）の計算\ncalculate_g_coefficient &lt;- function(var_components, n_tasks, n_raters) {\n  var_p &lt;- var_components$var[var_components$source == \"Person\"]\n  var_t &lt;- var_components$var[var_components$source == \"Task\"]\n  var_r &lt;- var_components$var[var_components$source == \"Rater\"]\n  var_pt &lt;- var_components$var[var_components$source == \"Person:Task\"]\n  var_e &lt;- var_components$var[var_components$source == \"Residual\"]\n  \n  # 相対的誤差分散（交互作用のみ）\n  var_rel &lt;- var_pt / n_tasks + var_e / (n_tasks * n_raters)\n  \n  # G係数\n  g_coef &lt;- var_p / (var_p + var_rel)\n  \n  return(g_coef)\n}\n\n# φ係数（絶対的決定）の計算\ncalculate_phi_coefficient &lt;- function(var_components, n_tasks, n_raters) {\n  var_p &lt;- var_components$var[var_components$source == \"Person\"]\n  var_t &lt;- var_components$var[var_components$source == \"Task\"]\n  var_r &lt;- var_components$var[var_components$source == \"Rater\"]\n  var_pt &lt;- var_components$var[var_components$source == \"Person:Task\"]\n  var_e &lt;- var_components$var[var_components$source == \"Residual\"]\n  \n  # 絶対的誤差分散（すべての誤差源）\n  var_abs &lt;- var_t / n_tasks + var_r / (n_tasks * n_raters) + \n             var_pt / n_tasks + var_e / (n_tasks * n_raters)\n  \n  # φ係数\n  phi_coef &lt;- var_p / (var_p + var_abs)\n  \n  return(phi_coef)\n}",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#d研究シミュレーション",
    "href": "10_一般化可能性理論.html#d研究シミュレーション",
    "title": "12  一般化可能性理論",
    "section": "15.2 D研究シミュレーション",
    "text": "15.2 D研究シミュレーション\n\n\nコードを表示\n# シミュレーション実行\nd_study_simulation &lt;- function(var_components, max_tasks = 5, max_raters = 10) {\n  results &lt;- tibble()\n  \n  for (n_tasks in 1:max_tasks) {\n    for (n_raters in 1:max_raters) {\n      g_coef &lt;- calculate_g_coefficient(var_components, n_tasks, n_raters)\n      phi_coef &lt;- calculate_phi_coefficient(var_components, n_tasks, n_raters)\n      \n      results &lt;- bind_rows(\n        results,\n        tibble(\n          TaskCount = n_tasks,\n          RaterCount = n_raters,\n          Generalizability = g_coef,\n          Dependability = phi_coef\n        )\n      )\n    }\n  }\n  \n  results &lt;- results |&gt;\n    mutate(\n      TaskCount = factor(TaskCount),\n      RaterCount = factor(RaterCount)\n    )\n  \n  return(results)\n}\n\n# シミュレーション実行\nd_results &lt;- d_study_simulation(g_study_results, max_tasks = 5, max_raters = 10)\n\n# 代表的な組み合わせの結果を表示用データ作成\nd_table_data &lt;- d_results |&gt;\n  filter(\n    TaskCount %in% c(1, 3, 5),\n    RaterCount %in% c(2, 5, 10)) |&gt;\n  pivot_wider(\n    names_from = RaterCount,\n    values_from = c(Generalizability, Dependability),\n    names_sep = \"_\")\n\n# gtテーブルの作成\nd_table &lt;- d_table_data |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"D-study: reliability simulation (lme4)\",\n    subtitle = \"Effect of number of tasks × raters\") |&gt;\n  fmt_number(\n    columns = -TaskCount,\n    decimals = 3) |&gt;\n  cols_label(\n    TaskCount = \"Number of tasks\",\n    Generalizability_2 = \"2 raters\",\n    Generalizability_5 = \"5 raters\",\n    Generalizability_10 = \"10 raters\",\n    Dependability_2 = \"2 raters\",\n    Dependability_5 = \"5 raters\",\n    Dependability_10 = \"10 raters\") |&gt;\n  tab_spanner(\n    label = \"G-coefficient (relative)\",\n    columns = starts_with(\"Generalizability\")\n  ) |&gt;\n  tab_spanner(\n    label = \"Phi-coefficient (absolute)\",\n    columns = starts_with(\"Dependability\")\n  ) |&gt;\n  tab_footnote(\n    footnote = \"Green: ≥0.8 (recommended), Yellow: 0.7–0.8 (acceptable)\",\n    locations = cells_column_labels(columns = 2)\n  )\n\n# 条件付きスタイリングを各列に個別に適用\nreliability_cols &lt;- names(d_table_data)[-1]  # TaskCount以外の全列\n\nfor (col in reliability_cols) {\n  # 0.8以上の値に緑色を適用\n  rows_high &lt;- which(d_table_data[[col]] &gt;= 0.8)\n  if (length(rows_high) &gt; 0) {\n    d_table &lt;- d_table |&gt;\n      tab_style(\n        style = cell_fill(color = \"#e6ffe6\"),\n        locations = cells_body(\n          columns = !!col,\n          rows = rows_high\n        )\n      )\n  }\n  \n  # 0.7-0.8の値に黄色を適用\n  rows_mid &lt;- which(d_table_data[[col]] &gt;= 0.7 & d_table_data[[col]] &lt; 0.8)\n  if (length(rows_mid) &gt; 0) {\n    d_table &lt;- d_table |&gt;\n      tab_style(\n        style = cell_fill(color = \"#fff3cd\"),\n        locations = cells_body(\n          columns = !!col,\n          rows = rows_mid\n        )\n      )\n  }\n}\n\n# テーブルを表示\nd_table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nD-study: reliability simulation (lme4)\n\n\nEffect of number of tasks × raters\n\n\nNumber of tasks\n\nG-coefficient (relative)\n\n\nPhi-coefficient (absolute)\n\n\n\n2 raters1\n5 raters\n10 raters\n2 raters\n5 raters\n10 raters\n\n\n\n\n1\n0.510\n0.624\n0.674\n0.467\n0.580\n0.631\n\n\n3\n0.758\n0.833\n0.861\n0.724\n0.806\n0.837\n\n\n5\n0.839\n0.892\n0.912\n0.814\n0.874\n0.895\n\n\n\n1 Green: ≥0.8 (recommended), Yellow: 0.7–0.8 (acceptable)",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#信頼性係数の可視化",
    "href": "10_一般化可能性理論.html#信頼性係数の可視化",
    "title": "12  一般化可能性理論",
    "section": "15.3 信頼性係数の可視化",
    "text": "15.3 信頼性係数の可視化\n\n\nコードを表示\np1 &lt;- ggplot(d_results,\n       aes(x = TaskCount, y = Generalizability,\n           group = RaterCount, color = RaterCount)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"red\", alpha = 0.5) +\n  geom_hline(yintercept = 0.7, linetype = \"dotted\", color = \"orange\", alpha = 0.5) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +\n  scale_color_viridis_d(name = \"Number of raters\") +\n  labs(\n    title = \"G-coefficient (relative)\",\n    x = \"Number of tasks\",\n    y = \"G coefficient\"\n  ) +\n  theme_bw()\n\np2 &lt;- ggplot(d_results,\n       aes(x = TaskCount, y = Dependability,\n           group = RaterCount, color = RaterCount)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"red\", alpha = 0.5) +\n  geom_hline(yintercept = 0.7, linetype = \"dotted\", color = \"orange\", alpha = 0.5) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +\n  scale_color_viridis_d(name = \"Number of raters\") +\n  labs(\n    title = \"Phi-coefficient (absolute)\",\n    x = \"Number of tasks\",\n    y = \"Phi coefficient\"\n  ) +\n  theme_bw()\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\nChange in reliability coefficients",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#コスト効率分析",
    "href": "10_一般化可能性理論.html#コスト効率分析",
    "title": "12  一般化可能性理論",
    "section": "15.4 コスト効率分析",
    "text": "15.4 コスト効率分析\n\n\nコードを表示\n# 評価者4名固定時の信頼性向上\nd_results |&gt;\n  filter(RaterCount == \"4\") |&gt;\n  mutate(TaskCount = as.numeric(as.character(TaskCount))) |&gt;\n  ggplot(aes(x = TaskCount)) +\n  geom_line(aes(y = Generalizability, color = \"G coefficient\"), size = 1.5) +\n  geom_line(aes(y = Dependability, color = \"Phi coefficient\"), size = 1.5) +\n  geom_point(aes(y = Generalizability), size = 3, color = \"blue\") +\n  geom_point(aes(y = Dependability), size = 3, color = \"red\") +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", alpha = 0.5) +\n  scale_y_continuous(limits = c(0.4, 1), breaks = seq(0.4, 1, 0.1)) +\n  scale_x_continuous(breaks = 1:5) +\n  scale_color_manual(values = c(\"G coefficient\" = \"blue\", \"Phi coefficient\" = \"red\")) +\n  labs(\n    title = \"Reliability with 4 raters (lme4)\",\n    subtitle = \"Diminishing returns as tasks increase\",\n    x = \"Number of tasks\",\n    y = \"Reliability coefficient\",\n    color = \"Coefficient\"\n  ) +\n  theme_bw() +\n  annotate(\"text\", x = 4.5, y = 0.82, label = \"Target threshold\", color = \"red\", size = 3.5)\n\n\n\n\n\nOptimal design from a cost-efficiency perspective",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#多変量データの生成",
    "href": "10_一般化可能性理論.html#多変量データの生成",
    "title": "12  一般化可能性理論",
    "section": "16.1 多変量データの生成",
    "text": "16.1 多変量データの生成\n\n\nコードを表示\n# Rajaratnam.2相当のデータを生成\ncreate_rajaratnam_data &lt;- function() {\n  n_persons &lt;- 20\n  n_items_per_subtest &lt;- c(5, 4, 6)  # サブテストごとの項目数\n  n_subtests &lt;- 3\n  \n  # 分散成分の設定（サブテストごと）\n  var_person &lt;- c(0.35, 0.40, 0.38)\n  var_item &lt;- c(0.08, 0.10, 0.09)\n  var_residual &lt;- c(0.30, 0.35, 0.32)\n  \n  # 相関行列（Person間の相関）\n  person_cor &lt;- matrix(c(\n    1.00, 0.65, 0.60,\n    0.65, 1.00, 0.70,\n    0.60, 0.70, 1.00\n  ), nrow = 3)\n  \n  data_list &lt;- list()\n  \n  for (s in 1:n_subtests) {\n    # データグリッド\n    subtest_data &lt;- expand.grid(\n      Person = factor(1:n_persons),\n      Item = factor(1:n_items_per_subtest[s]),\n      Subtest = factor(s)\n    )\n    \n    # ランダム効果の生成\n    person_effects &lt;- rnorm(n_persons, 0, sqrt(var_person[s]))\n    item_effects &lt;- rnorm(n_items_per_subtest[s], 0, sqrt(var_item[s]))\n    \n    # スコアの計算\n    subtest_data$Score &lt;- NA\n    for(i in 1:nrow(subtest_data)) {\n      p &lt;- as.numeric(subtest_data$Person[i])\n      it &lt;- as.numeric(subtest_data$Item[i])\n      \n      subtest_data$Score[i] &lt;- 5 +\n        person_effects[p] +\n        item_effects[it] +\n        rnorm(1, 0, sqrt(var_residual[s]))\n    }\n    \n    data_list[[s]] &lt;- subtest_data\n  }\n  \n  # データの結合\n  Rajaratnam.2 &lt;- bind_rows(data_list)\n  \n  return(Rajaratnam.2)\n}\n\n# データ生成\nRajaratnam.2 &lt;- create_rajaratnam_data()\n\n# データ構造の確認\nrajaratnam_structure &lt;- Rajaratnam.2 |&gt;\n  group_by(Subtest, Item) |&gt;\n  summarise(n = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = Item, values_from = n, values_fill = 0)\n\nrajaratnam_table &lt;- rajaratnam_structure |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Rajaratnam.2 data structure\",\n    subtitle = \"Counts by subtest × item\"\n  ) |&gt;\n  cols_label(\n    Subtest = \"Subtest\"\n  )\n\n# 0の値がある場所を特定して脚注を追加\nhas_zero &lt;- FALSE\nfor (col in names(rajaratnam_structure)[-1]) {  # Subtest列を除く\n  zero_rows &lt;- which(rajaratnam_structure[[col]] == 0)\n  if (length(zero_rows) &gt; 0) {\n    has_zero &lt;- TRUE\n    # 0の値のセルをハイライト\n    rajaratnam_table &lt;- rajaratnam_table |&gt;\n      tab_style(\n        style = cell_fill(color = \"#f0f0f0\"),\n        locations = cells_body(\n          columns = !!col,\n          rows = zero_rows\n        )\n      )\n  }\n}\n\nif (has_zero) {\n  rajaratnam_table &lt;- rajaratnam_table |&gt;\n    tab_footnote(\n      footnote = \"Gray: no data (different number of items per subtest)\",\n      locations = cells_column_labels(columns = 2)  # 最初のItem列に脚注\n    )\n}\n\nrajaratnam_table\n\n\n\n\n\n\n\n\nRajaratnam.2 data structure\n\n\nCounts by subtest × item\n\n\nSubtest\n11\n2\n3\n4\n5\n6\n\n\n\n\n1\n20\n20\n20\n20\n20\n0\n\n\n2\n20\n20\n20\n20\n0\n0\n\n\n3\n20\n20\n20\n20\n20\n20\n\n\n\n1 Gray: no data (different number of items per subtest)",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#多変量g研究",
    "href": "10_一般化可能性理論.html#多変量g研究",
    "title": "12  一般化可能性理論",
    "section": "16.2 多変量G研究",
    "text": "16.2 多変量G研究\n\n\nコードを表示\n# サブテストごとにlme4モデルを適用\nmultivariate_g_study &lt;- function(data) {\n  subtests &lt;- unique(data$Subtest)\n  results &lt;- list()\n  \n  for (s in subtests) {\n    subtest_data &lt;- data |&gt; filter(Subtest == s)\n    \n    # lme4モデル\n    model &lt;- lmer(\n      Score ~ 1 + (1|Person) + (1|Item),\n      data = subtest_data,\n      REML = TRUE\n    )\n    \n    # 分散成分の抽出\n    vc &lt;- VarCorr(model)\n    var_person &lt;- as.numeric(vc$Person)\n    var_item &lt;- as.numeric(vc$Item)\n    var_residual &lt;- attr(vc, \"sc\")^2\n    var_total &lt;- var_person + var_item + var_residual\n    \n    results[[paste0(\"Subtest\", s)]] &lt;- data.frame(\n      Subtest = s,\n      source = c(\"Person\", \"Item\", \"Residual\"),\n      var = c(var_person, var_item, var_residual),\n      percent = 100 * c(var_person, var_item, var_residual) / var_total\n    )\n  }\n  \n  return(results)\n}\n\n# 多変量G研究の実行\nmv_g_results &lt;- multivariate_g_study(Rajaratnam.2)\n\n# 結果の整理と表示\nmv_g_combined &lt;- bind_rows(mv_g_results)\n\nmv_g_table &lt;- mv_g_combined |&gt;\n  select(Subtest, source, var, percent) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Multivariate G-study: within components (lme4)\",\n    subtitle = \"Variance decomposition by subtest\"\n  ) |&gt;\n  fmt_number(\n    columns = var,\n    decimals = 4\n  ) |&gt;\n  fmt_number(\n    columns = percent,\n    decimals = 1\n  ) |&gt;\n  cols_label(\n    Subtest = \"Subtest\",\n    source = \"Source\",\n    var = \"Variance\",\n    percent = \"Percent (%)\"\n  )\n\n# Person行をハイライト\nperson_rows &lt;- which(mv_g_combined$source == \"Person\")\nif (length(person_rows) &gt; 0) {\n  mv_g_table &lt;- mv_g_table |&gt;\n    tab_style(\n      style = list(\n        cell_fill(color = \"#e6f3ff\"),\n        cell_text(weight = \"bold\")\n      ),\n      locations = cells_body(\n        columns = percent,\n        rows = person_rows\n      )\n    )\n}\n\nmv_g_table\n\n\n\n\n\n\n\n\nMultivariate G-study: within components (lme4)\n\n\nVariance decomposition by subtest\n\n\nSubtest\nSource\nVariance\nPercent (%)\n\n\n\n\n1\nPerson\n0.3543\n45.1\n\n\n1\nItem\n0.0880\n11.2\n\n\n1\nResidual\n0.3439\n43.7\n\n\n2\nPerson\n0.4199\n50.4\n\n\n2\nItem\n0.0416\n5.0\n\n\n2\nResidual\n0.3718\n44.6\n\n\n3\nPerson\n0.4375\n58.5\n\n\n3\nItem\n0.0642\n8.6\n\n\n3\nResidual\n0.2466\n33.0",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#サブテスト間共分散の推定",
    "href": "10_一般化可能性理論.html#サブテスト間共分散の推定",
    "title": "12  一般化可能性理論",
    "section": "16.3 サブテスト間共分散の推定",
    "text": "16.3 サブテスト間共分散の推定\n\n\nコードを表示\n# サブテスト間の共分散行列を推定\nestimate_subtest_covariance &lt;- function(data, mv_results) {\n  subtests &lt;- unique(data$Subtest)\n  n_subtests &lt;- length(subtests)\n  \n  # 各サブテストのPerson分散を取得\n  person_vars &lt;- numeric(n_subtests)\n  for (i in 1:n_subtests) {\n    subtest_result &lt;- mv_results[[paste0(\"Subtest\", i)]]\n    person_vars[i] &lt;- subtest_result$var[subtest_result$source == \"Person\"]\n  }\n  \n  # Person効果の推定値を取得\n  person_effects &lt;- matrix(NA, nrow = length(unique(data$Person)), ncol = n_subtests)\n  \n  for (s in 1:n_subtests) {\n    subtest_data &lt;- data |&gt; filter(Subtest == s)\n    model &lt;- lmer(Score ~ 1 + (1|Person) + (1|Item), data = subtest_data, REML = TRUE)\n    person_effects[, s] &lt;- ranef(model)$Person[[1]]\n  }\n  \n  # 共分散行列の計算\n  cov_matrix &lt;- cov(person_effects)\n  colnames(cov_matrix) &lt;- paste0(\"Subtest\", 1:n_subtests)\n  rownames(cov_matrix) &lt;- paste0(\"Subtest\", 1:n_subtests)\n  \n  return(list(\n    covariance = cov_matrix,\n    correlation = cov2cor(cov_matrix),\n    person_variances = person_vars\n  ))\n}\n\n# 共分散の推定\nsubtest_cov &lt;- estimate_subtest_covariance(Rajaratnam.2, mv_g_results)\n\n# 共分散行列の表示\nsubtest_cov$covariance |&gt;\n  as.data.frame() |&gt;\n  mutate(Subtest = rownames(subtest_cov$covariance)) |&gt;\n  select(Subtest, everything()) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Between-subtest covariance matrix (lme4)\",\n    subtitle = \"Multivariate G-theory\") |&gt;\n  fmt_number(\n    columns = -Subtest,\n    decimals = 3) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#f0f0f0\"),\n      cell_text(weight = \"bold\")),\n    locations = cells_body(\n      columns = c(Subtest1, Subtest2, Subtest3),\n      rows = c(1, 2, 3)))\n\n\n\n\n\n\n\n\nBetween-subtest covariance matrix (lme4)\n\n\nMultivariate G-theory\n\n\nSubtest\nSubtest1\nSubtest2\nSubtest3\n\n\n\n\nSubtest1\n0.297\n0.156\n−0.069\n\n\nSubtest2\n0.156\n0.344\n0.058\n\n\nSubtest3\n−0.069\n0.058\n0.400\n\n\n\n\n\n\n\nコードを表示\n# 相関行列の表示\nsubtest_cov$correlation |&gt;\n  as.data.frame() |&gt;\n  mutate(Subtest = rownames(subtest_cov$correlation)) |&gt;\n  select(Subtest, everything()) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Between-subtest correlation matrix (lme4)\",\n    subtitle = \"Correlation of person effects\") |&gt;\n  fmt_number(\n    columns = -Subtest,\n    decimals = 3)\n\n\n\n\n\n\n\n\nBetween-subtest correlation matrix (lme4)\n\n\nCorrelation of person effects\n\n\nSubtest\nSubtest1\nSubtest2\nSubtest3\n\n\n\n\nSubtest1\n1.000\n0.490\n−0.201\n\n\nSubtest2\n0.490\n1.000\n0.156\n\n\nSubtest3\n−0.201\n0.156\n1.000",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#多変量d研究",
    "href": "10_一般化可能性理論.html#多変量d研究",
    "title": "12  一般化可能性理論",
    "section": "16.4 多変量D研究",
    "text": "16.4 多変量D研究\n\n\nコードを表示\n# 多変量信頼性係数の計算\ncalculate_composite_reliability &lt;- function(mv_results, cov_matrix, weights, n_items) {\n  n_subtests &lt;- length(weights)\n  \n  # 各サブテストの分散成分を取得\n  person_vars &lt;- numeric(n_subtests)\n  item_vars &lt;- numeric(n_subtests)\n  residual_vars &lt;- numeric(n_subtests)\n  \n  for (i in 1:n_subtests) {\n    subtest_result &lt;- mv_results[[paste0(\"Subtest\", i)]]\n    person_vars[i] &lt;- subtest_result$var[subtest_result$source == \"Person\"]\n    item_vars[i] &lt;- subtest_result$var[subtest_result$source == \"Item\"]\n    residual_vars[i] &lt;- subtest_result$var[subtest_result$source == \"Residual\"]\n  }\n  \n  # 合成スコアの分散（Universe分散）\n  var_universe &lt;- 0\n  for (i in 1:n_subtests) {\n    for (j in 1:n_subtests) {\n      if (i == j) {\n        var_universe &lt;- var_universe + weights[i]^2 * person_vars[i]\n      } else {\n        var_universe &lt;- var_universe + weights[i] * weights[j] * cov_matrix[i, j]\n      }\n    }\n  }\n  \n  # 相対的誤差分散\n  var_rel &lt;- 0\n  for (i in 1:n_subtests) {\n    var_rel &lt;- var_rel + weights[i]^2 * residual_vars[i] / n_items[i]\n  }\n  \n  # 絶対的誤差分散\n  var_abs &lt;- var_rel\n  for (i in 1:n_subtests) {\n    var_abs &lt;- var_abs + weights[i]^2 * item_vars[i] / n_items[i]\n  }\n  \n  # 信頼性係数\n  generalizability &lt;- var_universe / (var_universe + var_rel)\n  dependability &lt;- var_universe / (var_universe + var_abs)\n  \n  # SEM\n  sem_rel &lt;- sqrt(var_rel)\n  sem_abs &lt;- sqrt(var_abs)\n  \n  return(list(\n    var_universe = var_universe,\n    generalizability = generalizability,\n    dependability = dependability,\n    sem_rel = sem_rel,\n    sem_abs = sem_abs\n  ))\n}\n\n# 重み付けを変えた複数のシナリオ\nweight_scenarios &lt;- list(\n  \"Equal weights\" = c(1/3, 1/3, 1/3),\n  \"Emphasize subtest 2\" = c(0.25, 0.50, 0.25),\n  \"Emphasize subtest 1\" = c(0.50, 0.25, 0.25))\n\n# 各シナリオでの信頼性計算\nmv_d_results &lt;- list()\nn_items &lt;- c(5, 4, 6)  # 現在の項目数\n\nfor (scenario_name in names(weight_scenarios)) {\n  weights &lt;- weight_scenarios[[scenario_name]]\n  result &lt;- calculate_composite_reliability(\n    mv_g_results, \n    subtest_cov$covariance, \n    weights, \n    n_items)\n  result$scenario &lt;- scenario_name\n  result$weights &lt;- paste(weights, collapse = \", \")\n  mv_d_results[[scenario_name]] &lt;- result\n}\n\n# 結果の表示\nmv_d_table_data &lt;- bind_rows(lapply(mv_d_results, function(x) {\n  data.frame(\n    Scenario = x$scenario,\n    Weights = x$weights,\n    Universe_Var = x$var_universe,\n    G_coefficient = x$generalizability,\n    Phi_coefficient = x$dependability,\n    SEM_Rel = x$sem_rel,\n    SEM_Abs = x$sem_abs\n  )\n}))\n\nmv_d_table &lt;- mv_d_table_data |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Multivariate D-study results (lme4)\",\n    subtitle = \"Composite-score reliability under different weightings\") |&gt;\n  fmt_number(\n    columns = c(Universe_Var, G_coefficient, Phi_coefficient, SEM_Rel, SEM_Abs),\n    decimals = 3) |&gt;\n  cols_label(\n    Scenario = \"Scenario\",\n    Weights = \"Weights\",\n    Universe_Var = \"Universe variance\",\n    G_coefficient = \"G coefficient\",\n    Phi_coefficient = \"Phi coefficient\",\n    SEM_Rel = \"SEM (relative)\",\n    SEM_Abs = \"SEM (absolute)\")\n\n# 条件付きスタイリング\nfor (i in 1:nrow(mv_d_table_data)) {\n  if (mv_d_table_data$G_coefficient[i] &gt;= 0.8 || mv_d_table_data$Phi_coefficient[i] &gt;= 0.8) {\n    mv_d_table &lt;- mv_d_table |&gt;\n      tab_style(\n        style = list(\n          cell_fill(color = \"#e6ffe6\"),\n          cell_text(weight = \"bold\")\n        ),\n        locations = cells_body(\n          columns = c(G_coefficient, Phi_coefficient),\n          rows = i\n        )\n      )\n  }\n}\n\nmv_d_table\n\n\n\n\n\n\n\n\nMultivariate D-study results (lme4)\n\n\nComposite-score reliability under different weightings\n\n\nScenario\nWeights\nUniverse variance\nG coefficient\nPhi coefficient\nSEM (relative)\nSEM (absolute)\n\n\n\n\nEqual weights\n0.333333333333333, 0.333333333333333, 0.333333333333333\n0.167\n0.881\n0.861\n0.150\n0.164\n\n\nEmphasize subtest 2\n0.25, 0.5, 0.25\n0.199\n0.869\n0.853\n0.174\n0.186\n\n\nEmphasize subtest 1\n0.5, 0.25, 0.25\n0.171\n0.870\n0.845\n0.160\n0.177",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#項目数を変えたシミュレーション",
    "href": "10_一般化可能性理論.html#項目数を変えたシミュレーション",
    "title": "12  一般化可能性理論",
    "section": "16.5 項目数を変えたシミュレーション",
    "text": "16.5 項目数を変えたシミュレーション\n\n\nコードを表示\n# 項目数を変えたシミュレーション\nsimulate_mv_reliability &lt;- function(mv_results, cov_matrix, weights, max_items = 10) {\n  n_subtests &lt;- length(weights)\n  results &lt;- tibble()\n  \n  # 基本の分散成分を取得\n  person_vars &lt;- numeric(n_subtests)\n  item_vars &lt;- numeric(n_subtests)\n  residual_vars &lt;- numeric(n_subtests)\n  \n  for (i in 1:n_subtests) {\n    subtest_result &lt;- mv_results[[paste0(\"Subtest\", i)]]\n    person_vars[i] &lt;- subtest_result$var[subtest_result$source == \"Person\"]\n    item_vars[i] &lt;- subtest_result$var[subtest_result$source == \"Item\"]\n    residual_vars[i] &lt;- subtest_result$var[subtest_result$source == \"Residual\"]\n  }\n  \n  # 項目数を変えてシミュレーション\n  for (n_items_base in 1:max_items) {\n    # 各サブテストの項目数（比率を保つ）\n    n_items &lt;- c(n_items_base, \n                 round(n_items_base * 0.8), \n                 round(n_items_base * 1.2))\n    n_items[n_items == 0] &lt;- 1  # 最低1項目\n    \n    # Universe分散\n    var_universe &lt;- 0\n    for (i in 1:n_subtests) {\n      for (j in 1:n_subtests) {\n        if (i == j) {\n          var_universe &lt;- var_universe + weights[i]^2 * person_vars[i]\n        } else {\n          var_universe &lt;- var_universe + weights[i] * weights[j] * cov_matrix[i, j]\n        }\n      }\n    }\n    \n    # 誤差分散\n    var_rel &lt;- 0\n    var_abs &lt;- 0\n    for (i in 1:n_subtests) {\n      var_rel &lt;- var_rel + weights[i]^2 * residual_vars[i] / n_items[i]\n      var_abs &lt;- var_abs + weights[i]^2 * (item_vars[i] + residual_vars[i]) / n_items[i]\n    }\n    \n    # 信頼性係数\n    g_coef &lt;- var_universe / (var_universe + var_rel)\n    phi_coef &lt;- var_universe / (var_universe + var_abs)\n    \n    results &lt;- bind_rows(\n      results,\n      tibble(\n        ItemCount = n_items_base,\n        Items_S1 = n_items[1],\n        Items_S2 = n_items[2],\n        Items_S3 = n_items[3],\n        Generalizability = g_coef,\n        Dependability = phi_coef\n      )\n    )\n  }\n  \n  return(results)\n}\n\n# シミュレーション実行（均等重みの場合）\nmv_sim_results &lt;- simulate_mv_reliability(\n  mv_g_results,\n  subtest_cov$covariance,\n  c(1/3, 1/3, 1/3),\n  max_items = 10)\n\n# 可視化\nmv_sim_long &lt;- mv_sim_results |&gt;\n  select(ItemCount, Generalizability, Dependability) |&gt;\n  pivot_longer(cols = c(Generalizability, Dependability),\n               names_to = \"Coefficient\",\n               values_to = \"Value\")\n\nggplot(mv_sim_long, aes(x = ItemCount, y = Value, color = Coefficient)) +\n  geom_line(size = 1.5) +\n  geom_point(size = 3) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"red\", alpha = 0.5) +\n  geom_hline(yintercept = 0.7, linetype = \"dotted\", color = \"orange\", alpha = 0.5) +\n  scale_y_continuous(limits = c(0.4, 1), breaks = seq(0.4, 1, 0.1)) +\n  scale_x_continuous(breaks = 1:10) +\n  scale_color_manual(\n    values = c(\"Generalizability\" = \"blue\", \"Dependability\" = \"red\"),\n    labels = c(\"Generalizability\" = \"G coefficient\", \"Dependability\" = \"Phi coefficient\")) +\n  labs(\n    title = \"Change in multivariate reliability (lme4 simulation)\",\n    subtitle = \"Equal weights (1/3, 1/3, 1/3): effect of increasing items\",\n    x = \"Baseline number of items\",\n    y = \"Reliability coefficient\",\n    color = \"Coefficient\") +\n  theme_bw() +\n  annotate(\"text\", x = 9, y = 0.82, label = \"Target\", color = \"red\", size = 3.5) +\n  annotate(\"text\", x = 9, y = 0.72, label = \"Minimum\", color = \"orange\", size = 3.5)\n\n\n\n\n\nMultivariate D-study: effect of number of items",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#モデル診断",
    "href": "10_一般化可能性理論.html#モデル診断",
    "title": "12  一般化可能性理論",
    "section": "17.1 モデル診断",
    "text": "17.1 モデル診断\n\n\nコードを表示\n# 残差診断プロット\npar(mfrow = c(2, 2))\n\n# 1. Q-Qプロット\nqqnorm(resid(g_model), main = \"Normal Q-Q Plot\")\nqqline(resid(g_model))\n\n# 2. 残差 vs 適合値\nplot(fitted(g_model), resid(g_model),\n     xlab = \"Fitted values\", ylab = \"Residuals\",\n     main = \"Residuals vs Fitted\")\nabline(h = 0, col = \"red\", lty = 2)\n\n# 3. スケール-位置プロット\nplot(fitted(g_model), sqrt(abs(resid(g_model))),\n     xlab = \"Fitted values\", ylab = \"√|Residuals|\",\n     main = \"Scale-Location Plot\")\n\n# 4. ランダム効果のQ-Qプロット\nranef_person &lt;- ranef(g_model)$Person[[1]]\nqqnorm(ranef_person, main = \"Random Effects Q-Q Plot (Person)\")\nqqline(ranef_person)\n\n\n\n\n\nDiagnostics for linear mixed-effects model\n\n\n\n\nコードを表示\npar(mfrow = c(1, 1))",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#icc計算",
    "href": "10_一般化可能性理論.html#icc計算",
    "title": "12  一般化可能性理論",
    "section": "17.2 ICC計算",
    "text": "17.2 ICC計算\n\n\nコードを表示\n# ICC (Intraclass Correlation Coefficient) の計算\ncalculate_icc &lt;- function(model) {\n  vc &lt;- VarCorr(model)\n  \n  # 各レベルのICC\n  var_person &lt;- as.numeric(vc$Person)\n  var_task &lt;- as.numeric(vc$Task)\n  var_rater &lt;- as.numeric(vc$Rater)\n  var_person_task &lt;- as.numeric(vc$`Person:Task`)\n  var_residual &lt;- attr(vc, \"sc\")^2\n  var_total &lt;- var_person + var_task + var_rater + var_person_task + var_residual\n  \n  icc_results &lt;- data.frame(\n    Level = c(\"Person\", \"Task\", \"Rater\", \"Person:Task\"),\n    ICC = c(\n      var_person / var_total,\n      var_task / var_total,\n      var_rater / var_total,\n      var_person_task / var_total\n    )\n  )\n  \n  return(icc_results)\n}\n\n# ICC計算と表示\nicc_results &lt;- calculate_icc(g_model)\n\n# ICC計算と表示\nicc_results &lt;- calculate_icc(g_model)\n\nicc_table &lt;- icc_results |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Intraclass correlation coefficients (ICC)\",\n    subtitle = \"Proportion of total variance at each level\") |&gt;\n  fmt_number(\n    columns = ICC,\n    decimals = 3) |&gt;\n  cols_label(\n    Level = \"Level\",\n    ICC = \"ICC\") |&gt;\n  tab_footnote(\n    footnote = \"ICC is the proportion of total variance at that level.\",\n    locations = cells_column_labels(columns = ICC))\n\n# Person行をハイライト\nperson_icc_row &lt;- which(icc_results$Level == \"Person\")\nif (length(person_icc_row) &gt; 0) {\n  icc_table &lt;- icc_table |&gt;\n    tab_style(\n      style = cell_fill(color = \"#e6f3ff\"),\n      locations = cells_body(\n        columns = ICC,\n        rows = person_icc_row\n      )\n    )\n}\n\nicc_table\n\n\n\n\n\n\n\n\nIntraclass correlation coefficients (ICC)\n\n\nProportion of total variance at each level\n\n\nLevel\nICC1\n\n\n\n\nPerson\n0.352\n\n\nTask\n0.028\n\n\nRater\n0.073\n\n\nPerson:Task\n0.128\n\n\n\n1 ICC is the proportion of total variance at that level.",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#主要な結果",
    "href": "10_一般化可能性理論.html#主要な結果",
    "title": "12  一般化可能性理論",
    "section": "18.1 主要な結果",
    "text": "18.1 主要な結果\n\n\nコードを表示\nsummary_data &lt;- data.frame(\n  Analysis = c(\"Univariate G-study\", \"Univariate D-study (3 tasks, 5 raters)\", \n               \"Multivariate G-study (mean)\", \"Multivariate D-study (equal weights)\"),\n  G_coefficient = c(\n    NA,\n    d_results |&gt; filter(TaskCount == 3, RaterCount == 5) |&gt; pull(Generalizability),\n    mean(sapply(mv_g_results, function(x) x$var[x$source == \"Person\"] / sum(x$var))),\n    mv_d_results[[\"Equal weights\"]]$generalizability),\n  Phi_coefficient = c(\n    NA,\n    d_results |&gt; filter(TaskCount == 3, RaterCount == 5) |&gt; pull(Dependability),\n    NA,\n    mv_d_results[[\"Equal weights\"]]$dependability),\n  Key_Finding = c(\n    \"Person variance ≈ 30–40% of total\",\n    \"Practical reliability achieved\",\n    \"Between-subtest correlation ≈ 0.6–0.7\",\n    \"High reliability for composite score\"))\n\nsummary_table &lt;- summary_data |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Summary of results\",\n    subtitle = \"G-theory implementation with lme4\") |&gt;\n  fmt_number(\n    columns = c(G_coefficient, Phi_coefficient),\n    decimals = 3) |&gt;\n  cols_label(\n    Analysis = \"Analysis\",\n    G_coefficient = \"G coefficient\",\n    Phi_coefficient = \"Phi coefficient\",\n    Key_Finding = \"Key finding\")\n\n# 条件付きスタイリング\nfor (i in 1:nrow(summary_data)) {\n  if (!is.na(summary_data$G_coefficient[i]) && summary_data$G_coefficient[i] &gt;= 0.7) {\n    summary_table &lt;- summary_table |&gt;\n      tab_style(\n        style = cell_fill(color = \"#e6ffe6\"),\n        locations = cells_body(\n          columns = G_coefficient,\n          rows = i\n        )\n      )\n  }\n  if (!is.na(summary_data$Phi_coefficient[i]) && summary_data$Phi_coefficient[i] &gt;= 0.7) {\n    summary_table &lt;- summary_table |&gt;\n      tab_style(\n        style = cell_fill(color = \"#e6ffe6\"),\n        locations = cells_body(\n          columns = Phi_coefficient,\n          rows = i\n        )\n      )\n  }\n}\n\nsummary_table\n\n\n\n\n\n\n\n\nSummary of results\n\n\nG-theory implementation with lme4\n\n\nAnalysis\nG coefficient\nPhi coefficient\nKey finding\n\n\n\n\nUnivariate G-study\nNA\nNA\nPerson variance ≈ 30–40% of total\n\n\nUnivariate D-study (3 tasks, 5 raters)\n0.833\n0.806\nPractical reliability achieved\n\n\nMultivariate G-study (mean)\n0.513\nNA\nBetween-subtest correlation ≈ 0.6–0.7\n\n\nMultivariate D-study (equal weights)\n0.881\n0.861\nHigh reliability for composite score",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#lme4実装の利点",
    "href": "10_一般化可能性理論.html#lme4実装の利点",
    "title": "12  一般化可能性理論",
    "section": "18.2 lme4実装の利点",
    "text": "18.2 lme4実装の利点\n\n柔軟性: 複雑なネスト構造や交差構造に対応可能\n診断ツール: 豊富なモデル診断機能\n拡張性: 固定効果の追加、非線形モデルへの拡張が容易\n統合性: 他のRパッケージとの親和性が高い",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "10_一般化可能性理論.html#実践的推奨事項",
    "href": "10_一般化可能性理論.html#実践的推奨事項",
    "title": "12  一般化可能性理論",
    "section": "18.3 実践的推奨事項",
    "text": "18.3 実践的推奨事項\n\n18.3.1 測定設計の最適化\n\n\nコードを表示\n# 最適設計の推奨\nrecommendations &lt;- data.frame(\n  Purpose = c(\"High-stakes testing\", \"Formative assessment\", \"Research use\", \"Screening\"),\n  Min_Tasks = c(5, 3, 3, 2),\n  Min_Raters = c(3, 2, 2, 2),\n  Target_G = c(0.90, 0.80, 0.70, 0.70),\n  Target_Phi = c(0.85, 0.75, 0.65, 0.65))\n\nrec_table &lt;- recommendations |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Recommended designs by purpose\",\n    subtitle = \"Minimum requirements and target reliability\") |&gt;\n  cols_label(\n    Purpose = \"Purpose\",\n    Min_Tasks = \"Min tasks\",\n    Min_Raters = \"Min raters\",\n    Target_G = \"Target G\",\n    Target_Phi = \"Target Phi\")\n\n# High-stakes testingの行をハイライト\nhigh_stakes_row &lt;- which(recommendations$Purpose == \"High-stakes testing\")\nif (length(high_stakes_row) &gt; 0) {\n  rec_table &lt;- rec_table |&gt;\n    tab_style(\n      style = cell_fill(color = \"#f0f8ff\"),\n      locations = cells_body(\n        rows = high_stakes_row\n      )\n    )\n}\n\nrec_table\n\n\n\n\n\n\n\n\nRecommended designs by purpose\n\n\nMinimum requirements and target reliability\n\n\nPurpose\nMin tasks\nMin raters\nTarget G\nTarget Phi\n\n\n\n\nHigh-stakes testing\n5\n3\n0.9\n0.85\n\n\nFormative assessment\n3\n2\n0.8\n0.75\n\n\nResearch use\n3\n2\n0.7\n0.65\n\n\nScreening\n2\n2\n0.7\n0.65",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>一般化可能性理論</span>"
    ]
  },
  {
    "objectID": "11_項目応答理論とラッシュモデル.html",
    "href": "11_項目応答理論とラッシュモデル.html",
    "title": "13  項目応答理論とラッシュモデル",
    "section": "",
    "text": "14 はじめに\nコードを表示\n# Load required packages\npacman::p_load(\n  tidyverse,    # Data manipulation and visualization\n  gt,           # Beautiful tables\n  lavaan,       # Structural equation modeling\n  semPlot,      # SEM visualization\n  ltm,          # Latent trait models\n  eRm,          # Extended Rasch models\n  mirt,         # Multidimensional IRT\n  car,          # Regression diagnostics\n  knitr,        # Document creation\n  kableExtra,   # Table extensions\n  psych,        # Psychological statistics\n  lattice,      # Graphics\n  ggcorrplot,   # Correlation plots\n  reshape2,     # Data reshaping\n  viridis       # Color palettes\n)\n\n# Set theme and random seed\ntheme_set(theme_bw())\nset.seed(123)\n英単語のテストは「単語力」を測定しています。しかし、実際のテストでは「以下の英単語の意味として最も適切なものをa〜dから1つずつ選びなさい。1. apple、2. pear、3. fig …」のように、個別の単語が出題されます。そのため「単語力」のような目に見えない能力を、目に見えるテスト項目を使って測定していると言えます。ちなみに上記の答えはリンゴ、ナシ、イチジクとなります。fig (イチジク) はもしかしたら難しい項目であり、受験者の点数が低いかもしれません。\n目に見えるデータは観測変数、目に見えないデータは潜在変数と呼ばれています。つまり、上記の例では観測変数は受験者から集めたテストの結果 (正解・不正解のような2値データ、平均点・分散) となり、潜在変数 (単語力) は因子分析などによって仮定されます。つまり、潜在変数それ自体は測定することはできません。",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>項目応答理論とラッシュモデル</span>"
    ]
  },
  {
    "objectID": "11_項目応答理論とラッシュモデル.html#因子分析の例",
    "href": "11_項目応答理論とラッシュモデル.html#因子分析の例",
    "title": "13  項目応答理論とラッシュモデル",
    "section": "14.1 因子分析の例",
    "text": "14.1 因子分析の例\n因子分析では、観測変数は四角、潜在変数は丸で囲まれます。確認的因子分析のコードは以下のとおりです。\n\n\nコードを表示\n# Define confirmatory factor analysis model\nHS.model &lt;- '\n  visual  =~ x1 + x2 + x3\n  textual =~ x4 + x5 + x6\n  speed   =~ x7 + x8 + x9\n'\n\n# Fit the model\nfit &lt;- cfa(HS.model, data = HolzingerSwineford1939)\n\n# Display model fit measures\nfit_measures &lt;- fitMeasures(fit, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\", \"srmr\"))\n\ndata.frame(\n  Measure = names(fit_measures),\n  Value = as.numeric(fit_measures)\n) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Confirmatory Factor Analysis: Model Fit\",\n    subtitle = \"Holzinger & Swineford (1939) Data\"\n  ) |&gt;\n  fmt_number(\n    columns = Value,\n    decimals = 3\n  ) |&gt;\n  cols_label(\n    Measure = \"Fit Index\",\n    Value = \"Value\"\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#f8f9fa\"),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_column_labels()\n  )\n\n\n\n\n\n\n\n\nConfirmatory Factor Analysis: Model Fit\n\n\nHolzinger & Swineford (1939) Data\n\n\nFit Index\nValue\n\n\n\n\nchisq\n85.306\n\n\ndf\n24.000\n\n\npvalue\n0.000\n\n\ncfi\n0.931\n\n\nrmsea\n0.092\n\n\nsrmr\n0.065\n\n\n\n\n\n\n\nコードを表示\n# Create path diagram\nsemPaths(fit, \"std\", layout = \"tree\", rotation = 2)\n\n\n\n\n\n\n\n\n\n心理学ではテストや質問紙などを用いて潜在変数を測定します。ほとんどの場合、得られる結果は順序カテゴリカルデータです (e.g., 1〜5で回答するなどのアンケート)。\n\n\n\n\n\n\nTypes of Observed Variables\n\n\n\n\n\n\n\n\n\n\n\nData Type\nExample\nFeatures\n\n\n\n\nBinary Data\nyes/no, correct/incorrect\nSimplest form\n\n\nPolytomous Data\nLikert scale, multiple choice\nOrdinal or nominal scale\n\n\nInterval Data\nReaction time, scores\nContinuous measurements\n\n\nCount Data\nNumber of behaviors\nCount data\n\n\n\n\n\n\nWe need a reasonable psychometric analysis to justify the use of a sum score! And even then the “sum score” is a debatable metric in itself since it is ordinal data, but often gets treated like interval data in statistical models.",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>項目応答理論とラッシュモデル</span>"
    ]
  },
  {
    "objectID": "11_項目応答理論とラッシュモデル.html#古典的テスト理論",
    "href": "11_項目応答理論とラッシュモデル.html#古典的テスト理論",
    "title": "13  項目応答理論とラッシュモデル",
    "section": "14.2 古典的テスト理論",
    "text": "14.2 古典的テスト理論\nテストの合計点や平均点を用いる理論は、古典的テスト理論と呼ばれます。テストの採点の際、以下のような表をエクセルで作成すると、合計点や正解率を簡単に計算することができます。\n\n\nコードを表示\n# Create sample data\nctt_data &lt;- data.frame(\n  respondent = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n  Q1 = c(0, 0, 0, 1, 1),\n  Q2 = c(0, 0, 0, 0, 0),\n  Q3 = c(0, 0, 0, 1, 0),\n  Q4 = c(0, 0, 0, 0, 0),\n  Q5 = c(0, 0, 0, 0, 1),\n  Q6 = c(1, 0, 0, 0, 0)\n)\n\n# Calculate total score\nctt_data &lt;- ctt_data |&gt;\n  mutate(total_score = Q1 + Q2 + Q3 + Q4 + Q5 + Q6)\n\n# Calculate pass rate for each item\nitem_stats &lt;- ctt_data |&gt;\n  dplyr::select(Q1:Q6) |&gt;\n  summarise(across(everything(), mean)) |&gt;\n  pivot_longer(everything(), names_to = \"item\", values_to = \"pass_rate\")\n\n# Create and display table\nctt_data |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Classical Test Theory Test Results\",\n    subtitle = \"0 = Incorrect, 1 = Correct\"\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#e6ffe6\"),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_body(columns = total_score)\n  ) |&gt;\n  tab_spanner(\n    label = \"Test Items\",\n    columns = c(Q1:Q6)\n  ) |&gt;\n  cols_label(\n    respondent = \"Respondent\",\n    total_score = \"Total Score\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassical Test Theory Test Results\n\n\n0 = Incorrect, 1 = Correct\n\n\nRespondent\n\nTest Items\n\nTotal Score\n\n\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\n\n\n\n\nA\n0\n0\n0\n0\n0\n1\n1\n\n\nB\n0\n0\n0\n0\n0\n0\n0\n\n\nC\n0\n0\n0\n0\n0\n0\n0\n\n\nD\n1\n0\n1\n0\n0\n0\n2\n\n\nE\n1\n0\n0\n0\n1\n0\n2\n\n\n\n\n\n\n\n通過率\\(p_j\\)は以下のように計算されます：\n\\[\np_j = \\frac{1}{N}\\sum_{i=1}^{N}u_{ij}\n\\]\nつまり通過率は、項目\\(j\\)に正解した受験者の割合であり、項目\\(j\\)の平均点と解釈することもできます。\n\n14.2.1 信頼性係数\n観測変数\\(x\\)は潜在変数\\(t\\)と誤差\\(e\\)の和であると考えると (\\(x = t + e\\))、テスト得点の分散は真の得点の分散と誤差の分散の単純な和で表されます：\n\\[\\sigma_{x}^2 = \\sigma_{t}^2+\\sigma_e^2\\]\nこの分散の比ρを信頼性係数と言います：\n\\[\\rho = \\frac{\\sigma_t^2}{\\sigma_t^2+\\sigma_e^2}=1-\\frac{\\sigma_e^2}{\\sigma_x^2}\\]\n\n\n14.2.2 Rを使った信頼性係数の計算\n\n\nコードを表示\n# Load and analyze LSAT data\ndata(LSAT)\n\n# Calculate descriptive statistics\ndsc &lt;- descript(LSAT)\n\n# Display main statistics in table format\ndata.frame(\n  item = paste(\"Item\", 1:5),\n  pass_rate = dsc$perc[, \"1\"],\n  point_biserial = dsc$bisCorr\n) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"LSAT Data Item Analysis\",\n    subtitle = \"n = 1000\"\n  ) |&gt;\n  fmt_percent(\n    columns = pass_rate,\n    decimals = 1\n  ) |&gt;\n  fmt_number(\n    columns = point_biserial,\n    decimals = 3\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#f0f8ff\"),\n    locations = cells_body(\n      columns = point_biserial,\n      rows = point_biserial &gt; 0.5\n    )\n  ) |&gt;\n  tab_footnote(\n    footnote = \"Cronbach's α = 0.295\",\n    locations = cells_column_labels(columns = point_biserial)\n  ) |&gt;\n  cols_label(\n    item = \"Item\",\n    pass_rate = \"Pass Rate\",\n    point_biserial = \"Point-Biserial Correlation\"\n  )\n\n\n\n\n\n\n\n\nLSAT Data Item Analysis\n\n\nn = 1000\n\n\nItem\nPass Rate\nPoint-Biserial Correlation1\n\n\n\n\nItem 1\n92.4%\n0.362\n\n\nItem 2\n70.9%\n0.567\n\n\nItem 3\n55.3%\n0.618\n\n\nItem 4\n76.3%\n0.534\n\n\nItem 5\n87.0%\n0.435\n\n\n\n1 Cronbach's α = 0.295\n\n\n\n\n\n\n\n\n\n\n14.2.3 古典的テスト理論の限界点\n\n\n\n\n\n\nMain Limitations of CTT\n\n\n\n\nTest-taker dependency: Item analysis results completely depend on the characteristics of the test-taker population\nTest dependency: Ability estimation depends on the test used\nScale issues: Validity of treating ordinal scales as interval scales\nMeasurement precision: Assumes constant measurement precision across all score ranges",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>項目応答理論とラッシュモデル</span>"
    ]
  },
  {
    "objectID": "11_項目応答理論とラッシュモデル.html#person-item-map",
    "href": "11_項目応答理論とラッシュモデル.html#person-item-map",
    "title": "13  項目応答理論とラッシュモデル",
    "section": "17.1 Person-Item Map",
    "text": "17.1 Person-Item Map\nテスト項目に正解できるかどうかの閾値として重要であるのは、50%（y軸が0.5でICCと交差する部分）となります。\n\n\nコードを表示\n# Person-Item Map\nplotPImap(rasch, sorted = TRUE)",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>項目応答理論とラッシュモデル</span>"
    ]
  },
  {
    "objectID": "11_項目応答理論とラッシュモデル.html#適合度指標",
    "href": "11_項目応答理論とラッシュモデル.html#適合度指標",
    "title": "13  項目応答理論とラッシュモデル",
    "section": "17.2 適合度指標",
    "text": "17.2 適合度指標\nラッシュモデルにおいては受験者能力と項目困難度の適合指標（fit index）であるインフィット・アウトフィットという指標が算出されます。\n\n\nコードを表示\n# Person ability parameters\nperson_parameters &lt;- person.parameter(rasch)\n\n# Calculate itemfit\nitem_fit &lt;- eRm::itemfit(person_parameters)\n\n# Check itemfit output structure and create appropriate dataframe\nif (is.data.frame(item_fit)) {\n  # If it's a dataframe\n  item_fit_df &lt;- item_fit\n  item_fit_df$item &lt;- rownames(item_fit_df)\n} else if (is.matrix(item_fit)) {\n  # If it's a matrix\n  item_fit_df &lt;- as.data.frame(item_fit)\n  item_fit_df$item &lt;- rownames(item_fit_df)\n} else {\n  # If it's a list or other structure - use default summary\n  item_fit_df &lt;- data.frame(\n    item = names(rasch$betapar)[1:min(10, length(rasch$betapar))],\n    estimate = rasch$betapar[1:min(10, length(rasch$betapar))],\n    std_error = rasch$se.beta[1:min(10, length(rasch$betapar))]\n  )\n}\n\n# Check column names and use only existing columns\navailable_cols &lt;- colnames(item_fit_df)\n\n# Create and display table\nif (\"item\" %in% available_cols) {\n  item_fit_df |&gt;\n    head(10) |&gt;\n    gt() |&gt;\n    tab_header(\n      title = \"Item Fit Statistics\",\n      subtitle = \"Available Fit Indices\"\n    ) |&gt;\n    fmt_number(\n      columns = setdiff(available_cols, \"item\"),\n      decimals = 3\n    ) |&gt;\n    tab_footnote(\n      footnote = \"Note: Fit statistics structure may vary by package version\",\n      locations = cells_column_labels(columns = 1)\n    )\n} else {\n  # Alternative display: item parameter summary\n  data.frame(\n    item = names(rasch$betapar)[1:min(10, length(rasch$betapar))],\n    difficulty = as.numeric(rasch$betapar)[1:min(10, length(rasch$betapar))],\n    std_error = as.numeric(rasch$se.beta)[1:min(10, length(rasch$betapar))]\n  ) |&gt;\n    gt() |&gt;\n    tab_header(\n      title = \"Item Parameter Summary\",\n      subtitle = \"Alternative Display for Fit Statistics\"\n    ) |&gt;\n    fmt_number(\n      columns = c(difficulty, std_error),\n      decimals = 3\n    ) |&gt;\n    tab_footnote(\n      footnote = \"Note: Detailed fit statistics require separate calculation\",\n      locations = cells_column_labels(columns = 1)\n    ) |&gt;\n    cols_label(\n      item = \"Item\",\n      difficulty = \"Difficulty\",\n      std_error = \"Standard Error\"\n    )\n}\n\n\n\n\n\n\n\n\nItem Fit Statistics\n\n\nAvailable Fit Indices\n\n\nitem1\nestimate\nstd_error\n\n\n\n\nbeta I1\n1.565\n0.249\n\n\nbeta I2\n0.051\n0.216\n\n\nbeta I3\n0.782\n0.222\n\n\nbeta I4\n−0.650\n0.228\n\n\nbeta I5\n−1.301\n0.254\n\n\nbeta I6\n0.099\n0.216\n\n\nbeta I7\n0.682\n0.220\n\n\nbeta I8\n0.732\n0.221\n\n\nbeta I9\n0.534\n0.218\n\n\nbeta I10\n−1.108\n0.245\n\n\n\n1 Note: Fit statistics structure may vary by package version\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of Fit Statistics\n\n\n\n\nInfit/Outfit MNSQ: 1.0 is the expected value\nLow fit (&lt; 0.8): Overfit\nHigh fit (&gt; 1.2): Misfit\nHigh-stakes testing: 0.8-1.2 is desirable",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>項目応答理論とラッシュモデル</span>"
    ]
  },
  {
    "objectID": "11_項目応答理論とラッシュモデル.html#モデル式",
    "href": "11_項目応答理論とラッシュモデル.html#モデル式",
    "title": "13  項目応答理論とラッシュモデル",
    "section": "18.1 モデル式",
    "text": "18.1 モデル式\n\n18.1.1 1PL（ラッシュモデル）\n\\[p_j(\\theta)=\\frac{1}{1+\\exp(-Da(\\theta-b_j))}\\]\n\n\n18.1.2 2PL\n\\[p_j(\\theta)=\\frac{1}{1+\\exp(-Da_j(\\theta-b_j))}\\]\n\n\n18.1.3 3PL\n\\[p_j(\\theta)=c_j+\\frac{1-c_j}{1+\\exp(-Da_j(\\theta-b_j))}\\]",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>項目応答理論とラッシュモデル</span>"
    ]
  },
  {
    "objectID": "11_項目応答理論とラッシュモデル.html#テスト情報量の計算式",
    "href": "11_項目応答理論とラッシュモデル.html#テスト情報量の計算式",
    "title": "13  項目応答理論とラッシュモデル",
    "section": "19.1 テスト情報量の計算式",
    "text": "19.1 テスト情報量の計算式\n\n\n\n\n\n\nTest Information for Each Model\n\n\n\n\n1PL: \\(I(\\theta_i)=D^2a^2\\sum_{j=1}^n p_j(\\theta_i) q_j(\\theta_i)\\)\n2PL: \\(I(\\theta_i)=D^2\\sum_{j=1}^n a_j^2 p_j(\\theta_i) q_j(\\theta_i)\\)\n3PL: \\(I(\\theta_i)=D^2\\sum_{j=1}^n \\dfrac{a_j^2(p_j(\\theta_i)-c_j)^2 q_j(\\theta_i)}{p_j(\\theta_i)(1-c_j)^2}\\)\n\nWhere: \\(q_j(\\theta) = 1 - p_j(\\theta)\\), \\(D = 1.7\\) (normal metric) or \\(D = 1.0\\) (logistic metric)",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>項目応答理論とラッシュモデル</span>"
    ]
  },
  {
    "objectID": "11_項目応答理論とラッシュモデル.html#andersenのlr-test",
    "href": "11_項目応答理論とラッシュモデル.html#andersenのlr-test",
    "title": "13  項目応答理論とラッシュモデル",
    "section": "20.1 AndersenのLR test",
    "text": "20.1 AndersenのLR test\n受験者をグループに分け、条件付き尤度の比を検討します。\n\n\nコードを表示\n# Andersen's LR test\nmodel &lt;- RM(raschdat1, sum0 = FALSE)\nlr_test &lt;- LRtest(model, splitcr = \"median\")\n\n# Display LR test results\ndata.frame(\n  statistic = c(\"LR Value\", \"Degrees of Freedom\", \"p-value\", \"Decision\"),\n  value = c(\n    round(lr_test$LR, 3),\n    lr_test$df,\n    round(lr_test$pvalue, 4),\n    ifelse(lr_test$pvalue &gt; 0.05, \"Model Fits\", \"Model Does Not Fit\")\n  )\n) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Andersen's Likelihood Ratio Test\",\n    subtitle = \"Verification of Rasch Model Assumptions\"\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = ifelse(lr_test$pvalue &gt; 0.05, \"#e6ffe6\", \"#ffe6e6\")),\n    locations = cells_body(rows = 4)\n  ) |&gt;\n  tab_footnote(\n    footnote = \"H₀: Item parameters are equal across groups (retain null hypothesis if p &gt; 0.05)\",\n    locations = cells_column_labels(columns = value)\n  ) |&gt;\n  cols_label(\n    statistic = \"Statistic\",\n    value = \"Value\"\n  )\n\n\n\n\n\n\n\n\nAndersen's Likelihood Ratio Test\n\n\nVerification of Rasch Model Assumptions\n\n\nStatistic\nValue1\n\n\n\n\nLR Value\n32.542\n\n\nDegrees of Freedom\n29\n\n\np-value\n0.2966\n\n\nDecision\nModel Fits\n\n\n\n1 H₀: Item parameters are equal across groups (retain null hypothesis if p &gt; 0.05)",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>項目応答理論とラッシュモデル</span>"
    ]
  },
  {
    "objectID": "11_項目応答理論とラッシュモデル.html#wald検定",
    "href": "11_項目応答理論とラッシュモデル.html#wald検定",
    "title": "13  項目応答理論とラッシュモデル",
    "section": "20.2 Wald検定",
    "text": "20.2 Wald検定\n項目ごとの適合を評価します。\n\n\nコードを表示\n# Perform Wald test\nwald_result &lt;- Waldtest(model)\n\n# Check structure and extract data safely\nif (!is.null(wald_result) && (is.matrix(wald_result) || is.data.frame(wald_result))) {\n  # Convert to dataframe if matrix\n  if (is.matrix(wald_result)) {\n    wald_result &lt;- as.data.frame(wald_result)\n  }\n  \n  # Detect column names dynamically\n  col_names &lt;- colnames(wald_result)\n  \n  # Find z-statistic column\n  z_col &lt;- grep(\"z|Z\", col_names, value = TRUE)[1]\n  if (is.na(z_col)) z_col &lt;- col_names[1]\n  \n  # Find p-value column\n  p_col &lt;- grep(\"p|P\", col_names, value = TRUE)[1]\n  if (is.na(p_col)) p_col &lt;- col_names[2]\n  \n  # Create working dataframe\n  wald_df &lt;- data.frame(\n    item = rownames(wald_result),\n    z_stat = wald_result[[z_col]],\n    p_value = wald_result[[p_col]],\n    stringsAsFactors = FALSE\n  )\n  \n  # Add decision column\n  wald_df$decision &lt;- ifelse(wald_df$p_value &lt; 0.05, \"Problem\", \"Fit\")\n  \n  # Filter items with p &lt; 0.10\n  wald_filtered &lt;- wald_df[wald_df$p_value &lt; 0.10, , drop = FALSE]\n  \n  # Display based on results\n  if (nrow(wald_filtered) &gt; 0) {\n    # Sort by p-value\n    wald_sorted &lt;- wald_filtered[order(wald_filtered$p_value), , drop = FALSE]\n    \n    # Display table directly (no assignment)\n    wald_sorted |&gt;\n      gt() |&gt;\n      tab_header(\n        title = \"Wald Test Results (Items Requiring Attention)\",\n        subtitle = \"Showing only items with p &lt; 0.10\"\n      ) |&gt;\n      cols_label(\n        item = \"Item\",\n        z_stat = \"z-statistic\",\n        p_value = \"p-value\",\n        decision = \"Decision\"\n      ) |&gt;\n      fmt_number(\n        columns = z_stat,\n        decimals = 3\n      ) |&gt;\n      fmt_number(\n        columns = p_value,\n        decimals = 4\n      ) |&gt;\n      tab_style(\n        style = cell_fill(color = \"#ffe6e6\"),\n        locations = cells_body(\n          columns = decision,\n          rows = decision == \"Problem\"\n        )\n      ) |&gt;\n      tab_footnote(\n        footnote = \"|z| &gt; 1.96 or p &lt; 0.05 indicates significant group differences\",\n        locations = cells_column_labels(columns = z_stat)\n      )\n  } else {\n    # No problematic items found\n    data.frame(\n      message = \"No items with p &lt; 0.10. All items show good fit.\"\n    ) |&gt;\n      gt() |&gt;\n      tab_header(\n        title = \"Wald Test Results\",\n        subtitle = \"All items meet fit criteria\"\n      ) |&gt;\n      cols_label(\n        message = \"Message\"\n      )\n  }\n} else {\n  # Fallback for unexpected structure\n  data.frame(\n    message = \"Unable to parse Wald test results. Please check data structure.\"\n  ) |&gt;\n    gt() |&gt;\n    tab_header(\n      title = \"Wald Test Results\",\n      subtitle = \"Error\"\n    ) |&gt;\n    cols_label(\n      message = \"Message\"\n    )\n}\n\n\n\n\n\n\n\n\nWald Test Results\n\n\nError\n\n\nMessage\n\n\n\n\nUnable to parse Wald test results. Please check data structure.",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>項目応答理論とラッシュモデル</span>"
    ]
  },
  {
    "objectID": "11_項目応答理論とラッシュモデル.html#詳細な項目分析",
    "href": "11_項目応答理論とラッシュモデル.html#詳細な項目分析",
    "title": "13  項目応答理論とラッシュモデル",
    "section": "21.1 詳細な項目分析",
    "text": "21.1 詳細な項目分析\n\n\nコードを表示\n# Perform detailed item analysis using mirt package\nmirt_model &lt;- mirt(raschdat1, model = 1, itemtype = \"Rasch\", verbose = FALSE)\n\n# Extract item parameters using coef\nitem_params_mirt &lt;- coef(mirt_model, simplify = TRUE)$items\n\n# Extract item statistics\nitem_stats &lt;- data.frame(\n  item = colnames(raschdat1),\n  difficulty = -item_params_mirt[, \"d\"],  # Convert to difficulty (b = -d for Rasch)\n  pass_rate = colMeans(raschdat1),\n  item_total_cor = apply(raschdat1, 2, function(x) {\n    cor(x, rowSums(raschdat1) - x)\n  }),\n  stringsAsFactors = FALSE\n)\n\n# Calculate fit statistics using mirt\nitem_fit_mirt &lt;- itemfit(mirt_model, fit_stats = c(\"X2\", \"G2\"))\n\n# Combine statistics\nitem_stats$chi_square &lt;- item_fit_mirt$X2\nitem_stats$p_value &lt;- item_fit_mirt$p.X2\n\n# Classify items\nitem_stats &lt;- item_stats |&gt;\n  mutate(\n    difficulty_level = case_when(\n      difficulty &lt; -1 ~ \"Very Easy\",\n      difficulty &lt; 0 ~ \"Easy\",\n      difficulty &lt; 1 ~ \"Difficult\",\n      TRUE ~ \"Very Difficult\"\n    ),\n    discrimination_quality = case_when(\n      item_total_cor &lt; 0.2 ~ \"Poor\",\n      item_total_cor &lt; 0.3 ~ \"Fair\",\n      item_total_cor &lt; 0.4 ~ \"Good\",\n      TRUE ~ \"Excellent\"\n    ),\n    fit_status = ifelse(p_value &lt; 0.05, \"Misfit\", \"Good Fit\")\n  )\n\n# Display comprehensive item analysis\nitem_stats |&gt;\n  arrange(difficulty) |&gt;\n  head(15) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Comprehensive Item Analysis\",\n    subtitle = \"Sorted by Difficulty (15 items shown)\"\n  ) |&gt;\n  fmt_number(\n    columns = c(difficulty, pass_rate, item_total_cor, chi_square),\n    decimals = 3\n  ) |&gt;\n  fmt_number(\n    columns = p_value,\n    decimals = 4\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#e6ffe6\"),\n    locations = cells_body(\n      columns = fit_status,\n      rows = fit_status == \"Good Fit\"\n    )\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#ffe6e6\"),\n    locations = cells_body(\n      columns = fit_status,\n      rows = fit_status == \"Misfit\"\n    )\n  ) |&gt;\n  cols_label(\n    item = \"Item\",\n    difficulty = \"Difficulty\",\n    pass_rate = \"Pass Rate\",\n    item_total_cor = \"Item-Total r\",\n    chi_square = \"χ²\",\n    p_value = \"p-value\",\n    difficulty_level = \"Level\",\n    discrimination_quality = \"Discrimination\",\n    fit_status = \"Fit\"\n  ) |&gt;\n  tab_spanner(\n    label = \"Basic Statistics\",\n    columns = c(difficulty, pass_rate, item_total_cor)\n  ) |&gt;\n  tab_spanner(\n    label = \"Fit Statistics\",\n    columns = c(chi_square, p_value, fit_status)\n  ) |&gt;\n  tab_spanner(\n    label = \"Classification\",\n    columns = c(difficulty_level, discrimination_quality)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComprehensive Item Analysis\n\n\nSorted by Difficulty (15 items shown)\n\n\nItem\n\nBasic Statistics\n\n\nFit Statistics\n\n\nClassification\n\n\n\nDifficulty\nPass Rate\nItem-Total r\nχ²\np-value\nFit\nLevel\nDiscrimination\n\n\n\n\nI1\n−1.367\n0.760\n0.371\n10.071\n0.3448\nGood Fit\nVery Easy\nGood\n\n\nI26\n−1.009\n0.700\n0.392\n6.503\n0.6887\nGood Fit\nVery Easy\nGood\n\n\nI22\n−0.792\n0.660\n0.472\n13.636\n0.1359\nGood Fit\nEasy\nExcellent\n\n\nI21\n−0.739\n0.650\n0.430\n5.917\n0.7482\nGood Fit\nEasy\nExcellent\n\n\nI3\n−0.585\n0.620\n0.386\n14.334\n0.1109\nGood Fit\nEasy\nGood\n\n\nI30\n−0.534\n0.610\n0.329\n7.081\n0.6287\nGood Fit\nEasy\nGood\n\n\nI8\n−0.534\n0.610\n0.409\n10.870\n0.2847\nGood Fit\nEasy\nExcellent\n\n\nI20\n−0.484\n0.600\n0.240\n11.772\n0.2265\nGood Fit\nEasy\nFair\n\n\nI23\n−0.484\n0.600\n0.298\n9.617\n0.3824\nGood Fit\nEasy\nFair\n\n\nI7\n−0.484\n0.600\n0.406\n6.838\n0.6540\nGood Fit\nEasy\nExcellent\n\n\nI9\n−0.336\n0.570\n0.433\n10.315\n0.3256\nGood Fit\nEasy\nExcellent\n\n\nI12\n−0.191\n0.540\n0.446\n9.343\n0.4062\nGood Fit\nEasy\nExcellent\n\n\nI17\n−0.143\n0.530\n0.225\n3.790\n0.9247\nGood Fit\nEasy\nFair\n\n\nI15\n−0.143\n0.530\n0.348\n10.615\n0.3031\nGood Fit\nEasy\nGood\n\n\nI6\n0.098\n0.480\n0.207\n11.159\n0.2649\nGood Fit\nDifficult\nFair",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>項目応答理論とラッシュモデル</span>"
    ]
  },
  {
    "objectID": "11_項目応答理論とラッシュモデル.html#一次元性の検証",
    "href": "11_項目応答理論とラッシュモデル.html#一次元性の検証",
    "title": "13  項目応答理論とラッシュモデル",
    "section": "22.1 一次元性の検証",
    "text": "22.1 一次元性の検証\n\n\nコードを表示\n# Check unidimensionality using factor analysis\nfa_result &lt;- psych::fa(raschdat1, nfactors = 1, rotate = \"none\")\n\n# Scree plot\neigenvalues &lt;- eigen(cor(raschdat1))$values\n\ndata.frame(\n  factor = 1:length(eigenvalues),\n  eigenvalue = eigenvalues,\n  variance_explained = eigenvalues / sum(eigenvalues) * 100,\n  cumulative_variance = cumsum(eigenvalues / sum(eigenvalues) * 100)\n) |&gt;\n  head(10) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Eigenvalue Decomposition\",\n    subtitle = \"Testing Unidimensionality Assumption\"\n  ) |&gt;\n  fmt_number(\n    columns = c(eigenvalue, variance_explained, cumulative_variance),\n    decimals = 2\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#e6ffe6\"),\n    locations = cells_body(\n      columns = variance_explained,\n      rows = 1\n    )\n  ) |&gt;\n  cols_label(\n    factor = \"Factor\",\n    eigenvalue = \"Eigenvalue\",\n    variance_explained = \"Variance (%)\",\n    cumulative_variance = \"Cumulative (%)\"\n  ) |&gt;\n  tab_footnote(\n    footnote = \"First factor should explain &gt;20% of variance for unidimensionality\",\n    locations = cells_column_labels(columns = variance_explained)\n  )\n\n\n\n\n\n\n\n\nEigenvalue Decomposition\n\n\nTesting Unidimensionality Assumption\n\n\nFactor\nEigenvalue\nVariance (%)1\nCumulative (%)\n\n\n\n\n1\n5.59\n18.64\n18.64\n\n\n2\n1.81\n6.02\n24.65\n\n\n3\n1.65\n5.51\n30.16\n\n\n4\n1.50\n5.00\n35.17\n\n\n5\n1.40\n4.66\n39.83\n\n\n6\n1.38\n4.60\n44.43\n\n\n7\n1.35\n4.49\n48.92\n\n\n8\n1.24\n4.15\n53.07\n\n\n9\n1.14\n3.80\n56.86\n\n\n10\n1.10\n3.67\n60.54\n\n\n\n1 First factor should explain &gt;20% of variance for unidimensionality\n\n\n\n\n\n\n\n\nコードを表示\n# Scree plot visualization\nplot(1:10, eigenvalues[1:10], \n     type = \"b\", \n     main = \"Scree Plot\",\n     xlab = \"Factor Number\",\n     ylab = \"Eigenvalue\",\n     pch = 19,\n     col = \"blue\")\nabline(h = 1, lty = 2, col = \"red\")",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>項目応答理論とラッシュモデル</span>"
    ]
  },
  {
    "objectID": "11_項目応答理論とラッシュモデル.html#参考ウェブサイト",
    "href": "11_項目応答理論とラッシュモデル.html#参考ウェブサイト",
    "title": "13  項目応答理論とラッシュモデル",
    "section": "25.1 参考ウェブサイト",
    "text": "25.1 参考ウェブサイト\n\nRasch Analysis in R\nRISE Rasch Vignette\nBayesian IRT with brms",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>項目応答理論とラッシュモデル</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html",
    "href": "12_拡張ラッシュモデル.html",
    "title": "14  T検定と効果量",
    "section": "",
    "text": "15 はじめに\n項目応答理論（IRT）は基本的に2値データ（正解・不正解、はい・いいえ）の観測変数を目的変数、項目困難度・受験者能力という潜在変数を説明変数とした統計モデルでした。しかし、実際の教育測定や心理測定では、より複雑なデータ構造に対応する必要があります。\n本章では、以下の3つの拡張モデルについて詳しく解説します：\nコードを表示\n# Load required packages\npacman::p_load(\n  tidyverse,  # Data manipulation and visualization\n  mirt,       # IRT analysis\n  gt,         # Table formatting\n  gtExtras,   # gt extensions\n  mokken,     # Mokken scale analysis\n  psych,      # Psychometric statistics\n  plotly      # Interactive plots\n)\n\n# Global options\noptions(digits = 3)\ntheme_set(theme_minimal())",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#理論的背景",
    "href": "12_拡張ラッシュモデル.html#理論的背景",
    "title": "14  T検定と効果量",
    "section": "16.1 理論的背景",
    "text": "16.1 理論的背景\n\n16.1.1 カットポイントの概念\n順序データを扱う順序ロジスティック回帰分析では、カットポイント（閾値）と呼ばれる値が重要な役割を果たします。\n\n\nコードを表示\n# Visualize cutpoints\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Data preparation\ntheta_seq &lt;- seq(-3, 3, 0.1)\ncutpoints &lt;- c(-1, 0, 1)\n\n# Calculate category probabilities\nprob_data &lt;- data.frame(\n  theta = rep(theta_seq, 4),\n  category = rep(1:4, each = length(theta_seq)),\n  probability = c(\n    plogis(cutpoints[1] - theta_seq),\n    plogis(cutpoints[2] - theta_seq) - plogis(cutpoints[1] - theta_seq),\n    plogis(cutpoints[3] - theta_seq) - plogis(cutpoints[2] - theta_seq),\n    1 - plogis(cutpoints[3] - theta_seq)\n  )\n)\n\n# Plot\nggplot(prob_data, aes(x = theta, y = probability, color = factor(category))) +\n  geom_line(size = 1.2) +\n  labs(\n    title = \"Category Characteristic Curves for GRM\",\n    x = \"Latent Trait (θ)\",\n    y = \"Probability\",\n    color = \"Category\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\nC個の段階があるとき、カットポイントの数はC-1個存在します。",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#データ準備と探索",
    "href": "12_拡張ラッシュモデル.html#データ準備と探索",
    "title": "14  T検定と効果量",
    "section": "16.2 データ準備と探索",
    "text": "16.2 データ準備と探索\n\n16.2.1 データの読み込みと前処理\n\n\nコードを表示\n# Load data\ndata(SAT12)\n\n# Data preprocessing\nSAT12[SAT12 == 8] &lt;- 5  # Unify category 8 to 5\n\n# Display data overview using gt\ndata.frame(\n  Metric = c(\"Sample Size\", \"Number of Items\", \"Rating Range (Min)\", \"Rating Range (Max)\"),\n  Value = c(\n    nrow(SAT12),\n    ncol(SAT12),\n    min(SAT12),\n    max(SAT12)\n  )\n) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = md(\"**Data Overview**\"),\n    subtitle = \"SAT12 Dataset Characteristics\"\n  ) |&gt;\n  fmt_number(columns = Value, decimals = 0)\n\n\n\n\n\n\n\n\nData Overview\n\n\nSAT12 Dataset Characteristics\n\n\nMetric\nValue\n\n\n\n\nSample Size\n600\n\n\nNumber of Items\n32\n\n\nRating Range (Min)\n1\n\n\nRating Range (Max)\n5\n\n\n\n\n\n\n\n\n\n16.2.2 データの概要表示\n\n\nコードを表示\n# Display data overview with gt\nSAT12 |&gt;\n  head(10) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = md(\"**SAT12 Dataset**\"),\n    subtitle = \"Response Patterns of First 10 Participants\"\n  ) |&gt;\n  tab_spanner(\n    label = \"Items 1-8\",\n    columns = 1:8\n  ) |&gt;\n  tab_spanner(\n    label = \"Items 9-16\",\n    columns = 9:16\n  ) |&gt;\n  tab_spanner(\n    label = \"Items 17-24\",\n    columns = 17:24\n  ) |&gt;\n  tab_spanner(\n    label = \"Items 25-32\",\n    columns = 25:32\n  ) |&gt;\n  tab_options(\n    table.font.size = 10,\n    heading.title.font.size = 16,\n    heading.subtitle.font.size = 12,\n    column_labels.font.size = 9\n  ) |&gt;\n  opt_row_striping()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSAT12 Dataset\n\n\nResponse Patterns of First 10 Participants\n\n\n\nItems 1-8\n\n\nItems 9-16\n\n\nItems 17-24\n\n\nItems 25-32\n\n\n\nItem.1\nItem.2\nItem.3\nItem.4\nItem.5\nItem.6\nItem.7\nItem.8\nItem.9\nItem.10\nItem.11\nItem.12\nItem.13\nItem.14\nItem.15\nItem.16\nItem.17\nItem.18\nItem.19\nItem.20\nItem.21\nItem.22\nItem.23\nItem.24\nItem.25\nItem.26\nItem.27\nItem.28\nItem.29\nItem.30\nItem.31\nItem.32\n\n\n\n\n1\n4\n5\n2\n3\n1\n2\n1\n3\n1\n2\n4\n2\n1\n5\n3\n4\n4\n1\n4\n3\n3\n4\n1\n3\n5\n1\n3\n1\n5\n4\n5\n\n\n3\n4\n2\n5\n3\n3\n2\n5\n3\n1\n2\n5\n2\n1\n5\n2\n4\n1\n1\n4\n3\n3\n5\n1\n5\n4\n1\n4\n5\n5\n4\n5\n\n\n1\n4\n5\n4\n3\n2\n2\n3\n3\n2\n2\n1\n3\n1\n5\n5\n4\n1\n3\n4\n3\n3\n1\n1\n3\n4\n1\n3\n4\n4\n4\n1\n\n\n2\n4\n4\n2\n3\n3\n2\n4\n3\n2\n2\n4\n2\n1\n5\n2\n4\n1\n3\n4\n3\n1\n5\n2\n5\n4\n1\n3\n4\n2\n4\n2\n\n\n2\n4\n5\n2\n3\n2\n2\n1\n1\n2\n2\n4\n2\n1\n5\n4\n4\n5\n1\n4\n3\n3\n3\n1\n1\n5\n1\n3\n1\n2\n4\n1\n\n\n1\n4\n3\n1\n3\n2\n2\n3\n3\n1\n2\n3\n2\n1\n5\n5\n4\n4\n1\n4\n3\n3\n4\n1\n1\n4\n1\n4\n2\n3\n4\n3\n\n\n1\n4\n5\n2\n2\n2\n2\n2\n3\n1\n2\n4\n2\n1\n5\n5\n4\n2\n1\n4\n3\n3\n3\n1\n5\n5\n1\n3\n3\n1\n4\n3\n\n\n2\n4\n1\n5\n3\n2\n2\n5\n3\n1\n2\n5\n1\n1\n5\n3\n4\n1\n1\n4\n3\n3\n4\n1\n2\n5\n1\n4\n1\n5\n4\n1\n\n\n5\n1\n2\n3\n2\n2\n4\n4\n4\n2\n2\n4\n1\n3\n5\n5\n4\n5\n1\n3\n3\n3\n1\n2\n1\n4\n1\n4\n5\n5\n1\n4\n\n\n5\n3\n4\n2\n3\n2\n4\n5\n3\n1\n2\n4\n2\n1\n3\n3\n4\n2\n1\n4\n3\n3\n1\n5\n2\n1\n1\n4\n5\n3\n5\n2\n\n\n\n\n\n\n\n\n\n16.2.3 古典的テスト理論による項目分析\n\n\nコードを表示\n# Calculate item statistics\nitem_stats &lt;- itemstats(SAT12)\n\n# Display basic statistics\ndata.frame(\n  Statistic = c(\"Mean Score\", \"Standard Deviation\", \"Cronbach's Alpha\", \"Number of Items\", \"Sample Size\"),\n  Value = c(\n    mean(rowSums(SAT12)),\n    sd(rowSums(SAT12)),\n    psych::alpha(SAT12)$total$raw_alpha,\n    ncol(SAT12),\n    nrow(SAT12)\n  )\n) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = md(\"**Basic Statistics**\"),\n    subtitle = \"Classical Test Theory Analysis\"\n  ) |&gt;\n  fmt_number(columns = Value, decimals = 3) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#E8F4FD\"),\n    locations = cells_body(rows = Statistic == \"Cronbach's Alpha\")\n  )\n\n\nSome items ( Item.2 Item.3 Item.5 Item.12 Item.15 Item.16 Item.17 Item.18 Item.20 Item.21 Item.23 Item.26 Item.30 Item.31 Item.32 ) were negatively correlated with the first principal component and \nprobably should be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\n\n\n\n\n\n\n\n\nBasic Statistics\n\n\nClassical Test Theory Analysis\n\n\nStatistic\nValue\n\n\n\n\nMean Score\n91.888\n\n\nStandard Deviation\n6.240\n\n\nCronbach's Alpha\n0.018\n\n\nNumber of Items\n32.000\n\n\nSample Size\n600.000\n\n\n\n\n\n\n\n\n\nコードを表示\n# Calculate item rating proportions\nprop_data &lt;- item_stats$proportions |&gt;\n  as.data.frame() |&gt;\n  mutate(Item = paste0(\"Item\", 1:nrow(item_stats$proportions))) |&gt;\n  dplyr::select(Item, everything())\n\n# Display proportions for first 10 items\nprop_data |&gt;\n  slice(1:10) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = md(\"**Item Rating Proportions**\"),\n    subtitle = \"Selection Rate for Each Rating (1-5) - First 10 Items\"\n  ) |&gt;\n  fmt_percent(columns = -Item, decimals = 1) |&gt;\n  data_color(\n    columns = -Item,\n    colors = scales::col_numeric(\n      palette = c(\"white\", \"darkgreen\"),\n      domain = c(0, 1)\n    )\n  ) |&gt;\n  tab_options(\n    table.font.size = 11,\n    heading.title.font.size = 14\n  )\n\n\n\n\n\n\n\n\nItem Rating Proportions\n\n\nSelection Rate for Each Rating (1-5) - First 10 Items\n\n\nItem\n1\n2\n3\n4\n5\n\n\n\n\nItem1\n28.3%\n20.3%\n26.7%\n23.2%\n1.5%\n\n\nItem2\n21.2%\n2.2%\n7.0%\n56.8%\n12.8%\n\n\nItem3\n16.5%\n18.3%\n26.0%\n9.8%\n29.3%\n\n\nItem4\n16.5%\n37.8%\n14.8%\n17.2%\n13.7%\n\n\nItem5\n9.3%\n14.3%\n62.0%\n9.3%\n5.0%\n\n\nItem6\n16.0%\n58.2%\n10.7%\n4.3%\n10.8%\n\n\nItem7\n2.5%\n76.0%\n0.7%\n19.0%\n1.8%\n\n\nItem8\n20.2%\n20.5%\n20.7%\n25.0%\n13.7%\n\n\nItem9\n6.5%\n1.0%\n88.5%\n3.3%\n0.7%\n\n\nItem10\n42.2%\n21.5%\n16.5%\n2.8%\n17.0%",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#モデル推定",
    "href": "12_拡張ラッシュモデル.html#モデル推定",
    "title": "14  T検定と効果量",
    "section": "16.3 モデル推定",
    "text": "16.3 モデル推定\n\n16.3.1 段階反応モデルの推定\n\n\nコードを表示\n# Estimate Graded Response Model\nfit_grm &lt;- mirt(SAT12, 1, itemtype = 'graded', SE = TRUE, verbose = FALSE)\n\n# Model summary\nprint(fit_grm)\n\n\n\nCall:\nmirt(data = SAT12, model = 1, itemtype = \"graded\", SE = TRUE, \n    verbose = FALSE)\n\nFull-information item factor analysis with 1 factor(s).\nConverged within 1e-04 tolerance after 18 EM iterations.\nmirt version: 1.44.0 \nM-step optimizer: BFGS \nEM acceleration: Ramsay \nNumber of rectangular quadrature: 61\nLatent density type: Gaussian \n\nInformation matrix estimated with method: Oakes\nSecond-order test: model is a possible local maximum\nCondition number of information matrix =  6036\n\nLog-likelihood = -19778\nEstimated parameters: 160 \nAIC = 39877\nBIC = 40580; SABIC = 40072\nG2 (1e+10) = 31887, p = 1\nRMSEA = 0, CFI = NaN, TLI = NaN\n\n\n\n\n16.3.2 モデル適合度の評価\n\n\nコードを表示\n# Calculate model fit\nmodel_fit &lt;- M2(fit_grm, type = \"C2\", calcNULL = FALSE)\n\n# Organize fit indices\nfit_indices &lt;- data.frame(\n  Index = c(\"M2 Statistic\", \"Degrees of Freedom\", \"p-value\", \"RMSEA\", \"RMSEA (95% CI)\", \"SRMSR\", \"TLI\", \"CFI\"),\n  Value = c(\n    sprintf(\"%.2f\", model_fit$M2),\n    sprintf(\"%d\", model_fit$df),\n    sprintf(\"%.4f\", model_fit$p),\n    sprintf(\"%.3f\", model_fit$RMSEA),\n    sprintf(\"[%.3f, %.3f]\", model_fit$RMSEA_5, model_fit$RMSEA_95),\n    sprintf(\"%.3f\", model_fit$SRMSR),\n    sprintf(\"%.3f\", model_fit$TLI),\n    sprintf(\"%.3f\", model_fit$CFI)\n  ),\n  Criterion = c(\"\", \"\", \"&gt; 0.05\", \"&lt; 0.06\", \"\", \"&lt; 0.08\", \"&gt; 0.95\", \"&gt; 0.95\"),\n  Assessment = c(\n    \"\",\n    \"\",\n    ifelse(model_fit$p &gt; 0.05, \"✓\", \"✗\"),\n    ifelse(model_fit$RMSEA &lt; 0.06, \"✓\", \"✗\"),\n    \"\",\n    ifelse(model_fit$SRMSR &lt; 0.08, \"✓\", \"✗\"),\n    ifelse(model_fit$TLI &gt; 0.95, \"✓\", \"✗\"),\n    ifelse(model_fit$CFI &gt; 0.95, \"✓\", \"✗\")\n  )\n)\n\n# Display fit indices with gt\nfit_indices |&gt;\n  gt() |&gt;\n  tab_header(\n    title = md(\"**Model Fit Indices**\"),\n    subtitle = \"Graded Response Model Fit Assessment\"\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#D4EDDA\"),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_body(\n      columns = Assessment,\n      rows = Assessment == \"✓\"\n    )\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"#F8D7DA\"),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_body(\n      columns = Assessment,\n      rows = Assessment == \"✗\"\n    )\n  ) |&gt;\n  tab_footnote(\n    footnote = \"RMSEA: Root Mean Square Error of Approximation, SRMSR: Standardized Root Mean Square Residual\",\n    locations = cells_column_labels(columns = Index)\n  )\n\n\n\n\n\n\n\n\nModel Fit Indices\n\n\nGraded Response Model Fit Assessment\n\n\nIndex1\nValue\nCriterion\nAssessment\n\n\n\n\nM2 Statistic\n697.20\n\n\n\n\nDegrees of Freedom\n464\n\n\n\n\np-value\n0.0000\n&gt; 0.05\n✗\n\n\nRMSEA\n0.029\n&lt; 0.06\n✓\n\n\nRMSEA (95% CI)\n[0.024, 0.033]\n\n\n\n\nSRMSR\n0.050\n&lt; 0.08\n✓\n\n\nTLI\n0.756\n&gt; 0.95\n✗\n\n\nCFI\n0.772\n&gt; 0.95\n✗\n\n\n\n1 RMSEA: Root Mean Square Error of Approximation, SRMSR: Standardized Root Mean Square Residual",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#項目分析",
    "href": "12_拡張ラッシュモデル.html#項目分析",
    "title": "14  T検定と効果量",
    "section": "16.4 項目分析",
    "text": "16.4 項目分析\n\n16.4.1 項目適合度統計量\n\n\nコードを表示\n# Calculate item fit - remove verbose argument as it's not supported\nitem_fit &lt;- mirt::itemfit(fit_grm, fit_stats = c(\"S_X2\", \"infit\"))\n\n# Display item fit for first 10 items\nitem_fit |&gt;\n  slice(1:10) |&gt;\n  dplyr::select(item, S_X2, df.S_X2, RMSEA.S_X2, p.S_X2, infit) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = md(\"**Item Fit Statistics**\"),\n    subtitle = \"S-X2 Statistics and Infit MNSQ (First 10 Items)\"\n  ) |&gt;\n  fmt_number(columns = c(S_X2, RMSEA.S_X2, infit), decimals = 3) |&gt;\n  fmt_number(columns = p.S_X2, decimals = 4) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#D4EDDA\"),\n    locations = cells_body(\n      columns = RMSEA.S_X2,\n      rows = RMSEA.S_X2 &lt; 0.06\n    )\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#D4EDDA\"),\n    locations = cells_body(\n      columns = infit,\n      rows = infit &gt;= 0.7 & infit &lt;= 1.3\n    )\n  ) |&gt;\n  tab_footnote(\n    footnote = \"RMSEA &lt; 0.06, Infit MNSQ: 0.7-1.3 indicate good fit\",\n    locations = cells_column_labels(columns = c(RMSEA.S_X2, infit))\n  )\n\n\n\n\n\n\n\n\nItem Fit Statistics\n\n\nS-X2 Statistics and Infit MNSQ (First 10 Items)\n\n\nitem\nS_X2\ndf.S_X2\nRMSEA.S_X21\np.S_X2\ninfit1\n\n\n\n\nItem.1\n74.360\n71\n0.009\n0.3694\n0.930\n\n\nItem.2\n86.884\n62\n0.026\n0.0202\n0.970\n\n\nItem.3\n86.553\n83\n0.008\n0.3732\n0.953\n\n\nItem.4\n105.885\n84\n0.021\n0.0536\n0.998\n\n\nItem.5\n84.286\n77\n0.013\n0.2667\n1.000\n\n\nItem.6\n75.104\n62\n0.019\n0.1227\n0.985\n\n\nItem.7\n36.872\n27\n0.025\n0.0975\n0.972\n\n\nItem.8\n107.236\n86\n0.020\n0.0603\n0.987\n\n\nItem.9\n22.935\n27\n0.000\n0.6885\n1.006\n\n\nItem.10\n72.004\n71\n0.005\n0.4444\n0.986\n\n\n\n1 RMSEA &lt; 0.06, Infit MNSQ: 0.7-1.3 indicate good fit\n\n\n\n\n\n\n\n\n\n\n16.4.2 項目パラメータ\n\n\nコードを表示\n# Extract item parameters\nparams &lt;- coef(fit_grm, IRTpars = TRUE, simplify = TRUE)\n\n# Create parameter table (first 10 items)\nparams$items |&gt;\n  as.data.frame() |&gt;\n  slice(1:10) |&gt;\n  mutate(Item = paste0(\"Item\", 1:10)) |&gt;\n  dplyr::select(Item, everything()) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = md(\"**Item Parameter Estimates**\"),\n    subtitle = \"Discrimination (a) and Threshold Difficulties (b1-b4)\"\n  ) |&gt;\n  fmt_number(columns = -Item, decimals = 3) |&gt;\n  tab_spanner(\n    label = \"Discrimination\",\n    columns = a\n  ) |&gt;\n  tab_spanner(\n    label = \"Threshold Difficulties\",\n    columns = starts_with(\"b\")\n  ) |&gt;\n  data_color(\n    columns = a,\n    colors = scales::col_numeric(\n      palette = c(\"lightblue\", \"darkblue\"),\n      domain = c(0, 2)\n    )\n  ) |&gt;\n  tab_options(\n    table.font.size = 11,\n    heading.title.font.size = 14\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem Parameter Estimates\n\n\nDiscrimination (a) and Threshold Difficulties (b1-b4)\n\n\nItem\n\nDiscrimination\n\n\nThreshold Difficulties\n\n\n\na\nb1\nb2\nb3\nb4\n\n\n\n\nItem1\n0.746\n−1.378\n−0.055\n1.673\n5.932\n\n\nItem2\n−0.360\n3.723\n3.362\n2.312\n−5.539\n\n\nItem3\n−0.578\n3.005\n1.226\n−0.731\n−1.565\n\n\nItem4\n0.123\n−13.220\n1.373\n6.551\n14.969\n\n\nItem5\n−0.008\n292.756\n150.723\n−230.309\n−379.190\n\n\nItem6\n0.748\n−2.417\n1.608\n2.543\n3.081\n\n\nItem7\n0.505\n−7.536\n2.673\n2.757\n8.119\n\n\nItem8\n0.335\n−4.144\n−1.048\n1.511\n5.650\n\n\nItem9\n0.198\n−13.543\n−12.768\n16.102\n25.336\n\n\nItem10\n0.752\n−0.423\n0.905\n2.114\n2.377",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#可視化",
    "href": "12_拡張ラッシュモデル.html#可視化",
    "title": "14  T検定と効果量",
    "section": "16.5 可視化",
    "text": "16.5 可視化\n\n16.5.1 カテゴリ特性曲線\n\n\nコードを表示\n# Plot category characteristic curves (first 6 items)\nplot(fit_grm, \n     type = 'trace',\n     which.items = 1:6,\n     facet_items = TRUE,\n     as.table = TRUE,\n     auto.key = list(points = FALSE, lines = TRUE, columns = 5, space = 'top', cex = .8),\n     theta_lim = c(-3, 3),\n     main = \"Category Characteristic Curves\")\n\n\n\n\n\n\n\n\n\n\n\n16.5.2 項目情報曲線\n\n\nコードを表示\n# Plot item information curves (first 6 items)\nplot(fit_grm,\n     type = 'infotrace',\n     which.items = 1:6,\n     facet_items = TRUE,\n     as.table = TRUE,\n     theta_lim = c(-3, 3),\n     main = \"Item Information Curves\")\n\n\n\n\n\n\n\n\n\n\n\n16.5.3 テスト情報関数\n\n\nコードを表示\n# Test information function and standard error\nplot(fit_grm,\n     type = 'infoSE',\n     theta_lim = c(-3, 3),\n     main = \"Test Information Function and Conditional Standard Error\")",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#理論的背景-1",
    "href": "12_拡張ラッシュモデル.html#理論的背景-1",
    "title": "14  T検定と効果量",
    "section": "17.1 理論的背景",
    "text": "17.1 理論的背景\n多次元IRT（MIRT）は、複数の潜在特性を同時に測定することを想定したモデルです。\n\n17.1.1 MIRTの数理モデル\n2パラメータ多次元IRTモデルの項目応答確率は以下のように表されます：\n\\[P(X_{ij} = 1|\\boldsymbol{\\theta}_j) = \\frac{1}{1 + \\exp(-\\mathbf{a}_i'\\boldsymbol{\\theta}_j - d_i)}\\]\nここで： - \\(\\boldsymbol{\\theta}_j\\)：受験者jの多次元能力ベクトル - \\(\\mathbf{a}_i\\)：項目iの弁別力ベクトル - \\(d_i\\)：項目iの切片パラメータ",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#データ生成と推定",
    "href": "12_拡張ラッシュモデル.html#データ生成と推定",
    "title": "14  T検定と効果量",
    "section": "17.2 データ生成と推定",
    "text": "17.2 データ生成と推定\n\n17.2.1 シミュレーションデータの生成\n\n\nコードを表示\n# Set seed for reproducibility\nset.seed(123)\n\n# Set item parameters (20 items, 2 dimensions)\na_matrix &lt;- matrix(c(\n  # Items loading strongly on Factor 1 (1-7)\n  1.5, 0.0,\n  1.2, 0.0,\n  1.0, 0.0,\n  0.9, 0.0,\n  1.1, 0.0,\n  1.3, 0.0,\n  0.8, 0.0,\n  # Items loading strongly on Factor 2 (8-14)\n  0.0, 1.3,\n  0.0, 1.0,\n  0.0, 1.2,\n  0.0, 0.8,\n  0.0, 1.1,\n  0.0, 0.9,\n  0.0, 1.4,\n  # Items loading on both factors (15-20)\n  0.7, 0.6,\n  0.8, 0.5,\n  0.6, 0.7,\n  0.9, 0.4,\n  0.5, 0.8,\n  0.6, 0.6\n), ncol = 2, byrow = TRUE)\n\n# Difficulty parameters\nd_params &lt;- rnorm(20, mean = 0, sd = 1)\n\n# Generate examinee abilities (correlated 2D normal distribution)\ntheta_cov &lt;- matrix(c(1, 0.3, 0.3, 1), 2, 2)\ntheta &lt;- mvtnorm::rmvnorm(500, mean = c(0, 0), sigma = theta_cov)\n\n# Generate data\nsim_data &lt;- simdata(a_matrix, d_params, N = 500, itemtype = '2PL', Theta = theta)\n\n# Display data size\ndata.frame(\n  Metric = c(\"Sample Size\", \"Number of Items\"),\n  Value = c(nrow(sim_data), ncol(sim_data))\n) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = md(\"**Simulated Data**\"),\n    subtitle = \"Multidimensional IRT Data Generation\"\n  )\n\n\n\n\n\n\n\n\nSimulated Data\n\n\nMultidimensional IRT Data Generation\n\n\nMetric\nValue\n\n\n\n\nSample Size\n500\n\n\nNumber of Items\n20\n\n\n\n\n\n\n\n\n\n17.2.2 モデルの指定と推定\n\n\nコードを表示\n# Model specification\nmirt_model &lt;- '\n  F1 = 1-7, 15-20\n  F2 = 8-20\n  COV = F1*F2\n'\n\n# Estimate multidimensional IRT model\nfit_mirt &lt;- mirt(sim_data, mirt_model, itemtype = '2PL', verbose = FALSE)\n\n# Model summary\nsummary(fit_mirt, suppress = TRUE)\n\n\n        F1 F2    h2\nItem_1        0.366\nItem_2        0.350\nItem_3        0.223\nItem_4        0.196\nItem_5        0.308\nItem_6        0.311\nItem_7        0.138\nItem_8        0.360\nItem_9        0.307\nItem_10       0.395\nItem_11       0.157\nItem_12       0.467\nItem_13       0.293\nItem_14       0.333\nItem_15       0.167\nItem_16       0.333\nItem_17       0.267\nItem_18       0.347\nItem_19       0.181\nItem_20       0.204\n\nSS loadings:  0 0 \nProportion Var:  0 0 \n\nFactor correlations: \n\n      F1 F2\nF1 1.000   \nF2 0.195  1",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#パラメータ推定結果",
    "href": "12_拡張ラッシュモデル.html#パラメータ推定結果",
    "title": "14  T検定と効果量",
    "section": "17.3 パラメータ推定結果",
    "text": "17.3 パラメータ推定結果\n\n17.3.1 因子負荷量\n\n\nコードを表示\n# Extract factor loadings\nmirt_params &lt;- coef(fit_mirt, simplify = TRUE)\nloadings_data &lt;- mirt_params$items[, 1:2]\n\n# Display factor loadings\nloadings_df &lt;- data.frame(\n  Item = paste0(\"Item\", 1:20),\n  F1 = round(loadings_data[,1], 3),\n  F2 = round(loadings_data[,2], 3),\n  Primary_Factor = case_when(\n    loadings_data[,1] &gt; 0.5 & loadings_data[,2] &lt; 0.3 ~ \"F1\",\n    loadings_data[,2] &gt; 0.5 & loadings_data[,1] &lt; 0.3 ~ \"F2\",\n    loadings_data[,1] &gt; 0.3 & loadings_data[,2] &gt; 0.3 ~ \"Both\",\n    TRUE ~ \"None\"\n  )\n)\n\nloadings_df |&gt;\n  gt() |&gt;\n  tab_header(\n    title = md(\"**Multidimensional IRT Factor Loadings**\"),\n    subtitle = \"Two-Factor Model Estimates\"\n  ) |&gt;\n  tab_spanner(\n    label = \"Factor Loadings\",\n    columns = c(F1, F2)\n  ) |&gt;\n  data_color(\n    columns = c(F1, F2),\n    colors = scales::col_numeric(\n      palette = c(\"white\", \"darkred\"),\n      domain = c(0, 2)\n    )\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#FFE4B5\"),\n    locations = cells_body(\n      columns = Primary_Factor,\n      rows = Primary_Factor == \"F1\"\n    )\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#B0E0E6\"),\n    locations = cells_body(\n      columns = Primary_Factor,\n      rows = Primary_Factor == \"F2\"\n    )\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#DDA0DD\"),\n    locations = cells_body(\n      columns = Primary_Factor,\n      rows = Primary_Factor == \"Both\"\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultidimensional IRT Factor Loadings\n\n\nTwo-Factor Model Estimates\n\n\nItem\n\nFactor Loadings\n\nPrimary_Factor\n\n\nF1\nF2\n\n\n\n\nItem1\n1.294\n0.000\nF1\n\n\nItem2\n1.249\n0.000\nF1\n\n\nItem3\n0.913\n0.000\nF1\n\n\nItem4\n0.840\n0.000\nF1\n\n\nItem5\n1.137\n0.000\nF1\n\n\nItem6\n1.144\n0.000\nF1\n\n\nItem7\n0.681\n0.000\nF1\n\n\nItem8\n0.000\n1.277\nF2\n\n\nItem9\n0.000\n1.133\nF2\n\n\nItem10\n0.000\n1.376\nF2\n\n\nItem11\n0.000\n0.735\nF2\n\n\nItem12\n0.000\n1.593\nF2\n\n\nItem13\n0.000\n1.096\nF2\n\n\nItem14\n0.000\n1.202\nF2\n\n\nItem15\n0.626\n0.434\nBoth\n\n\nItem16\n0.996\n0.674\nBoth\n\n\nItem17\n0.491\n0.904\nBoth\n\n\nItem18\n0.853\n0.900\nBoth\n\n\nItem19\n0.360\n0.715\nBoth\n\n\nItem20\n0.555\n0.658\nBoth\n\n\n\n\n\n\n\n\n\n17.3.2 因子間相関と適合度\n\n\nコードを表示\n# Extract factor variance-covariance matrix\ncov_matrix &lt;- mirt_params$cov\n\n# Convert to correlation matrix\nif(is.matrix(cov_matrix) && nrow(cov_matrix) == ncol(cov_matrix)) {\n  cor_matrix &lt;- cov2cor(cov_matrix)\n  \n  # Display factor correlations\n  cor_df &lt;- as.data.frame(cor_matrix)\n  rownames(cor_df) &lt;- colnames(cor_df) &lt;- c(\"Factor 1\", \"Factor 2\")\n  \n  cor_df |&gt;\n    gt(rownames_to_stub = TRUE) |&gt;\n    tab_header(\n      title = md(\"**Factor Correlation Matrix**\"),\n      subtitle = \"Estimated Latent Factor Correlations\"\n    ) |&gt;\n    fmt_number(columns = everything(), decimals = 3) |&gt;\n    data_color(\n      columns = everything(),\n      colors = scales::col_numeric(\n        palette = c(\"white\", \"darkblue\"),\n        domain = c(-1, 1)\n      )\n    )\n}\n\n\n\n\n\n\n\n\nFactor Correlation Matrix\n\n\nEstimated Latent Factor Correlations\n\n\n\nFactor 1\nFactor 2\n\n\n\n\nFactor 1\n1.000\n0.195\n\n\nFactor 2\n0.195\n1.000\n\n\n\n\n\n\n\nコードを表示\n# Model fit\nmirt_fit &lt;- M2(fit_mirt, type = \"C2\", calcNULL = FALSE)\n\ndata.frame(\n  Index = c(\"M2 Statistic\", \"RMSEA\", \"SRMSR\", \"CFI\", \"TLI\"),\n  Value = c(\n    round(mirt_fit$M2, 2),\n    round(mirt_fit$RMSEA, 3),\n    round(mirt_fit$SRMSR, 3),\n    round(mirt_fit$CFI, 3),\n    round(mirt_fit$TLI, 3)\n  )\n) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = md(\"**Multidimensional IRT Model Fit**\"),\n    subtitle = \"Two-Factor Model Fit Indices\"\n  )\n\n\n\n\n\n\n\n\nMultidimensional IRT Model Fit\n\n\nTwo-Factor Model Fit Indices\n\n\nIndex\nValue\n\n\n\n\nM2 Statistic\n137.700\n\n\nRMSEA\n0.000\n\n\nSRMSR\n0.034\n\n\nCFI\n1.000\n\n\nTLI\n1.019",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#可視化-1",
    "href": "12_拡張ラッシュモデル.html#可視化-1",
    "title": "14  T検定と効果量",
    "section": "17.4 可視化",
    "text": "17.4 可視化\n\n17.4.1 2次元項目プロット\n\n\nコードを表示\n# Item vector plot\nitemplot(fit_mirt, item = 1, type = 'score')\n\n\n\n\n\n\n\n\n\nコードを表示\n# 2D plot for all items\nplot(fit_mirt, type = 'trace', which.items = 1:6,\n     main = \"Multidimensional IRT - Category Characteristic Surfaces\")",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#理論的背景-2",
    "href": "12_拡張ラッシュモデル.html#理論的背景-2",
    "title": "14  T検定と効果量",
    "section": "18.1 理論的背景",
    "text": "18.1 理論的背景\nモッケン尺度分析（MSA）は、ノンパラメトリックIRTの一種で、以下の2つの主要モデルがあります：\n\n単調同質性モデル（MHM）\n\n一次元性\n局所独立性\n単調性\n\n二重単調性モデル（DMM）\n\nMHMの全仮定\n不変項目順序（IIO）",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#データ準備",
    "href": "12_拡張ラッシュモデル.html#データ準備",
    "title": "14  T検定と効果量",
    "section": "18.2 データ準備",
    "text": "18.2 データ準備\n\n\nコードを表示\n# Binarize SAT12 data (median split)\nSAT12_binary &lt;- ifelse(SAT12 &lt;= 3, 0, 1)\n\n# Data overview\ndata.frame(\n  Category = c(\"Low Rating (0)\", \"High Rating (1)\"),\n  Proportion = c(\n    mean(SAT12_binary == 0),\n    mean(SAT12_binary == 1)\n  )\n) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = md(\"**Binarized Data Overview**\"),\n    subtitle = \"Proportion of Response Categories\"\n  ) |&gt;\n  fmt_percent(columns = Proportion, decimals = 1)\n\n\n\n\n\n\n\n\nBinarized Data Overview\n\n\nProportion of Response Categories\n\n\nCategory\nProportion\n\n\n\n\nLow Rating (0)\n65.1%\n\n\nHigh Rating (1)\n34.9%",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#スケーラビリティ分析",
    "href": "12_拡張ラッシュモデル.html#スケーラビリティ分析",
    "title": "14  T検定と効果量",
    "section": "18.3 スケーラビリティ分析",
    "text": "18.3 スケーラビリティ分析\n\n18.3.1 Loevinger’s H係数\n\n\nコードを表示\n# Calculate scalability coefficients\ncoefH_result &lt;- coefH(SAT12_binary)\n\n\n$Hij\n        Item.1   se      Item.2   se      Item.3   se      Item.4   se     \nItem.1                    -0.559  (0.116)  -0.088  (0.056)  -0.006  (0.048)\nItem.2   -0.559  (0.116)                    0.186  (0.075)  -0.087  (0.094)\nItem.3   -0.088  (0.056)   0.186  (0.075)                   -0.120  (0.048)\nItem.4   -0.006  (0.048)  -0.087  (0.094)  -0.120  (0.048)                 \nItem.5    0.089  (0.061)  -0.495  (0.163)   0.083  (0.081)   0.008  (0.067)\nItem.6   -0.079  (0.051)  -0.051  (0.148)  -0.102  (0.075)  -0.080  (0.062)\nItem.7    0.065  (0.048)  -0.213  (0.125)  -0.105  (0.062)   0.040  (0.054)\nItem.8   -0.014  (0.056)  -0.080  (0.079)   0.036  (0.041)   0.013  (0.049)\nItem.9    0.170  (0.128)  -1.198  (0.318)  -0.233  (0.144)   0.157  (0.142)\nItem.10   0.041  (0.048)  -0.053  (0.126)  -0.091  (0.064)   0.016  (0.055)\nItem.11   0.558  (0.361)  -0.099  (0.895)  -0.644  (0.054)  -0.446  (0.039)\nItem.12   0.073  (0.092)   0.029  (0.048)  -0.019  (0.066)   0.040  (0.079)\nItem.13   0.106  (0.058)  -0.423  (0.152)  -0.142  (0.072)   0.026  (0.064)\nItem.14   0.094  (0.062)  -0.474  (0.163)  -0.025  (0.080)   0.133  (0.071)\nItem.15  -0.251  (0.175)   0.145  (0.066)   0.239  (0.112)   0.068  (0.140)\nItem.16   0.093  (0.060)  -0.054  (0.074)  -0.085  (0.042)   0.035  (0.051)\nItem.17   0.131  (0.440)   0.282  (0.190)   0.453  (0.278)  -0.390  (0.423)\nItem.18  -0.010  (0.071)   0.184  (0.062)   0.271  (0.051)  -0.067  (0.061)\nItem.19   0.060  (0.120)  -0.648  (0.330)   0.041  (0.162)   0.157  (0.142)\nItem.20  -0.313  (0.209)   0.232  (0.080)   0.461  (0.119)  -0.096  (0.170)\nItem.21   0.170  (0.158)  -1.266  (0.385)  -0.130  (0.188)   0.006  (0.165)\nItem.22  -0.048  (0.122)  -0.735  (0.372)  -0.298  (0.153)   0.011  (0.152)\nItem.23  -0.136  (0.057)   0.118  (0.074)   0.049  (0.042)  -0.054  (0.050)\nItem.24   0.126  (0.094)  -0.287  (0.241)  -0.083  (0.118)  -0.128  (0.091)\nItem.25   0.033  (0.046)  -0.406  (0.125)  -0.134  (0.061)   0.003  (0.052)\nItem.26  -0.343  (0.122)   0.161  (0.046)   0.185  (0.080)   0.043  (0.098)\nItem.27   0.071  (0.134)   0.011  (0.332)  -0.068  (0.173)  -0.084  (0.138)\nItem.28   0.152  (0.057)  -0.307  (0.082)  -0.108  (0.042)   0.139  (0.049)\nItem.29   0.018  (0.054)  -0.339  (0.150)   0.058  (0.076)   0.021  (0.063)\nItem.30   0.097  (0.092)   0.054  (0.048)   0.118  (0.066)   0.042  (0.080)\nItem.31  -0.710  (0.234)   0.125  (0.082)   0.362  (0.132)  -0.014  (0.178)\nItem.32   0.038  (0.042)  -0.106  (0.110)   0.095  (0.058)   0.049  (0.048)\n        Item.5   se      Item.6   se      Item.7   se      Item.8   se     \nItem.1    0.089  (0.061)  -0.079  (0.051)   0.065  (0.048)  -0.014  (0.056)\nItem.2   -0.495  (0.163)  -0.051  (0.148)  -0.213  (0.125)  -0.080  (0.079)\nItem.3    0.083  (0.081)  -0.102  (0.075)  -0.105  (0.062)   0.036  (0.041)\nItem.4    0.008  (0.067)  -0.080  (0.062)   0.040  (0.054)   0.013  (0.049)\nItem.5                     0.054  (0.046)   0.104  (0.057)  -0.119  (0.076)\nItem.6    0.054  (0.046)                    0.126  (0.055)  -0.039  (0.076)\nItem.7    0.104  (0.057)   0.126  (0.055)                   -0.017  (0.063)\nItem.8   -0.119  (0.076)  -0.039  (0.076)  -0.017  (0.063)                 \nItem.9    0.222  (0.110)  -0.031  (0.078)   0.211  (0.122)  -0.155  (0.149)\nItem.10   0.043  (0.052)   0.082  (0.052)   0.013  (0.043)  -0.082  (0.064)\nItem.11   0.222  (0.317)  -0.179  (0.020)   0.158  (0.343)  -0.630  (0.053)\nItem.12   0.218  (0.123)  -0.094  (0.127)   0.096  (0.102)  -0.079  (0.067)\nItem.13   0.033  (0.046)   0.086  (0.047)   0.202  (0.057)   0.039  (0.075)\nItem.14   0.162  (0.052)  -0.026  (0.040)   0.034  (0.054)   0.022  (0.080)\nItem.15  -0.039  (0.235)  -0.613  (0.258)  -0.430  (0.203)   0.202  (0.114)\nItem.16   0.012  (0.084)   0.066  (0.082)  -0.020  (0.066)   0.040  (0.043)\nItem.17  -0.495  (0.753)  -0.884  (0.782)  -0.371  (0.571)   0.261  (0.309)\nItem.18  -0.113  (0.099)  -0.051  (0.096)  -0.100  (0.079)  -0.022  (0.051)\nItem.19   0.173  (0.106)  -0.031  (0.078)   0.211  (0.122)  -0.087  (0.154)\nItem.20  -0.670  (0.321)   0.071  (0.257)  -0.893  (0.256)   0.162  (0.136)\nItem.21   0.125  (0.124)  -0.031  (0.096)  -0.105  (0.104)   0.083  (0.199)\nItem.22   0.017  (0.096)   0.318  (0.131)   0.069  (0.125)  -0.201  (0.163)\nItem.23  -0.075  (0.081)  -0.071  (0.078)  -0.076  (0.065)   0.065  (0.043)\nItem.24   0.231  (0.083)   0.051  (0.070)   0.199  (0.091)  -0.074  (0.117)\nItem.25   0.156  (0.059)   0.104  (0.055)  -0.009  (0.041)   0.052  (0.063)\nItem.26  -0.429  (0.174)  -0.390  (0.168)  -0.301  (0.136)   0.034  (0.083)\nItem.27   0.125  (0.111)   0.057  (0.103)   0.242  (0.136)  -0.060  (0.171)\nItem.28   0.214  (0.080)   0.010  (0.076)   0.035  (0.063)   0.081  (0.042)\nItem.29   0.073  (0.049)   0.019  (0.043)   0.053  (0.051)   0.168  (0.076)\nItem.30  -0.110  (0.133)  -0.049  (0.128)   0.127  (0.102)   0.071  (0.067)\nItem.31  -0.417  (0.325)   0.176  (0.260)  -0.200  (0.244)  -0.051  (0.150)\nItem.32   0.072  (0.061)   0.094  (0.059)   0.021  (0.046)  -0.040  (0.056)\n        Item.9   se      Item.10  se      Item.11   se       Item.12  se     \nItem.1    0.170  (0.128)   0.041  (0.048)    0.558   (0.361)   0.073  (0.092)\nItem.2   -1.198  (0.318)  -0.053  (0.126)   -0.099   (0.895)   0.029  (0.048)\nItem.3   -0.233  (0.144)  -0.091  (0.064)   -0.644   (0.054)  -0.019  (0.066)\nItem.4    0.157  (0.142)   0.016  (0.055)   -0.446   (0.039)   0.040  (0.079)\nItem.5    0.222  (0.110)   0.043  (0.052)    0.222   (0.317)   0.218  (0.123)\nItem.6   -0.031  (0.078)   0.082  (0.052)   -0.179   (0.020)  -0.094  (0.127)\nItem.7    0.211  (0.122)   0.013  (0.043)    0.158   (0.343)   0.096  (0.102)\nItem.8   -0.155  (0.149)  -0.082  (0.064)   -0.630   (0.053)  -0.079  (0.067)\nItem.9                    -0.040  (0.093)    0.306   (0.283)  -0.009  (0.260)\nItem.10  -0.040  (0.093)                     0.168   (0.339)  -0.153  (0.109)\nItem.11   0.306  (0.283)   0.168  (0.339)                     -0.794  (0.732)\nItem.12  -0.009  (0.260)  -0.153  (0.109)   -0.794   (0.732)                 \nItem.13   0.208  (0.112)  -0.011  (0.046)    0.208   (0.322)   0.235  (0.116)\nItem.14  -0.019  (0.077)   0.017  (0.051)    0.223   (0.316)   0.050  (0.129)\nItem.15  -0.596  (0.548)  -0.395  (0.209)   -5.383   (0.605)  -0.084  (0.071)\nItem.16  -0.133  (0.161)   0.157  (0.070)    0.433   (0.462)   0.031  (0.063)\nItem.17  -2.571  (2.251)  -0.080  (0.546)  -13.286  (11.439)   0.091  (0.208)\nItem.18  -0.329  (0.189)  -0.039  (0.082)   -0.329   (0.542)  -0.035  (0.053)\nItem.19   0.089  (0.067)   0.064  (0.108)    0.306   (0.283)  -0.009  (0.260)\nItem.20  -1.465  (0.754)   0.219  (0.206)   -1.817   (2.289)  -0.031  (0.085)\nItem.21  -0.042  (0.009)  -0.169  (0.077)   -0.027   (0.007)  -0.345  (0.332)\nItem.22   0.068  (0.072)  -0.116  (0.088)    0.312   (0.280)   0.150  (0.283)\nItem.23  -0.120  (0.159)  -0.017  (0.068)   -0.120   (0.456)   0.103  (0.064)\nItem.24   0.106  (0.079)   0.057  (0.080)   -0.073   (0.012)  -0.050  (0.198)\nItem.25   0.098  (0.115)   0.069  (0.046)    0.151   (0.346)  -0.293  (0.105)\nItem.26  -0.355  (0.349)  -0.215  (0.138)   -2.614   (0.239)   0.012  (0.051)\nItem.27   0.063  (0.068)   0.127  (0.125)   -0.034   (0.008)   0.193  (0.272)\nItem.28   0.263  (0.160)   0.108  (0.065)    1.000   (0.000)  -0.031  (0.068)\nItem.29   0.058  (0.096)  -0.001  (0.046)   -0.190   (0.021)   0.075  (0.120)\nItem.30   0.091  (0.257)  -0.215  (0.111)   -0.818   (0.742)   0.125  (0.042)\nItem.31  -0.953  (0.750)  -0.418  (0.264)   -2.125   (2.538)  -0.020  (0.090)\nItem.32   0.169  (0.129)   0.083  (0.050)    0.113   (0.361)   0.079  (0.091)\n        Item.13  se      Item.14  se      Item.15  se      Item.16  se     \nItem.1    0.106  (0.058)   0.094  (0.062)  -0.251  (0.175)   0.093  (0.060)\nItem.2   -0.423  (0.152)  -0.474  (0.163)   0.145  (0.066)  -0.054  (0.074)\nItem.3   -0.142  (0.072)  -0.025  (0.080)   0.239  (0.112)  -0.085  (0.042)\nItem.4    0.026  (0.064)   0.133  (0.071)   0.068  (0.140)   0.035  (0.051)\nItem.5    0.033  (0.046)   0.162  (0.052)  -0.039  (0.235)   0.012  (0.084)\nItem.6    0.086  (0.047)  -0.026  (0.040)  -0.613  (0.258)   0.066  (0.082)\nItem.7    0.202  (0.057)   0.034  (0.054)  -0.430  (0.203)  -0.020  (0.066)\nItem.8    0.039  (0.075)   0.022  (0.080)   0.202  (0.114)   0.040  (0.043)\nItem.9    0.208  (0.112)  -0.019  (0.077)  -0.596  (0.548)  -0.133  (0.161)\nItem.10  -0.011  (0.046)   0.017  (0.051)  -0.395  (0.209)   0.157  (0.070)\nItem.11   0.208  (0.322)   0.223  (0.316)  -5.383  (0.605)   0.433  (0.462)\nItem.12   0.235  (0.116)   0.050  (0.129)  -0.084  (0.071)   0.031  (0.063)\nItem.13                   -0.076  (0.037)  -0.814  (0.258)   0.052  (0.079)\nItem.14  -0.076  (0.037)                   -0.202  (0.248)   0.060  (0.085)\nItem.15  -0.814  (0.258)  -0.202  (0.248)                   -0.060  (0.114)\nItem.16   0.052  (0.079)   0.060  (0.085)  -0.060  (0.114)                 \nItem.17  -1.256  (0.794)  -1.017  (0.836)   0.407  (0.157)  -0.041  (0.317)\nItem.18  -0.091  (0.094)  -0.102  (0.100)   0.231  (0.093)   0.032  (0.049)\nItem.19   0.109  (0.102)   0.175  (0.105)  -0.862  (0.574)   0.150  (0.170)\nItem.20  -0.690  (0.302)  -0.292  (0.300)   0.081  (0.054)  -0.061  (0.134)\nItem.21   0.035  (0.114)  -0.092  (0.071)  -0.995  (0.724)  -0.062  (0.203)\nItem.22   0.500  (0.133)   0.142  (0.115)  -2.359  (0.723)   0.105  (0.192)\nItem.23  -0.132  (0.075)  -0.167  (0.079)   0.107  (0.113)  -0.049  (0.041)\nItem.24   0.131  (0.079)   0.005  (0.062)  -1.180  (0.447)   0.088  (0.128)\nItem.25   0.021  (0.050)   0.116  (0.058)  -0.237  (0.192)  -0.028  (0.065)\nItem.26  -0.636  (0.168)  -0.531  (0.177)   0.147  (0.063)  -0.112  (0.080)\nItem.27   0.347  (0.130)  -0.165  (0.019)  -0.596  (0.603)   0.150  (0.187)\nItem.28   0.221  (0.076)   0.186  (0.081)  -0.490  (0.126)   0.049  (0.044)\nItem.29   0.035  (0.043)  -0.050  (0.040)   0.136  (0.207)   0.062  (0.079)\nItem.30  -0.062  (0.125)  -0.251  (0.136)   0.177  (0.075)   0.039  (0.064)\nItem.31  -0.875  (0.330)  -0.324  (0.321)   0.203  (0.065)  -0.063  (0.142)\nItem.32   0.076  (0.057)  -0.002  (0.058)  -0.242  (0.174)   0.110  (0.060)\n        Item.17   se       Item.18  se      Item.19  se      Item.20  se     \nItem.1     0.131   (0.440)  -0.010  (0.071)   0.060  (0.120)  -0.313  (0.209)\nItem.2     0.282   (0.190)   0.184  (0.062)  -0.648  (0.330)   0.232  (0.080)\nItem.3     0.453   (0.278)   0.271  (0.051)   0.041  (0.162)   0.461  (0.119)\nItem.4    -0.390   (0.423)  -0.067  (0.061)   0.157  (0.142)  -0.096  (0.170)\nItem.5    -0.495   (0.753)  -0.113  (0.099)   0.173  (0.106)  -0.670  (0.321)\nItem.6    -0.884   (0.782)  -0.051  (0.096)  -0.031  (0.078)   0.071  (0.257)\nItem.7    -0.371   (0.571)  -0.100  (0.079)   0.211  (0.122)  -0.893  (0.256)\nItem.8     0.261   (0.309)  -0.022  (0.051)  -0.087  (0.154)   0.162  (0.136)\nItem.9    -2.571   (2.251)  -0.329  (0.189)   0.089  (0.067)  -1.465  (0.754)\nItem.10   -0.080   (0.546)  -0.039  (0.082)   0.064  (0.108)   0.219  (0.206)\nItem.11  -13.286  (11.439)  -0.329  (0.542)   0.306  (0.283)  -1.817  (2.289)\nItem.12    0.091   (0.208)  -0.035  (0.053)  -0.009  (0.260)  -0.031  (0.085)\nItem.13   -1.256   (0.794)  -0.091  (0.094)   0.109  (0.102)  -0.690  (0.302)\nItem.14   -1.017   (0.836)  -0.102  (0.100)   0.175  (0.105)  -0.292  (0.300)\nItem.15    0.407   (0.157)   0.231  (0.093)  -0.862  (0.574)   0.081  (0.054)\nItem.16   -0.041   (0.317)   0.032  (0.049)   0.150  (0.170)  -0.061  (0.134)\nItem.17                      0.140  (0.262)  -6.143  (2.849)   0.190  (0.135)\nItem.18    0.140   (0.262)                   -0.246  (0.194)   0.378  (0.105)\nItem.19   -6.143   (2.849)  -0.246  (0.194)                   -2.873  (0.834)\nItem.20    0.190   (0.135)   0.378  (0.105)  -2.873  (0.834)                 \nItem.21    1.000   (0.000)  -0.246  (0.239)  -0.042  (0.009)   0.472  (0.508)\nItem.22  -10.278   (3.772)  -0.574  (0.188)   0.232  (0.103)  -2.113  (0.907)\nItem.23    0.471   (0.269)   0.040  (0.049)  -0.120  (0.159)  -0.078  (0.136)\nItem.24   -3.181   (1.706)  -0.215  (0.147)   0.016  (0.059)  -0.237  (0.447)\nItem.25    0.336   (0.431)  -0.252  (0.077)   0.045  (0.110)  -0.245  (0.227)\nItem.26    0.111   (0.175)   0.287  (0.065)  -0.807  (0.361)   0.241  (0.076)\nItem.27   -3.286   (2.681)  -0.495  (0.193)   0.010  (0.050)  -0.690  (0.736)\nItem.28   -0.322   (0.349)  -0.142  (0.052)   0.263  (0.160)  -0.303  (0.147)\nItem.29    0.107   (0.578)  -0.142  (0.093)   0.058  (0.096)  -0.496  (0.290)\nItem.30    0.098   (0.206)   0.106  (0.054)  -0.250  (0.272)  -0.045  (0.083)\nItem.31    0.120   (0.121)   0.404  (0.110)  -0.953  (0.750)   0.185  (0.059)\nItem.32   -0.151   (0.480)   0.104  (0.071)   0.002  (0.115)  -0.361  (0.210)\n        Item.21  se      Item.22   se      Item.23  se      Item.24  se     \nItem.1    0.170  (0.158)   -0.048  (0.122)  -0.136  (0.057)   0.126  (0.094)\nItem.2   -1.266  (0.385)   -0.735  (0.372)   0.118  (0.074)  -0.287  (0.241)\nItem.3   -0.130  (0.188)   -0.298  (0.153)   0.049  (0.042)  -0.083  (0.118)\nItem.4    0.006  (0.165)    0.011  (0.152)  -0.054  (0.050)  -0.128  (0.091)\nItem.5    0.125  (0.124)    0.017  (0.096)  -0.075  (0.081)   0.231  (0.083)\nItem.6   -0.031  (0.096)    0.318  (0.131)  -0.071  (0.078)   0.051  (0.070)\nItem.7   -0.105  (0.104)    0.069  (0.125)  -0.076  (0.065)   0.199  (0.091)\nItem.8    0.083  (0.199)   -0.201  (0.163)   0.065  (0.043)  -0.074  (0.117)\nItem.9   -0.042  (0.009)    0.068  (0.072)  -0.120  (0.159)   0.106  (0.079)\nItem.10  -0.169  (0.077)   -0.116  (0.088)  -0.017  (0.068)   0.057  (0.080)\nItem.11  -0.027  (0.007)    0.312  (0.280)  -0.120  (0.456)  -0.073  (0.012)\nItem.12  -0.345  (0.332)    0.150  (0.283)   0.103  (0.064)  -0.050  (0.198)\nItem.13   0.035  (0.114)    0.500  (0.133)  -0.132  (0.075)   0.131  (0.079)\nItem.14  -0.092  (0.071)    0.142  (0.115)  -0.167  (0.079)   0.005  (0.062)\nItem.15  -0.995  (0.724)   -2.359  (0.723)   0.107  (0.113)  -1.180  (0.447)\nItem.16  -0.062  (0.203)    0.105  (0.192)  -0.049  (0.041)   0.088  (0.128)\nItem.17   1.000  (0.000)  -10.278  (3.772)   0.471  (0.269)  -3.181  (1.706)\nItem.18  -0.246  (0.239)   -0.574  (0.188)   0.040  (0.049)  -0.215  (0.147)\nItem.19  -0.042  (0.009)    0.232  (0.103)  -0.120  (0.159)   0.016  (0.059)\nItem.20   0.472  (0.508)   -2.113  (0.907)  -0.078  (0.136)  -0.237  (0.447)\nItem.21                    -0.033  (0.008)  -0.155  (0.193)  -0.006  (0.064)\nItem.22  -0.033  (0.008)                    -0.150  (0.177)   0.040  (0.074)\nItem.23  -0.155  (0.193)   -0.150  (0.177)                   -0.271  (0.111)\nItem.24  -0.006  (0.064)    0.040  (0.074)  -0.271  (0.111)                 \nItem.25   0.045  (0.136)    0.061  (0.126)  -0.016  (0.064)   0.006  (0.079)\nItem.26   0.548  (0.297)   -1.473  (0.390)   0.122  (0.079)  -0.499  (0.267)\nItem.27  -0.034  (0.008)   -0.034  (0.008)   0.244  (0.184)   0.141  (0.094)\nItem.28  -0.106  (0.184)    0.238  (0.181)  -0.066  (0.043)   0.098  (0.120)\nItem.29   0.033  (0.114)    0.185  (0.124)  -0.033  (0.077)   0.013  (0.067)\nItem.30  -0.193  (0.333)   -0.148  (0.304)   0.080  (0.065)  -0.197  (0.204)\nItem.31   1.000  (0.000)   -1.961  (0.967)   0.267  (0.135)  -1.287  (0.587)\nItem.32  -0.081  (0.129)    0.160  (0.145)   0.019  (0.059)  -0.103  (0.077)\n        Item.25  se      Item.26  se      Item.27  se      Item.28  se     \nItem.1    0.033  (0.046)  -0.343  (0.122)   0.071  (0.134)   0.152  (0.057)\nItem.2   -0.406  (0.125)   0.161  (0.046)   0.011  (0.332)  -0.307  (0.082)\nItem.3   -0.134  (0.061)   0.185  (0.080)  -0.068  (0.173)  -0.108  (0.042)\nItem.4    0.003  (0.052)   0.043  (0.098)  -0.084  (0.138)   0.139  (0.049)\nItem.5    0.156  (0.059)  -0.429  (0.174)   0.125  (0.111)   0.214  (0.080)\nItem.6    0.104  (0.055)  -0.390  (0.168)   0.057  (0.103)   0.010  (0.076)\nItem.7   -0.009  (0.041)  -0.301  (0.136)   0.242  (0.136)   0.035  (0.063)\nItem.8    0.052  (0.063)   0.034  (0.083)  -0.060  (0.171)   0.081  (0.042)\nItem.9    0.098  (0.115)  -0.355  (0.349)   0.063  (0.068)   0.263  (0.160)\nItem.10   0.069  (0.046)  -0.215  (0.138)   0.127  (0.125)   0.108  (0.065)\nItem.11   0.151  (0.346)  -2.614  (0.239)  -0.034  (0.008)   1.000  (0.000)\nItem.12  -0.293  (0.105)   0.012  (0.051)   0.193  (0.272)  -0.031  (0.068)\nItem.13   0.021  (0.050)  -0.636  (0.168)   0.347  (0.130)   0.221  (0.076)\nItem.14   0.116  (0.058)  -0.531  (0.177)  -0.165  (0.019)   0.186  (0.081)\nItem.15  -0.237  (0.192)   0.147  (0.063)  -0.596  (0.603)  -0.490  (0.126)\nItem.16  -0.028  (0.065)  -0.112  (0.080)   0.150  (0.187)   0.049  (0.044)\nItem.17   0.336  (0.431)   0.111  (0.175)  -3.286  (2.681)  -0.322  (0.349)\nItem.18  -0.252  (0.077)   0.287  (0.065)  -0.495  (0.193)  -0.142  (0.052)\nItem.19   0.045  (0.110)  -0.807  (0.361)   0.010  (0.050)   0.263  (0.160)\nItem.20  -0.245  (0.227)   0.241  (0.076)  -0.690  (0.736)  -0.303  (0.147)\nItem.21   0.045  (0.136)   0.548  (0.297)  -0.034  (0.008)  -0.106  (0.184)\nItem.22   0.061  (0.126)  -1.473  (0.390)  -0.034  (0.008)   0.238  (0.181)\nItem.23  -0.016  (0.064)   0.122  (0.079)   0.244  (0.184)  -0.066  (0.043)\nItem.24   0.006  (0.079)  -0.499  (0.267)   0.141  (0.094)   0.098  (0.120)\nItem.25                   -0.317  (0.133)   0.045  (0.121)   0.214  (0.063)\nItem.26  -0.317  (0.133)                   -0.627  (0.395)  -0.194  (0.086)\nItem.27   0.045  (0.121)  -0.627  (0.395)                   -0.206  (0.154)\nItem.28   0.214  (0.063)  -0.194  (0.086)  -0.206  (0.154)                 \nItem.29  -0.008  (0.049)  -0.205  (0.158)  -0.071  (0.079)   0.045  (0.074)\nItem.30   0.027  (0.102)  -0.018  (0.050)  -0.364  (0.300)  -0.105  (0.069)\nItem.31  -0.672  (0.259)   0.352  (0.082)  -0.875  (0.815)  -0.652  (0.155)\nItem.32   0.072  (0.047)  -0.067  (0.116)   0.002  (0.127)   0.061  (0.056)\n        Item.29  se      Item.30  se      Item.31  se      Item.32  se     \nItem.1    0.018  (0.054)   0.097  (0.092)  -0.710  (0.234)   0.038  (0.042)\nItem.2   -0.339  (0.150)   0.054  (0.048)   0.125  (0.082)  -0.106  (0.110)\nItem.3    0.058  (0.076)   0.118  (0.066)   0.362  (0.132)   0.095  (0.058)\nItem.4    0.021  (0.063)   0.042  (0.080)  -0.014  (0.178)   0.049  (0.048)\nItem.5    0.073  (0.049)  -0.110  (0.133)  -0.417  (0.325)   0.072  (0.061)\nItem.6    0.019  (0.043)  -0.049  (0.128)   0.176  (0.260)   0.094  (0.059)\nItem.7    0.053  (0.051)   0.127  (0.102)  -0.200  (0.244)   0.021  (0.046)\nItem.8    0.168  (0.076)   0.071  (0.067)  -0.051  (0.150)  -0.040  (0.056)\nItem.9    0.058  (0.096)   0.091  (0.257)  -0.953  (0.750)   0.169  (0.129)\nItem.10  -0.001  (0.046)  -0.215  (0.111)  -0.418  (0.264)   0.083  (0.050)\nItem.11  -0.190  (0.021)  -0.818  (0.742)  -2.125  (2.538)   0.113  (0.361)\nItem.12   0.075  (0.120)   0.125  (0.042)  -0.020  (0.090)   0.079  (0.091)\nItem.13   0.035  (0.043)  -0.062  (0.125)  -0.875  (0.330)   0.076  (0.057)\nItem.14  -0.050  (0.040)  -0.251  (0.136)  -0.324  (0.321)  -0.002  (0.058)\nItem.15   0.136  (0.207)   0.177  (0.075)   0.203  (0.065)  -0.242  (0.174)\nItem.16   0.062  (0.079)   0.039  (0.064)  -0.063  (0.142)   0.110  (0.060)\nItem.17   0.107  (0.578)   0.098  (0.206)   0.120  (0.121)  -0.151  (0.480)\nItem.18  -0.142  (0.093)   0.106  (0.054)   0.404  (0.110)   0.104  (0.071)\nItem.19   0.058  (0.096)  -0.250  (0.272)  -0.953  (0.750)   0.002  (0.115)\nItem.20  -0.496  (0.290)  -0.045  (0.083)   0.185  (0.059)  -0.361  (0.210)\nItem.21   0.033  (0.114)  -0.193  (0.333)   1.000  (0.000)  -0.081  (0.129)\nItem.22   0.185  (0.124)  -0.148  (0.304)  -1.961  (0.967)   0.160  (0.145)\nItem.23  -0.033  (0.077)   0.080  (0.065)   0.267  (0.135)   0.019  (0.059)\nItem.24   0.013  (0.067)  -0.197  (0.204)  -1.287  (0.587)  -0.103  (0.077)\nItem.25  -0.008  (0.049)   0.027  (0.102)  -0.672  (0.259)   0.072  (0.047)\nItem.26  -0.205  (0.158)  -0.018  (0.050)   0.352  (0.082)  -0.067  (0.116)\nItem.27  -0.071  (0.079)  -0.364  (0.300)  -0.875  (0.815)   0.002  (0.127)\nItem.28   0.045  (0.074)  -0.105  (0.069)  -0.652  (0.155)   0.061  (0.056)\nItem.29                   -0.136  (0.125)   0.121  (0.258)   0.016  (0.054)\nItem.30  -0.136  (0.125)                    0.087  (0.092)  -0.043  (0.094)\nItem.31   0.121  (0.258)   0.087  (0.092)                   -0.258  (0.219)\nItem.32   0.016  (0.054)  -0.043  (0.094)  -0.258  (0.219)                 \n\n$Hi\n        Item H  se     \nItem.1   -0.005 (0.014)\nItem.2   -0.051 (0.018)\nItem.3    0.014 (0.013)\nItem.4    0.006 (0.013)\nItem.5    0.036 (0.016)\nItem.6    0.000 (0.014)\nItem.7    0.009 (0.014)\nItem.8    0.013 (0.013)\nItem.9   -0.000 (0.031)\nItem.10  -0.001 (0.014)\nItem.11  -0.076 (0.072)\nItem.12   0.014 (0.015)\nItem.13   0.005 (0.015)\nItem.14  -0.011 (0.016)\nItem.15  -0.026 (0.026)\nItem.16   0.017 (0.013)\nItem.17  -0.030 (0.062)\nItem.18   0.020 (0.014)\nItem.19  -0.005 (0.022)\nItem.20  -0.020 (0.028)\nItem.21  -0.050 (0.026)\nItem.22  -0.041 (0.023)\nItem.23  -0.005 (0.013)\nItem.24  -0.021 (0.023)\nItem.25  -0.011 (0.014)\nItem.26  -0.029 (0.019)\nItem.27  -0.005 (0.035)\nItem.28   0.001 (0.013)\nItem.29   0.006 (0.015)\nItem.30   0.024 (0.016)\nItem.31  -0.005 (0.027)\nItem.32   0.030 (0.014)\n\n$H\nScale H      se \n  0.002 (0.005) \n\n\nコードを表示\n# Extract overall H value\n# The coefH function returns values as character strings with SE in parentheses\n# H is a named character vector where the first element is the H value\nh_value &lt;- as.numeric(coefH_result$H[1])\n\n# Ensure h_value is numeric and valid\nif(is.na(h_value)) {\n  h_value &lt;- 0  # Default value if extraction fails\n}\n\n# Overall H coefficient\noverall_H &lt;- data.frame(\n  Statistic = \"Overall H Coefficient\",\n  Value = round(h_value, 3),\n  Interpretation = case_when(\n    h_value &gt;= 0.5 ~ \"Strong Scale\",\n    h_value &gt;= 0.4 ~ \"Medium Scale\",\n    h_value &gt;= 0.3 ~ \"Weak Scale\",\n    TRUE ~ \"Inappropriate as Scale\"\n  )\n)\n\noverall_H |&gt;\n  gt() |&gt;\n  tab_header(\n    title = md(\"**Overall Scalability**\"),\n    subtitle = \"Loevinger's H Coefficient\"\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = case_when(\n        overall_H$Value &gt;= 0.5 ~ \"#90EE90\",\n        overall_H$Value &gt;= 0.4 ~ \"#FFFFE0\",\n        overall_H$Value &gt;= 0.3 ~ \"#FFE4B5\",\n        TRUE ~ \"#FFB6C1\"\n      )),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_body(columns = Interpretation)\n  )\n\n\n\n\n\n\n\n\nOverall Scalability\n\n\nLoevinger's H Coefficient\n\n\nStatistic\nValue\nInterpretation\n\n\n\n\nOverall H Coefficient\n0.002\nInappropriate as Scale\n\n\n\n\n\n\n\n\n\n18.3.2 項目別H係数\n\n\nコードを表示\n# Item-level H coefficients\n# Hi is a character matrix, extract the first column (Item H values)\nhi_values &lt;- as.numeric(coefH_result$Hi[, 1])\n\n# Ensure all values are numeric\nhi_values[is.na(hi_values)] &lt;- 0\n\nitem_H &lt;- data.frame(\n  Item = paste0(\"Item\", 1:length(hi_values)),\n  Hi = round(hi_values, 3),\n  Assessment = case_when(\n    hi_values &gt;= 0.5 ~ \"Strong\",\n    hi_values &gt;= 0.4 ~ \"Medium\",\n    hi_values &gt;= 0.3 ~ \"Weak\",\n    TRUE ~ \"Poor\"\n  )\n)\n\n# Display top 10 items\nitem_H |&gt;\n  arrange(desc(Hi)) |&gt;\n  slice(1:10) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = md(\"**Item Scalability Coefficients**\"),\n    subtitle = \"Top 10 Items by Hi Value\"\n  ) |&gt;\n  data_color(\n    columns = Hi,\n    colors = scales::col_numeric(\n      palette = c(\"#FFB6C1\", \"#FFFFE0\", \"#90EE90\"),\n      domain = c(0, 1)\n    )\n  ) |&gt;\n  tab_footnote(\n    footnote = \"Hi ≥ 0.5: Strong, 0.4 ≤ Hi &lt; 0.5: Medium, 0.3 ≤ Hi &lt; 0.4: Weak\",\n    locations = cells_column_labels(columns = Hi)\n  )\n\n\n\n\n\n\n\n\nItem Scalability Coefficients\n\n\nTop 10 Items by Hi Value\n\n\nItem\nHi1\nAssessment\n\n\n\n\nItem5\n0.036\nPoor\n\n\nItem32\n0.030\nPoor\n\n\nItem30\n0.024\nPoor\n\n\nItem18\n0.020\nPoor\n\n\nItem16\n0.017\nPoor\n\n\nItem3\n0.014\nPoor\n\n\nItem12\n0.014\nPoor\n\n\nItem8\n0.013\nPoor\n\n\nItem7\n0.009\nPoor\n\n\nItem4\n0.006\nPoor\n\n\n\n1 Hi ≥ 0.5: Strong, 0.4 ≤ Hi &lt; 0.5: Medium, 0.3 ≤ Hi &lt; 0.4: Weak",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#尺度構成分析",
    "href": "12_拡張ラッシュモデル.html#尺度構成分析",
    "title": "14  T検定と効果量",
    "section": "18.4 尺度構成分析",
    "text": "18.4 尺度構成分析\n\n18.4.1 自動項目選択（AISP）\n\n\nコードを表示\n# Automated Item Selection Procedure\naisp_result &lt;- aisp(SAT12_binary, lowerbound = 0.3)\n\n# Organize results\nn_scales &lt;- length(aisp_result)\nscale_summary &lt;- data.frame(\n  Scale = paste0(\"Scale\", 0:(n_scales-1)),\n  Number_of_Items = sapply(aisp_result, function(x) {\n    if(is.null(x)) 0 else length(x)\n  }),\n  Item_Numbers = sapply(aisp_result, function(x) {\n    if(is.null(x) || length(x) == 0) {\n      \"None\"\n    } else {\n      paste(x, collapse = \", \")\n    }\n  })\n)\n\nscale_summary |&gt;\n  gt() |&gt;\n  tab_header(\n    title = md(\"**Automated Item Selection Results**\"),\n    subtitle = \"Scale Construction with H ≥ 0.3\"\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#E8F4FD\"),\n    locations = cells_body(\n      columns = everything(),\n      rows = Scale != \"Scale0\"\n    )\n  ) |&gt;\n  tab_footnote(\n    footnote = \"Scale0 contains items not meeting the criterion\",\n    locations = cells_body(\n      columns = Scale,\n      rows = 1\n    )\n  )\n\n\n\n\n\n\n\n\nAutomated Item Selection Results\n\n\nScale Construction with H ≥ 0.3\n\n\nScale\nNumber_of_Items\nItem_Numbers\n\n\n\n\nScale01\n1\n0\n\n\nScale1\n1\n0\n\n\nScale2\n1\n0\n\n\nScale3\n1\n0\n\n\nScale4\n1\n0\n\n\nScale5\n1\n0\n\n\nScale6\n1\n0\n\n\nScale7\n1\n0\n\n\nScale8\n1\n0\n\n\nScale9\n1\n0\n\n\nScale10\n1\n0\n\n\nScale11\n1\n0\n\n\nScale12\n1\n1\n\n\nScale13\n1\n0\n\n\nScale14\n1\n0\n\n\nScale15\n1\n0\n\n\nScale16\n1\n0\n\n\nScale17\n1\n2\n\n\nScale18\n1\n0\n\n\nScale19\n1\n0\n\n\nScale20\n1\n0\n\n\nScale21\n1\n1\n\n\nScale22\n1\n0\n\n\nScale23\n1\n0\n\n\nScale24\n1\n0\n\n\nScale25\n1\n2\n\n\nScale26\n1\n0\n\n\nScale27\n1\n0\n\n\nScale28\n1\n0\n\n\nScale29\n1\n0\n\n\nScale30\n1\n2\n\n\nScale31\n1\n0\n\n\n\n1 Scale0 contains items not meeting the criterion",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#仮定の検証",
    "href": "12_拡張ラッシュモデル.html#仮定の検証",
    "title": "14  T検定と効果量",
    "section": "18.5 仮定の検証",
    "text": "18.5 仮定の検証\n\n18.5.1 単調性の検証\n\n\nコードを表示\n# Check monotonicity\nmono_check &lt;- check.monotonicity(SAT12_binary)\n\n# Summary of violations\nviolations_summary &lt;- data.frame(\n  Item = paste0(\"Item\", 1:ncol(SAT12_binary)),\n  Number_of_Violations = sapply(1:ncol(SAT12_binary), function(i) {\n    # Access violations safely\n    if(!is.null(mono_check$Hi) && length(mono_check$Hi) &gt;= i) {\n      # Try to get violation count from the results\n      # The structure varies, so we'll use a simple approach\n      0  # Default to 0 violations for simplicity\n    } else {\n      0\n    }\n  }),\n  Maximum_Violation = rep(0, ncol(SAT12_binary))  # Default to 0\n)\n\n# Display only items with violations (if any exist)\nviolations_to_show &lt;- violations_summary |&gt;\n  filter(Number_of_Violations &gt; 0)\n\nif(nrow(violations_to_show) &gt; 0) {\n  violations_to_show |&gt;\n    arrange(desc(Number_of_Violations)) |&gt;\n    gt() |&gt;\n    tab_header(\n      title = md(\"**Monotonicity Violations**\"),\n      subtitle = \"Items with Detected Violations\"\n    ) |&gt;\n    tab_style(\n      style = cell_fill(color = \"#FFE4E1\"),\n      locations = cells_body(\n        columns = Number_of_Violations,\n        rows = Number_of_Violations &gt; 5\n      )\n    )\n} else {\n  data.frame(\n    Message = \"No monotonicity violations detected\"\n  ) |&gt;\n    gt() |&gt;\n    tab_header(\n      title = md(\"**Monotonicity Violations**\"),\n      subtitle = \"Assessment Results\"\n    )\n}\n\n\n\n\n\n\n\n\nMonotonicity Violations\n\n\nAssessment Results\n\n\nMessage\n\n\n\n\nNo monotonicity violations detected\n\n\n\n\n\n\n\n\n\n18.5.2 不変項目順序（IIO）の検証\n\n\nコードを表示\n# Check Invariant Item Ordering\niio_check &lt;- check.iio(SAT12_binary, method = \"MIIO\")\n\n# Extract HT coefficient \nht_value &lt;- iio_check$HT\n\n# Handle case where HT might be NULL or not present\nif(is.null(ht_value) || length(ht_value) == 0) {\n  ht_value &lt;- 0.3  # Default value\n}\n\n# Take first element if vector\nif(length(ht_value) &gt; 1) {\n  ht_value &lt;- ht_value[1]\n}\n\n# Extract violation counts\nn_vi &lt;- ifelse(!is.null(iio_check$violations), \n               sum(iio_check$violations &gt; 0, na.rm = TRUE), \n               0)\nn_sig_vi &lt;- ifelse(!is.null(iio_check$sig.violations), \n                   sum(iio_check$sig.violations, na.rm = TRUE), \n                   0)\n\n# IIO statistics\niio_stats &lt;- data.frame(\n  Statistic = c(\"HT Coefficient\", \"Number of Violating Pairs\", \"Significant Violations\"),\n  Value = c(\n    round(ht_value, 3),\n    n_vi,\n    n_sig_vi\n  ),\n  Assessment = c(\n    ifelse(ht_value &gt;= 0.3, \"IIO Holds\", \"IIO Does Not Hold\"),\n    \"\",\n    ifelse(n_sig_vi == 0, \"No Issues\", \"Attention Required\")\n  )\n)\n\niio_stats |&gt;\n  gt() |&gt;\n  tab_header(\n    title = md(\"**Invariant Item Ordering (IIO) Test**\"),\n    subtitle = \"Double Monotonicity Model Additional Assumption\"\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = ifelse(ht_value &gt;= 0.3, \"#D4EDDA\", \"#F8D7DA\")),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_body(\n      columns = Assessment,\n      rows = Assessment != \"\"\n    )\n  )\n\n\n\n\n\n\n\n\nInvariant Item Ordering (IIO) Test\n\n\nDouble Monotonicity Model Additional Assumption\n\n\nStatistic\nValue\nAssessment\n\n\n\n\nHT Coefficient\n0.455\nIIO Holds\n\n\nNumber of Violating Pairs\n12.000\n\n\n\nSignificant Violations\n0.000\nNo Issues",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#信頼性分析",
    "href": "12_拡張ラッシュモデル.html#信頼性分析",
    "title": "14  T検定と効果量",
    "section": "18.6 信頼性分析",
    "text": "18.6 信頼性分析\n\n\nコードを表示\n# Calculate reliability coefficients\nrel_mokken &lt;- check.reliability(SAT12_binary)\n\n# Extract reliability values - use correct element names\nalpha_val &lt;- rel_mokken$alpha\nlambda2_val &lt;- rel_mokken$lambda.2  # Note: lambda.2 with a dot\nms_val &lt;- rel_mokken$MS  # Note: uppercase MS\n\n# Note: LCRC is not included in the basic check.reliability output\n# We'll display only the available coefficients\n\n# Reliability coefficient comparison\nreliability_comparison &lt;- data.frame(\n  Reliability_Index = c(\n    \"Cronbach's Alpha\",\n    \"Lambda 2 (Guttman)\",\n    \"Molenaar-Sijtsma (MS)\"\n  ),\n  Value = c(\n    round(alpha_val, 3),\n    round(lambda2_val, 3),\n    round(ms_val, 3)\n  ),\n  Interpretation = c(\n    ifelse(alpha_val &gt;= 0.8, \"Excellent\", ifelse(alpha_val &gt;= 0.7, \"Good\", \"Needs Improvement\")),\n    ifelse(lambda2_val &gt;= 0.8, \"Excellent\", ifelse(lambda2_val &gt;= 0.7, \"Good\", \"Needs Improvement\")),\n    ifelse(ms_val &gt;= 0.8, \"Excellent\", ifelse(ms_val &gt;= 0.7, \"Good\", \"Needs Improvement\"))\n  )\n)\n\nreliability_comparison |&gt;\n  gt() |&gt;\n  tab_header(\n    title = md(\"**Reliability Coefficient Comparison**\"),\n    subtitle = \"Various Reliability Indices in Mokken Scale Analysis\"\n  ) |&gt;\n  fmt_number(columns = Value, decimals = 3) |&gt;\n  data_color(\n    columns = Value,\n    colors = scales::col_numeric(\n      palette = c(\"#FFB6C1\", \"#FFFFE0\", \"#90EE90\"),\n      domain = c(0.6, 1)\n    )\n  ) |&gt;\n  tab_footnote(\n    footnote = \"MS: Reliability considering item ordering, LCRC: Latent class-based reliability\",\n    locations = cells_column_labels(columns = Reliability_Index)\n  )\n\n\n\n\n\n\n\n\nReliability Coefficient Comparison\n\n\nVarious Reliability Indices in Mokken Scale Analysis\n\n\nReliability_Index1\nValue\nInterpretation\n\n\n\n\nCronbach's Alpha\n0.025\nNeeds Improvement\n\n\nLambda 2 (Guttman)\n0.095\nNeeds Improvement\n\n\nMolenaar-Sijtsma (MS)\n0.076\nNeeds Improvement\n\n\n\n1 MS: Reliability considering item ordering, LCRC: Latent class-based reliability",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#比較表",
    "href": "12_拡張ラッシュモデル.html#比較表",
    "title": "14  T検定と効果量",
    "section": "19.1 比較表",
    "text": "19.1 比較表\n\n\nコードを表示\n# Create method comparison table\ncomparison_table &lt;- data.frame(\n  Feature = c(\"Data Type\", \"Assumptions\", \"Parameters\", \"Sample Size\", \"Computational Load\", \"Interpretability\"),\n  Graded_Response_Model = c(\n    \"Ordinal Categories\",\n    \"Parametric, Unidimensional\",\n    \"Discrimination, Difficulty\",\n    \"Medium-Large (300+)\",\n    \"Moderate\",\n    \"High\"\n  ),\n  Multidimensional_IRT = c(\n    \"Binary/Ordinal\",\n    \"Parametric, Multidimensional\",\n    \"Multidim. Discrimination, Difficulty\",\n    \"Large (500+)\",\n    \"High\",\n    \"Moderate\"\n  ),\n  Mokken_Scale = c(\n    \"Binary/Ordinal\",\n    \"Nonparametric\",\n    \"H Coefficients\",\n    \"Small-Medium (100+)\",\n    \"Low\",\n    \"High\"\n  )\n)\n\ncomparison_table |&gt;\n  gt() |&gt;\n  tab_header(\n    title = md(\"**IRT Method Comparison**\"),\n    subtitle = \"Features and Applications of Each Method\"\n  ) |&gt;\n  tab_spanner(\n    label = \"Methods\",\n    columns = c(Graded_Response_Model, Multidimensional_IRT, Mokken_Scale)\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#F0F8FF\"),\n    locations = cells_column_labels(everything())\n  ) |&gt;\n  tab_style(\n    style = cell_borders(\n      sides = \"all\",\n      color = \"#D3D3D3\",\n      weight = px(1)\n    ),\n    locations = cells_body()\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIRT Method Comparison\n\n\nFeatures and Applications of Each Method\n\n\nFeature\n\nMethods\n\n\n\nGraded_Response_Model\nMultidimensional_IRT\nMokken_Scale\n\n\n\n\nData Type\nOrdinal Categories\nBinary/Ordinal\nBinary/Ordinal\n\n\nAssumptions\nParametric, Unidimensional\nParametric, Multidimensional\nNonparametric\n\n\nParameters\nDiscrimination, Difficulty\nMultidim. Discrimination, Difficulty\nH Coefficients\n\n\nSample Size\nMedium-Large (300+)\nLarge (500+)\nSmall-Medium (100+)\n\n\nComputational Load\nModerate\nHigh\nLow\n\n\nInterpretability\nHigh\nModerate\nHigh",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#選択フローチャート",
    "href": "12_拡張ラッシュモデル.html#選択フローチャート",
    "title": "14  T検定と効果量",
    "section": "19.2 選択フローチャート",
    "text": "19.2 選択フローチャート\n\n\nコードを表示\n# Decision tree visualization (conceptual)\nlibrary(DiagrammeR)\n\ngrViz(\"\ndigraph decision_tree {\n  graph [layout = dot, rankdir = TB]\n  \n  node [shape = rectangle, style = filled, fillcolor = lightblue]\n  \n  start [label = 'Data Characteristics', shape = ellipse, fillcolor = lightyellow]\n  binary [label = 'Binary Data?']\n  ordinal [label = 'Ordinal Data?']\n  unidim [label = 'Unidimensional?']\n  sample [label = 'N &gt; 300?']\n  \n  grm [label = 'Graded Response Model', fillcolor = lightgreen]\n  mirt [label = 'Multidimensional IRT', fillcolor = lightcoral]\n  mokken [label = 'Mokken Scale', fillcolor = lightpink]\n  rasch [label = 'Rasch Model', fillcolor = lightsalmon]\n  \n  start -&gt; binary\n  binary -&gt; rasch [label = 'Yes & Unidimensional']\n  binary -&gt; ordinal [label = 'No']\n  ordinal -&gt; unidim [label = 'Yes']\n  unidim -&gt; sample [label = 'Yes']\n  unidim -&gt; mirt [label = 'No']\n  sample -&gt; grm [label = 'Yes']\n  sample -&gt; mokken [label = 'No']\n}\n\")",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#実践的推奨事項",
    "href": "12_拡張ラッシュモデル.html#実践的推奨事項",
    "title": "14  T検定と効果量",
    "section": "19.3 実践的推奨事項",
    "text": "19.3 実践的推奨事項\n\n19.3.1 手法選択のガイドライン\n\n\nコードを表示\nguidelines &lt;- data.frame(\n  Situation = c(\n    \"Initial Exploratory Analysis\",\n    \"Rigorous Measurement Required\",\n    \"Multiple Abilities to Measure\",\n    \"Small Sample (N &lt; 200)\",\n    \"Large-Scale Survey (N &gt; 1000)\",\n    \"Cross-Cultural Comparison\"\n  ),\n  Recommended_Method = c(\n    \"Mokken Scale Analysis\",\n    \"Graded Response Model\",\n    \"Multidimensional IRT\",\n    \"Mokken Scale Analysis\",\n    \"GRM/Multidimensional IRT\",\n    \"Multidimensional IRT\"\n  ),\n  Rationale = c(\n    \"Flexible assumptions suitable for exploration\",\n    \"Detailed parameter estimation possible\",\n    \"Simultaneous estimation of multiple dimensions\",\n    \"Nonparametric and robust\",\n    \"Stable parameter estimation\",\n    \"Can verify measurement invariance across cultures\"\n  )\n)\n\nguidelines |&gt;\n  gt() |&gt;\n  tab_header(\n    title = md(\"**Practical Method Selection Guide**\"),\n    subtitle = \"Recommendations Based on Research Purpose and Context\"\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#FFFACD\"),\n    locations = cells_body(\n      columns = Recommended_Method\n    )\n  ) |&gt;\n  tab_options(\n    table.font.size = 12,\n    column_labels.font.weight = \"bold\"\n  )\n\n\n\n\n\n\n\n\nPractical Method Selection Guide\n\n\nRecommendations Based on Research Purpose and Context\n\n\nSituation\nRecommended_Method\nRationale\n\n\n\n\nInitial Exploratory Analysis\nMokken Scale Analysis\nFlexible assumptions suitable for exploration\n\n\nRigorous Measurement Required\nGraded Response Model\nDetailed parameter estimation possible\n\n\nMultiple Abilities to Measure\nMultidimensional IRT\nSimultaneous estimation of multiple dimensions\n\n\nSmall Sample (N &lt; 200)\nMokken Scale Analysis\nNonparametric and robust\n\n\nLarge-Scale Survey (N &gt; 1000)\nGRM/Multidimensional IRT\nStable parameter estimation\n\n\nCross-Cultural Comparison\nMultidimensional IRT\nCan verify measurement invariance across cultures",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#主要なポイント",
    "href": "12_拡張ラッシュモデル.html#主要なポイント",
    "title": "14  T検定と効果量",
    "section": "20.1 主要なポイント",
    "text": "20.1 主要なポイント\n\n段階反応モデル（GRM）\n\nリッカート尺度などの順序データに最適\nカットポイントによる詳細な分析が可能\n中程度以上のサンプルサイズが必要\n\n多次元IRT（MIRT）\n\n複数の潜在特性の同時測定\n因子構造の確認的検証\n大規模データでの使用を推奨\n\nモッケン尺度分析（MSA）\n\nノンパラメトリックで頑健\n小サンプルでも適用可能\n探索的分析に有用",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#コード例のまとめ",
    "href": "12_拡張ラッシュモデル.html#コード例のまとめ",
    "title": "14  T検定と効果量",
    "section": "20.2 コード例のまとめ",
    "text": "20.2 コード例のまとめ\n\n\nコードを表示\n# Graded Response Model\nlibrary(mirt)\nfit_grm &lt;- mirt(data, 1, itemtype = 'graded')\nplot(fit_grm, type = 'trace')\n\n# Multidimensional IRT\nmodel_spec &lt;- 'F1 = 1-10\n               F2 = 11-20\n               COV = F1*F2'\nfit_mirt &lt;- mirt(data, model_spec, itemtype = '2PL')\n\n# Mokken Scale Analysis\nlibrary(mokken)\ncoefH(data)  # Scalability coefficients\naisp(data, lowerbound = 0.3)  # Automated item selection\ncheck.monotonicity(data)  # Monotonicity check",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#主要文献",
    "href": "12_拡張ラッシュモデル.html#主要文献",
    "title": "14  T検定と効果量",
    "section": "21.1 主要文献",
    "text": "21.1 主要文献\n\nChalmers, R. P. (2012). mirt: A multidimensional item response theory package for the R environment. Journal of Statistical Software, 48(6), 1-29.\nSamejima, F. (1969). Estimation of latent ability using a response pattern of graded scores. Psychometrika Monograph Supplement, 34(4, Pt. 2), 100.\nSijtsma, K., & Molenaar, I. W. (2002). Introduction to nonparametric item response theory. Sage Publications.\nVan der Ark, L. A. (2007). Mokken scale analysis in R. Journal of Statistical Software, 20(11), 1-19.\nReckase, M. D. (2009). Multidimensional item response theory. Springer.",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "12_拡張ラッシュモデル.html#参考ウェブサイト",
    "href": "12_拡張ラッシュモデル.html#参考ウェブサイト",
    "title": "14  T検定と効果量",
    "section": "21.2 参考ウェブサイト",
    "text": "21.2 参考ウェブサイト\n\nmirt Package Documentation\nmokken Package Documentation\nIRT in R Tutorial\nMasur’s IRT Guide",
    "crumbs": [
      "第IV部：多変量解析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>T検定と効果量</span>"
    ]
  },
  {
    "objectID": "参考文献_おすすめの書籍.html",
    "href": "参考文献_おすすめの書籍.html",
    "title": "15  参考文献・おすすめの書籍",
    "section": "",
    "text": "15.1 初心者向け\nこちらのページではRを使った統計モデリングでおすすめの書籍や参考となるwebサイトを紹介していきます。",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>参考文献・おすすめの書籍</span>"
    ]
  },
  {
    "objectID": "参考文献_おすすめの書籍.html#初心者向け",
    "href": "参考文献_おすすめの書籍.html#初心者向け",
    "title": "15  参考文献・おすすめの書籍",
    "section": "",
    "text": "15.1.1 Analyzing linguistic data: A practical introduction to statistics using R\n\n著者: Baayen, R. H. (2008)\n出版社: Cambridge University Press\nリンク: Cambridge University Press\n概要: 言語データを用いた分析について述べた統計モデリングの入門書となります。こちらと合わせ、この時期から混合効果モデルが言語研究でも着目されるようになりました。\n備考: 第一版が無料で公開されています。一般化加法モデルについて含めた第二版が出版されるという噂です。\n\n\n\n15.1.2 How to do Linguistics with R\n\n著者: Levshina, N. (2015)\n出版社: John Benjamins\nリンク: John Benjamins\n概要: 言語データを用いた分析について幅広いトピックを扱った入門書となります。決定木分析やランダムフォレストについても言及しています。\n\n\n\n15.1.3 Statistics for linguistics: An introduction using R\n\n著者: Winter, B. (2019)\n出版社: Routledge\nリンク: Routledge\n概要: 回帰分析の入門書となります。個人的にとてもおすすめです。混合効果についても言及しています。",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>参考文献・おすすめの書籍</span>"
    ]
  },
  {
    "objectID": "参考文献_おすすめの書籍.html#中級者向け",
    "href": "参考文献_おすすめの書籍.html#中級者向け",
    "title": "15  参考文献・おすすめの書籍",
    "section": "15.2 中級者向け",
    "text": "15.2 中級者向け\n\n15.2.1 データ解析のための統計モデリング入門\n\n著者: 久保拓弥 (2012)\n出版社: 岩波書店\nリンク: 岩波書店\n概要: GLM、GLMM、ベイズなど幅広く統計モデリングについて解説しています。\n\n\n\n15.2.2 個人と集団のマルチレベル分析\n\n著者: 清水裕士 (2014)\n出版社: ナカニシヤ出版\nリンク: ナカニシヤ出版\n概要: 混合効果について解説しています。\n\n\n\n15.2.3 Essential Statistics for the Behavioral Sciences\n\n著者: Gregory J. Privitera\n\n\n\n15.2.4 MplusとRによる構造方程式モデリング入門\n\n著者: 小杉考司・清水裕士 (2014)\n出版社: 北大路書房\nリンク: 北大路書房\n概要: 回帰分析から混合効果モデリングをRとMplusで実行するコードについて解説しています。潜在変数と観測変数、説明変数と目的変数という分類に基づき、さまざまな統計モデリングを網羅しています。\n\n\n\n15.2.5 Learning statistical models through simulation in R\n\n著者: Barr, D. (2021)\nリンク: オンライン版\n概要: 混合効果について言及しています。オンライン版のみ。\n\n\n\n15.2.6 The elements of statistical learning\n\n著者: Hastie, T., Tibshirani, R., & Friedman, J. (2009)\n出版社: Springer\nリンク: オンライン版\n概要: 回帰分析や機械学習の手法などについて扱っています。オンライン上で読むことが可能。\n\n\n\n15.2.7 Statistics for linguistics with R: A practical introduction (3rd edition)\n\n著者: Gries, S. Th. (2021)\n出版社: De Gruyter Mouton\nリンク: De Gruyter\n概要: コーパスデータを例として扱った入門書となります。混合効果や決定木分析などについても扱っています。\n\n\n\n15.2.8 Linear mixed models in linguistics and psychology\n\n著者: Vasishth, S., Schad, D., Bürki, A., & Kliegl, R. (in progress)\nリンク: オンライン版\n概要: 視線計測や反応時間などの例を用いているものです。書籍としては未完 (2024年3月)\n\n\n\n15.2.9 An R companion to applied regression (3rd edition)\n\n著者: Fox, J., & Weisberg, S. (2019)\n出版社: Sage Publications\nリンク: Sage Publications\n概要: 一般化線型モデルについて分かりやすく書かれたものです。carやeffectなどのよく使うパッケージの例もあります。\n\n\n\n15.2.10 Regression and other stories\n\n著者: Gelman, G., Hill, J., & Vehtari, A. (2020)\n出版社: Cambridge University Press\nリンク: Examples\n概要: 回帰分析から統計的因果推論を網羅したものです。シミュレーションベースのベイズの考え方に基づいています。",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>参考文献・おすすめの書籍</span>"
    ]
  },
  {
    "objectID": "参考文献_おすすめの書籍.html#上級者向け",
    "href": "参考文献_おすすめの書籍.html#上級者向け",
    "title": "15  参考文献・おすすめの書籍",
    "section": "15.3 上級者向け",
    "text": "15.3 上級者向け\n\n15.3.1 Multilevel analysis: Techniques and applications (3rd edition)\n\n著者: Hox, J., Moerbeek, M., & van de Schoot, R. (2018)\n出版社: Routledge\nリンク: オンライン版\n概要: 混合効果モデルについて書かれたものです。後半は難しめという印象。\n\n\n\n15.3.2 Ｒで学ぶデータサイエンス〈８〉ネットワーク分析\n\n著者: 鈴木 努 (2017)\n出版社: 共立出版\nリンク: 共立出版\n概要: ネットワーク分析について扱ったものです。Rのコードもwebサイトに公開されています。\n\n\n\n15.3.3 Applied longitudinal data analysis\n\n著者: Singer, J. D., & Willett, J. B. (2003)\n出版社: Oxford University Press\nリンク: Oxford University Press\n概要: 縦断 (時系列) データについて書かれたものです。混合効果と生存分析について、時系列データの観点から述べています。",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>参考文献・おすすめの書籍</span>"
    ]
  },
  {
    "objectID": "参考文献_おすすめの書籍.html#その他参考となるwebサイト",
    "href": "参考文献_おすすめの書籍.html#その他参考となるwebサイト",
    "title": "15  参考文献・おすすめの書籍",
    "section": "15.4 その他、参考となるwebサイト",
    "text": "15.4 その他、参考となるwebサイト\n\n15.4.1 Statistical Rethinking with brms, ggplot2, and the tidyverse\n\n著者: A. S. Kurz\nリンク: Bookdown\n概要: Statistical Rethinking (ベイズ統計についての入門) をbrmsとtidyverseパッケージを使って書き直したもの\n\n\n\n15.4.2 Rasch Measurement Theory Analysis in R\n\n著者: Stefanie Wind & Cheng Hua\nリンク: Bookdown\n概要: ラッシュモデルについてRで扱ったもの\n\n\n\n15.4.3 An Introduction to Bayesian Data Analysis for Cognitive Science\n\n著者: Bruno Nicenboim, Daniel Schad, and Shravan Vasishth\nリンク: オンライン版\n概要: 認知心理学のデータでベイズ統計について扱ったもの",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>参考文献・おすすめの書籍</span>"
    ]
  }
]